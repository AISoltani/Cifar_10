50000/50000 [==============================] - 67s 1ms/step - loss: 1.8005 - acc: 0.3408 - val_loss: 1.4938 - val_acc: 0.4660
Epoch 2/100
50000/50000 [==============================] - 61s 1ms/step - loss: 1.4684 - acc: 0.4660 - val_loss: 1.3310 - val_acc: 0.5238
Epoch 3/100
50000/50000 [==============================] - 61s 1ms/step - loss: 1.3402 - acc: 0.5195 - val_loss: 1.3002 - val_acc: 0.5396
Epoch 4/100
50000/50000 [==============================] - 63s 1ms/step - loss: 1.2528 - acc: 0.5551 - val_loss: 1.1815 - val_acc: 0.5784
Epoch 5/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.1782 - acc: 0.5843 - val_loss: 1.0739 - val_acc: 0.6203
Epoch 6/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.1186 - acc: 0.6063 - val_loss: 1.0261 - val_acc: 0.6395
Epoch 7/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.0638 - acc: 0.6259 - val_loss: 0.9777 - val_acc: 0.6566
Epoch 8/100
 8192/50000 [===>..........................] - ETA: 49s - loss: 1.0221 - acc: 0.6451
PS E:\Master\Semester 2\ML\Homeworks\Pure Code> cd 'e:\Master\Semester 2\ML\Homeworks\Pure Code'; ${env:PYTHONIOENCODING}='UTF-8'; ${env:PYTHONUNBUFFERED}='1'; & 'C:\ProgramData\Anaconda3\python.exe' 'c:\Users\Hamidreza\.vscode\extensions\ms-python.python-2019.6.22090\pythonFiles\ptvsd_launcher.py'
'--default' '--client' '--host' 'localhost' '--port' '64784' 'e:\Master\Semester 2\ML\Homeworks\Pure Code\ML\FinalProject\cnn2.py'
Using TensorFlow backend.
x_train shape: (50000, 32, 32, 3)
50000 train samples
10000 test samples
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Not using data augmentation.
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 50000 samples, validate on 10000 samples
Epoch 1/100
2019-07-01 12:13:05.714661: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-07-01 12:13:06.686865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:03:00.0
totalMemory: 4.00GiB freeMemory: 3.35GiB
2019-07-01 12:13:06.690506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-07-01 12:13:07.949317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-01 12:13:07.953159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-07-01 12:13:07.956274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-07-01 12:13:07.958138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3058 MB memory) -> physical GPU (device: 0, name: GeForce 840M, pci bus id: 0000:03:00.0, compute capability: 5.0)
2019-07-01 12:13:08.771890: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
50000/50000 [==============================] - 66s 1ms/step - loss: 1.8045 - acc: 0.3379 - val_loss: 1.5034 - val_acc: 0.4530
Epoch 2/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.4740 - acc: 0.4686 - val_loss: 1.3270 - val_acc: 0.5282
Epoch 3/100
50000/50000 [==============================] - 63s 1ms/step - loss: 1.3317 - acc: 0.5237 - val_loss: 1.3866 - val_acc: 0.5120
Epoch 4/100
50000/50000 [==============================] - 64s 1ms/step - loss: 1.2370 - acc: 0.5616 - val_loss: 1.1884 - val_acc: 0.5839
Epoch 5/100
50000/50000 [==============================] - 64s 1ms/step - loss: 1.1544 - acc: 0.5920 - val_loss: 1.0901 - val_acc: 0.6150
Epoch 6/100
50000/50000 [==============================] - 63s 1ms/step - loss: 1.0930 - acc: 0.6125 - val_loss: 1.0265 - val_acc: 0.6350
Epoch 7/100
50000/50000 [==============================] - 63s 1ms/step - loss: 1.0430 - acc: 0.6338 - val_loss: 0.9698 - val_acc: 0.6599
Epoch 8/100
50000/50000 [==============================] - 63s 1ms/step - loss: 1.0024 - acc: 0.6494 - val_loss: 0.9529 - val_acc: 0.6674
Epoch 9/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.9665 - acc: 0.6619 - val_loss: 0.9012 - val_acc: 0.6854
Epoch 10/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.9294 - acc: 0.6731 - val_loss: 0.8963 - val_acc: 0.6856
Epoch 11/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9055 - acc: 0.6842 - val_loss: 0.8615 - val_acc: 0.6986
Epoch 12/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8828 - acc: 0.6909 - val_loss: 0.8484 - val_acc: 0.7039
Epoch 13/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8634 - acc: 0.6982 - val_loss: 0.8449 - val_acc: 0.7037
Epoch 14/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8422 - acc: 0.7077 - val_loss: 0.8259 - val_acc: 0.7136
Epoch 15/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8246 - acc: 0.7136 - val_loss: 0.8298 - val_acc: 0.7098
Epoch 16/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8058 - acc: 0.7186 - val_loss: 0.7978 - val_acc: 0.7283
Epoch 17/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7936 - acc: 0.7235 - val_loss: 0.8040 - val_acc: 0.7234
Epoch 18/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7824 - acc: 0.7303 - val_loss: 0.8336 - val_acc: 0.7165
Epoch 19/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7749 - acc: 0.7330 - val_loss: 0.7806 - val_acc: 0.7341
Epoch 20/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7681 - acc: 0.7373 - val_loss: 0.7800 - val_acc: 0.7362
Epoch 21/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7557 - acc: 0.7410 - val_loss: 0.8025 - val_acc: 0.7249
Epoch 22/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7500 - acc: 0.7432 - val_loss: 0.7421 - val_acc: 0.7466
Epoch 23/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7419 - acc: 0.7468 - val_loss: 0.7176 - val_acc: 0.7516
Epoch 24/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7316 - acc: 0.7493 - val_loss: 0.7181 - val_acc: 0.7519
Epoch 25/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7293 - acc: 0.7504 - val_loss: 0.7317 - val_acc: 0.7544
Epoch 26/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7220 - acc: 0.7539 - val_loss: 0.7734 - val_acc: 0.7453
Epoch 27/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7179 - acc: 0.7564 - val_loss: 0.7035 - val_acc: 0.7607
Epoch 28/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7090 - acc: 0.7589 - val_loss: 0.7001 - val_acc: 0.7611
Epoch 29/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7085 - acc: 0.7598 - val_loss: 0.7010 - val_acc: 0.7604
Epoch 30/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7043 - acc: 0.7591 - val_loss: 0.7055 - val_acc: 0.7648
Epoch 31/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6954 - acc: 0.7655 - val_loss: 0.6927 - val_acc: 0.7670
Epoch 32/100
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6954 - acc: 0.7654 - val_loss: 0.6840 - val_acc: 0.7669
Epoch 33/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6905 - acc: 0.7676 - val_loss: 0.7168 - val_acc: 0.7663
Epoch 34/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6865 - acc: 0.7661 - val_loss: 0.6863 - val_acc: 0.7669
Epoch 35/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6829 - acc: 0.7690 - val_loss: 0.6830 - val_acc: 0.7719
Epoch 36/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6775 - acc: 0.7703 - val_loss: 0.7253 - val_acc: 0.7577
Epoch 37/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6710 - acc: 0.7736 - val_loss: 0.6644 - val_acc: 0.7758
Epoch 38/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6696 - acc: 0.7740 - val_loss: 0.6622 - val_acc: 0.7743
Epoch 39/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6642 - acc: 0.7759 - val_loss: 0.7062 - val_acc: 0.7605
Epoch 40/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6653 - acc: 0.7760 - val_loss: 0.6816 - val_acc: 0.7680
Epoch 41/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6588 - acc: 0.7780 - val_loss: 0.6746 - val_acc: 0.7697
Epoch 42/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6584 - acc: 0.7779 - val_loss: 0.6546 - val_acc: 0.7779
Epoch 43/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6521 - acc: 0.7787 - val_loss: 0.6756 - val_acc: 0.7727
Epoch 44/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6510 - acc: 0.7810 - val_loss: 0.6626 - val_acc: 0.7753
Epoch 45/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6474 - acc: 0.7811 - val_loss: 0.6737 - val_acc: 0.7808
Epoch 46/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6451 - acc: 0.7820 - val_loss: 0.6637 - val_acc: 0.7801
Epoch 47/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6434 - acc: 0.7849 - val_loss: 0.6656 - val_acc: 0.7804
Epoch 48/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6426 - acc: 0.7841 - val_loss: 0.6735 - val_acc: 0.7767
Epoch 49/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6389 - acc: 0.7879 - val_loss: 0.6794 - val_acc: 0.7741
Epoch 50/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6416 - acc: 0.7842 - val_loss: 0.6465 - val_acc: 0.7863
Epoch 51/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6357 - acc: 0.7878 - val_loss: 0.6557 - val_acc: 0.7806
Epoch 52/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6342 - acc: 0.7880 - val_loss: 0.6655 - val_acc: 0.7790
Epoch 53/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6313 - acc: 0.7883 - val_loss: 0.7495 - val_acc: 0.7628
Epoch 54/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6336 - acc: 0.7885 - val_loss: 0.6541 - val_acc: 0.7811
Epoch 55/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6256 - acc: 0.7923 - val_loss: 0.6561 - val_acc: 0.7849
Epoch 56/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6282 - acc: 0.7896 - val_loss: 0.6834 - val_acc: 0.7768
Epoch 57/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6310 - acc: 0.7898 - val_loss: 0.7369 - val_acc: 0.7707
Epoch 58/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6294 - acc: 0.7894 - val_loss: 0.6908 - val_acc: 0.7725
Epoch 59/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6226 - acc: 0.7925 - val_loss: 0.7099 - val_acc: 0.7696
Epoch 60/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6255 - acc: 0.7922 - val_loss: 0.7072 - val_acc: 0.7699
Epoch 61/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6233 - acc: 0.7923 - val_loss: 0.6851 - val_acc: 0.7725
Epoch 62/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6220 - acc: 0.7929 - val_loss: 0.6708 - val_acc: 0.7884
Epoch 63/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6224 - acc: 0.7908 - val_loss: 0.7016 - val_acc: 0.7789
Epoch 64/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6172 - acc: 0.7935 - val_loss: 0.6685 - val_acc: 0.7781
Epoch 65/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6186 - acc: 0.7933 - val_loss: 0.6547 - val_acc: 0.7837
Epoch 66/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6183 - acc: 0.7945 - val_loss: 0.6663 - val_acc: 0.7778
Epoch 67/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6137 - acc: 0.7961 - val_loss: 0.6545 - val_acc: 0.7852
Epoch 68/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6199 - acc: 0.7954 - val_loss: 0.6933 - val_acc: 0.7841
Epoch 69/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6096 - acc: 0.7963 - val_loss: 0.7188 - val_acc: 0.7664
Epoch 70/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6162 - acc: 0.7950 - val_loss: 0.6603 - val_acc: 0.7821
Epoch 71/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6211 - acc: 0.7956 - val_loss: 0.6516 - val_acc: 0.7837
Epoch 72/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6118 - acc: 0.7947 - val_loss: 0.6521 - val_acc: 0.7867
Epoch 73/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6147 - acc: 0.7944 - val_loss: 0.6627 - val_acc: 0.7817
Epoch 74/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6177 - acc: 0.7960 - val_loss: 0.7330 - val_acc: 0.7780
Epoch 75/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6171 - acc: 0.7969 - val_loss: 0.6656 - val_acc: 0.7794
Epoch 76/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6138 - acc: 0.7959 - val_loss: 0.6851 - val_acc: 0.7814
Epoch 77/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6111 - acc: 0.7964 - val_loss: 0.7134 - val_acc: 0.7638
Epoch 78/100
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6139 - acc: 0.7970 - val_loss: 0.6581 - val_acc: 0.7881
Epoch 79/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6129 - acc: 0.7964 - val_loss: 0.7093 - val_acc: 0.7697
Epoch 80/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6181 - acc: 0.7947 - val_loss: 0.7258 - val_acc: 0.7610
Epoch 81/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6151 - acc: 0.7947 - val_loss: 0.6557 - val_acc: 0.7872
Epoch 82/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6167 - acc: 0.7959 - val_loss: 0.6860 - val_acc: 0.7740
Epoch 83/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6143 - acc: 0.7965 - val_loss: 0.7492 - val_acc: 0.7513
Epoch 84/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6206 - acc: 0.7943 - val_loss: 0.6513 - val_acc: 0.7891
Epoch 85/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6133 - acc: 0.7966 - val_loss: 0.6601 - val_acc: 0.7863
Epoch 86/100
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6147 - acc: 0.7954 - val_loss: 0.7392 - val_acc: 0.7532
Epoch 87/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6199 - acc: 0.7954 - val_loss: 0.6943 - val_acc: 0.7739
Epoch 88/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6224 - acc: 0.7943 - val_loss: 0.7119 - val_acc: 0.7735
Epoch 89/100
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6187 - acc: 0.7948 - val_loss: 0.6544 - val_acc: 0.7857
Epoch 90/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6148 - acc: 0.7948 - val_loss: 0.7027 - val_acc: 0.7715
Epoch 91/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6146 - acc: 0.7976 - val_loss: 0.7035 - val_acc: 0.7783
Epoch 92/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6124 - acc: 0.7975 - val_loss: 0.7670 - val_acc: 0.7435
Epoch 93/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6129 - acc: 0.7959 - val_loss: 0.7392 - val_acc: 0.7736
Epoch 94/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6170 - acc: 0.7965 - val_loss: 0.6446 - val_acc: 0.7884
Epoch 95/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6158 - acc: 0.7964 - val_loss: 0.7045 - val_acc: 0.7751
Epoch 96/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6159 - acc: 0.7972 - val_loss: 0.6905 - val_acc: 0.7834
Epoch 97/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6147 - acc: 0.7961 - val_loss: 0.7106 - val_acc: 0.7755
Epoch 98/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6152 - acc: 0.7956 - val_loss: 0.6954 - val_acc: 0.7687
Epoch 99/100
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6193 - acc: 0.7937 - val_loss: 0.7672 - val_acc: 0.7737
Epoch 100/100
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6126 - acc: 0.7962 - val_loss: 0.7113 - val_acc: 0.7624
Saved trained model at E:\Master\Semester 2\ML\Homeworks\Pure Code\saved_models\keras_cifar10_trained_model.h5
10000/10000 [==============================] - 4s 356us/step
Test loss: 0.71133879737854
Test accuracy: 0.7624

############################
50000/50000 [==============================] - 70s 1ms/step - loss: 1.8195 - acc: 0.3307 - val_loss: 1.5081 - val_acc: 0.4549
Epoch 2/200
50000/50000 [==============================] - 61s 1ms/step - loss: 1.4952 - acc: 0.4561 - val_loss: 1.3666 - val_acc: 0.5107
Epoch 3/200
50000/50000 [==============================] - 61s 1ms/step - loss: 1.3597 - acc: 0.5122 - val_loss: 1.2550 - val_acc: 0.5571
Epoch 4/200
50000/50000 [==============================] - 61s 1ms/step - loss: 1.2643 - acc: 0.5501 - val_loss: 1.2056 - val_acc: 0.5725
Epoch 5/200
50000/50000 [==============================] - 61s 1ms/step - loss: 1.1823 - acc: 0.5819 - val_loss: 1.0780 - val_acc: 0.6209
Epoch 6/200
50000/50000 [==============================] - 61s 1ms/step - loss: 1.1154 - acc: 0.6070 - val_loss: 1.0348 - val_acc: 0.6379
Epoch 7/200
50000/50000 [==============================] - 62s 1ms/step - loss: 1.0608 - acc: 0.6289 - val_loss: 0.9933 - val_acc: 0.6528
Epoch 8/200
50000/50000 [==============================] - 62s 1ms/step - loss: 1.0140 - acc: 0.6456 - val_loss: 0.9446 - val_acc: 0.6722
Epoch 9/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.9771 - acc: 0.6584 - val_loss: 0.9161 - val_acc: 0.6798
Epoch 10/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.9459 - acc: 0.6697 - val_loss: 0.8811 - val_acc: 0.6953
Epoch 11/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.9153 - acc: 0.6799 - val_loss: 0.8559 - val_acc: 0.7009
Epoch 12/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8919 - acc: 0.6885 - val_loss: 0.8731 - val_acc: 0.6948
Epoch 13/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8702 - acc: 0.6970 - val_loss: 0.8259 - val_acc: 0.7123
Epoch 14/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8491 - acc: 0.7049 - val_loss: 0.8689 - val_acc: 0.6978
Epoch 15/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8375 - acc: 0.7115 - val_loss: 0.8115 - val_acc: 0.7181
Epoch 16/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8216 - acc: 0.7164 - val_loss: 0.8042 - val_acc: 0.7210
Epoch 17/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8038 - acc: 0.7233 - val_loss: 0.8014 - val_acc: 0.7232
Epoch 18/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7913 - acc: 0.7259 - val_loss: 0.7900 - val_acc: 0.7284
Epoch 19/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7837 - acc: 0.7311 - val_loss: 0.7752 - val_acc: 0.7384
Epoch 20/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.7724 - acc: 0.7337 - val_loss: 0.7641 - val_acc: 0.7403
Epoch 21/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.7636 - acc: 0.7380 - val_loss: 0.7470 - val_acc: 0.7452
Epoch 22/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7557 - acc: 0.7420 - val_loss: 0.7556 - val_acc: 0.7445
Epoch 23/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7483 - acc: 0.7450 - val_loss: 0.7230 - val_acc: 0.7551
Epoch 24/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7368 - acc: 0.7458 - val_loss: 0.7694 - val_acc: 0.7475
Epoch 25/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7312 - acc: 0.7503 - val_loss: 0.7354 - val_acc: 0.7518
Epoch 26/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7243 - acc: 0.7538 - val_loss: 0.7558 - val_acc: 0.7471
Epoch 27/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7103 - acc: 0.7565 - val_loss: 0.7112 - val_acc: 0.7594
Epoch 28/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7118 - acc: 0.7582 - val_loss: 0.7185 - val_acc: 0.7630
Epoch 29/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7022 - acc: 0.7602 - val_loss: 0.7042 - val_acc: 0.7622
Epoch 30/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6997 - acc: 0.7619 - val_loss: 0.7041 - val_acc: 0.7655
Epoch 31/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6974 - acc: 0.7640 - val_loss: 0.6871 - val_acc: 0.7744
Epoch 32/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6927 - acc: 0.7644 - val_loss: 0.6924 - val_acc: 0.7637
Epoch 33/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6868 - acc: 0.7669 - val_loss: 0.6930 - val_acc: 0.7691
Epoch 34/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6817 - acc: 0.7683 - val_loss: 0.6843 - val_acc: 0.7719
Epoch 35/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6774 - acc: 0.7716 - val_loss: 0.7008 - val_acc: 0.7602
Epoch 36/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6760 - acc: 0.7721 - val_loss: 0.7108 - val_acc: 0.7599
Epoch 37/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6726 - acc: 0.7732 - val_loss: 0.6827 - val_acc: 0.7713
Epoch 38/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6699 - acc: 0.7731 - val_loss: 0.6786 - val_acc: 0.7783
Epoch 39/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6662 - acc: 0.7760 - val_loss: 0.6575 - val_acc: 0.7782
Epoch 40/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6684 - acc: 0.7742 - val_loss: 0.6550 - val_acc: 0.7813
Epoch 41/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6568 - acc: 0.7780 - val_loss: 0.6797 - val_acc: 0.7724
Epoch 42/200
50000/50000 [==============================] - 67s 1ms/step - loss: 0.6551 - acc: 0.7801 - val_loss: 0.6717 - val_acc: 0.7758
Epoch 43/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6521 - acc: 0.7789 - val_loss: 0.6726 - val_acc: 0.7788
Epoch 44/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6541 - acc: 0.7814 - val_loss: 0.7399 - val_acc: 0.7667
Epoch 45/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6498 - acc: 0.7809 - val_loss: 0.7143 - val_acc: 0.7626
Epoch 46/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6536 - acc: 0.7778 - val_loss: 0.6495 - val_acc: 0.7803
Epoch 47/200
50000/50000 [==============================] - 74s 1ms/step - loss: 0.6470 - acc: 0.7809 - val_loss: 0.7007 - val_acc: 0.7748
Epoch 48/200
50000/50000 [==============================] - 100s 2ms/step - loss: 0.6434 - acc: 0.7843 - val_loss: 0.6613 - val_acc: 0.7801
Epoch 49/200
50000/50000 [==============================] - 79s 2ms/step - loss: 0.6506 - acc: 0.7816 - val_loss: 0.6553 - val_acc: 0.7803
Epoch 50/200
50000/50000 [==============================] - 67s 1ms/step - loss: 0.6376 - acc: 0.7843 - val_loss: 0.6582 - val_acc: 0.7849
Epoch 51/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6425 - acc: 0.7849 - val_loss: 0.6832 - val_acc: 0.7726
Epoch 52/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6375 - acc: 0.7856 - val_loss: 0.7613 - val_acc: 0.7696
Epoch 53/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6354 - acc: 0.7873 - val_loss: 0.6628 - val_acc: 0.7811
Epoch 54/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6316 - acc: 0.7879 - val_loss: 0.6811 - val_acc: 0.7718
Epoch 55/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6311 - acc: 0.7898 - val_loss: 0.6698 - val_acc: 0.7782
Epoch 56/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6299 - acc: 0.7908 - val_loss: 0.6482 - val_acc: 0.7828
Epoch 57/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6311 - acc: 0.7877 - val_loss: 0.6859 - val_acc: 0.7740
Epoch 58/200
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6302 - acc: 0.7904 - val_loss: 0.7807 - val_acc: 0.7714
Epoch 59/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6304 - acc: 0.7895 - val_loss: 0.6841 - val_acc: 0.7812
Epoch 60/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6347 - acc: 0.7908 - val_loss: 0.7186 - val_acc: 0.7714
Epoch 61/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6269 - acc: 0.7904 - val_loss: 0.6411 - val_acc: 0.7875
Epoch 62/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6252 - acc: 0.7904 - val_loss: 0.6497 - val_acc: 0.7798
Epoch 63/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6267 - acc: 0.7896 - val_loss: 0.6559 - val_acc: 0.7803
Epoch 64/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6232 - acc: 0.7908 - val_loss: 0.7186 - val_acc: 0.7654
Epoch 65/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6203 - acc: 0.7936 - val_loss: 0.6680 - val_acc: 0.7830
Epoch 66/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6209 - acc: 0.7937 - val_loss: 0.7206 - val_acc: 0.7785
Epoch 67/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6263 - acc: 0.7913 - val_loss: 0.6805 - val_acc: 0.7791
Epoch 68/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6226 - acc: 0.7939 - val_loss: 0.7585 - val_acc: 0.7580
Epoch 69/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6230 - acc: 0.7914 - val_loss: 0.6742 - val_acc: 0.7784
Epoch 70/200
50000/50000 [==============================] - 67s 1ms/step - loss: 0.6202 - acc: 0.7945 - val_loss: 0.6876 - val_acc: 0.7718
Epoch 71/200
50000/50000 [==============================] - 70s 1ms/step - loss: 0.6195 - acc: 0.7935 - val_loss: 0.6814 - val_acc: 0.7842
Epoch 72/200
50000/50000 [==============================] - 68s 1ms/step - loss: 0.6210 - acc: 0.7925 - val_loss: 0.6796 - val_acc: 0.7776
Epoch 73/200
50000/50000 [==============================] - 67s 1ms/step - loss: 0.6168 - acc: 0.7921 - val_loss: 0.6536 - val_acc: 0.7866
Epoch 74/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6192 - acc: 0.7936 - val_loss: 0.7155 - val_acc: 0.7722
Epoch 75/200
50000/50000 [==============================] - 67s 1ms/step - loss: 0.6180 - acc: 0.7948 - val_loss: 0.7243 - val_acc: 0.7641
Epoch 76/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6145 - acc: 0.7953 - val_loss: 0.6715 - val_acc: 0.7836
Epoch 77/200
50000/50000 [==============================] - 70s 1ms/step - loss: 0.6158 - acc: 0.7939 - val_loss: 0.6464 - val_acc: 0.7837
Epoch 78/200
50000/50000 [==============================] - 74s 1ms/step - loss: 0.6216 - acc: 0.7925 - val_loss: 0.7331 - val_acc: 0.7833
Epoch 79/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6231 - acc: 0.7925 - val_loss: 0.6782 - val_acc: 0.7765
Epoch 80/200
50000/50000 [==============================] - 68s 1ms/step - loss: 0.6158 - acc: 0.7926 - val_loss: 0.6702 - val_acc: 0.7754
Epoch 81/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6225 - acc: 0.7920 - val_loss: 0.7241 - val_acc: 0.7846
Epoch 82/200
50000/50000 [==============================] - 73s 1ms/step - loss: 0.6198 - acc: 0.7934 - val_loss: 0.6784 - val_acc: 0.7761
Epoch 83/200
50000/50000 [==============================] - 68s 1ms/step - loss: 0.6178 - acc: 0.7945 - val_loss: 0.6568 - val_acc: 0.7894
Epoch 84/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6147 - acc: 0.7953 - val_loss: 0.6626 - val_acc: 0.7829
Epoch 85/200
50000/50000 [==============================] - 69s 1ms/step - loss: 0.6167 - acc: 0.7961 - val_loss: 0.6878 - val_acc: 0.7736
Epoch 86/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6175 - acc: 0.7949 - val_loss: 0.6971 - val_acc: 0.7704
Epoch 87/200
50000/50000 [==============================] - 67s 1ms/step - loss: 0.6177 - acc: 0.7926 - val_loss: 0.8191 - val_acc: 0.7553
Epoch 88/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6161 - acc: 0.7946 - val_loss: 0.6893 - val_acc: 0.7808
Epoch 89/200
50000/50000 [==============================] - 68s 1ms/step - loss: 0.6189 - acc: 0.7943 - val_loss: 0.7294 - val_acc: 0.7600
Epoch 90/200
50000/50000 [==============================] - 70s 1ms/step - loss: 0.6204 - acc: 0.7921 - val_loss: 0.6388 - val_acc: 0.7883
Epoch 91/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6226 - acc: 0.7904 - val_loss: 0.7076 - val_acc: 0.7741
Epoch 92/200
50000/50000 [==============================] - 67s 1ms/step - loss: 0.6234 - acc: 0.7925 - val_loss: 0.7007 - val_acc: 0.7874
Epoch 93/200
50000/50000 [==============================] - 82s 2ms/step - loss: 0.6256 - acc: 0.7932 - val_loss: 0.6881 - val_acc: 0.7782
Epoch 94/200
50000/50000 [==============================] - 89s 2ms/step - loss: 0.6212 - acc: 0.7952 - val_loss: 0.6546 - val_acc: 0.7863
Epoch 95/200
50000/50000 [==============================] - 65s 1ms/step - loss: 0.6249 - acc: 0.7915 - val_loss: 0.6907 - val_acc: 0.7757
Epoch 96/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6258 - acc: 0.7930 - val_loss: 0.6691 - val_acc: 0.7829
Epoch 97/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6301 - acc: 0.7898 - val_loss: 0.6886 - val_acc: 0.7776
Epoch 98/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6235 - acc: 0.7932 - val_loss: 0.6926 - val_acc: 0.7776
Epoch 99/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6301 - acc: 0.7926 - val_loss: 0.6989 - val_acc: 0.7899
Epoch 100/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6284 - acc: 0.7922 - val_loss: 0.6769 - val_acc: 0.7812
Epoch 101/200
50000/50000 [==============================] - 66s 1ms/step - loss: 0.6235 - acc: 0.7929 - val_loss: 0.6980 - val_acc: 0.7750
Epoch 102/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6287 - acc: 0.7927 - val_loss: 0.7027 - val_acc: 0.7748
Epoch 103/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6258 - acc: 0.7936 - val_loss: 0.8148 - val_acc: 0.7393
Epoch 104/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6235 - acc: 0.7941 - val_loss: 0.7318 - val_acc: 0.7568
Epoch 105/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6278 - acc: 0.7930 - val_loss: 0.6501 - val_acc: 0.7894
Epoch 106/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6270 - acc: 0.7933 - val_loss: 0.7775 - val_acc: 0.7530
Epoch 107/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6307 - acc: 0.7918 - val_loss: 0.7422 - val_acc: 0.7707
Epoch 108/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6281 - acc: 0.7926 - val_loss: 0.7413 - val_acc: 0.7690
Epoch 109/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6296 - acc: 0.7926 - val_loss: 0.6905 - val_acc: 0.7742
Epoch 110/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6375 - acc: 0.7900 - val_loss: 0.7076 - val_acc: 0.7688
Epoch 111/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6298 - acc: 0.7918 - val_loss: 0.7651 - val_acc: 0.7714
Epoch 112/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6383 - acc: 0.7902 - val_loss: 0.7171 - val_acc: 0.7643
Epoch 113/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6382 - acc: 0.7898 - val_loss: 0.7904 - val_acc: 0.7488
Epoch 114/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6389 - acc: 0.7882 - val_loss: 0.7126 - val_acc: 0.7762
Epoch 115/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6346 - acc: 0.7911 - val_loss: 0.7216 - val_acc: 0.7701
Epoch 116/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6442 - acc: 0.7887 - val_loss: 0.7013 - val_acc: 0.7820
Epoch 117/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6461 - acc: 0.7896 - val_loss: 0.6856 - val_acc: 0.7799
Epoch 118/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6356 - acc: 0.7898 - val_loss: 0.6548 - val_acc: 0.7868
Epoch 119/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6478 - acc: 0.7876 - val_loss: 0.7631 - val_acc: 0.7712
Epoch 120/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6446 - acc: 0.7902 - val_loss: 0.7693 - val_acc: 0.7547
Epoch 121/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6470 - acc: 0.7880 - val_loss: 0.7759 - val_acc: 0.7583
Epoch 122/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6428 - acc: 0.7879 - val_loss: 0.6957 - val_acc: 0.7737
Epoch 123/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6531 - acc: 0.7880 - val_loss: 0.7328 - val_acc: 0.7627
Epoch 124/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6470 - acc: 0.7876 - val_loss: 0.7134 - val_acc: 0.7727
Epoch 125/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6548 - acc: 0.7869 - val_loss: 0.7521 - val_acc: 0.7571
Epoch 126/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6504 - acc: 0.7857 - val_loss: 0.8003 - val_acc: 0.7621
Epoch 127/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6513 - acc: 0.7888 - val_loss: 0.7142 - val_acc: 0.7678
Epoch 128/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6524 - acc: 0.7882 - val_loss: 0.7744 - val_acc: 0.7724
Epoch 129/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6577 - acc: 0.7842 - val_loss: 0.7606 - val_acc: 0.7579
Epoch 130/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6573 - acc: 0.7861 - val_loss: 0.7907 - val_acc: 0.7450
Epoch 131/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6636 - acc: 0.7864 - val_loss: 0.7060 - val_acc: 0.7737
Epoch 132/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6628 - acc: 0.7850 - val_loss: 0.7712 - val_acc: 0.7544
Epoch 133/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6689 - acc: 0.7827 - val_loss: 0.7192 - val_acc: 0.7742
Epoch 134/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6774 - acc: 0.7810 - val_loss: 0.7634 - val_acc: 0.7517
Epoch 135/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6671 - acc: 0.7820 - val_loss: 0.7564 - val_acc: 0.7738
Epoch 136/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6761 - acc: 0.7808 - val_loss: 0.7722 - val_acc: 0.7611
Epoch 137/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6728 - acc: 0.7821 - val_loss: 0.7503 - val_acc: 0.7545
Epoch 138/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6831 - acc: 0.7792 - val_loss: 0.8298 - val_acc: 0.7505
Epoch 139/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6839 - acc: 0.7781 - val_loss: 0.8035 - val_acc: 0.7416
Epoch 140/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.6897 - acc: 0.7768 - val_loss: 0.8253 - val_acc: 0.7510
Epoch 141/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6861 - acc: 0.7781 - val_loss: 0.7982 - val_acc: 0.7509
Epoch 142/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6964 - acc: 0.7761 - val_loss: 0.7692 - val_acc: 0.7685
Epoch 143/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7028 - acc: 0.7737 - val_loss: 0.8964 - val_acc: 0.7271
Epoch 144/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6961 - acc: 0.7731 - val_loss: 0.7064 - val_acc: 0.7665
Epoch 145/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7094 - acc: 0.7727 - val_loss: 0.7822 - val_acc: 0.7500
Epoch 146/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7115 - acc: 0.7692 - val_loss: 0.8155 - val_acc: 0.7516
Epoch 147/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7126 - acc: 0.7689 - val_loss: 0.8281 - val_acc: 0.7541
Epoch 148/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7120 - acc: 0.7729 - val_loss: 0.7082 - val_acc: 0.7691
Epoch 149/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7148 - acc: 0.7688 - val_loss: 1.0096 - val_acc: 0.6582
Epoch 150/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7236 - acc: 0.7689 - val_loss: 0.7023 - val_acc: 0.7738
Epoch 151/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7297 - acc: 0.7656 - val_loss: 0.8528 - val_acc: 0.7455
Epoch 152/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7372 - acc: 0.7633 - val_loss: 0.7569 - val_acc: 0.7421
Epoch 153/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7413 - acc: 0.7631 - val_loss: 0.7381 - val_acc: 0.7628
Epoch 154/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7531 - acc: 0.7577 - val_loss: 0.8923 - val_acc: 0.7406
Epoch 155/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7513 - acc: 0.7597 - val_loss: 0.8897 - val_acc: 0.7285
Epoch 156/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7552 - acc: 0.7584 - val_loss: 0.7139 - val_acc: 0.7694
Epoch 157/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7651 - acc: 0.7545 - val_loss: 0.7056 - val_acc: 0.7726
Epoch 158/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7752 - acc: 0.7527 - val_loss: 0.7987 - val_acc: 0.7429
Epoch 159/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7764 - acc: 0.7510 - val_loss: 0.8902 - val_acc: 0.7100
Epoch 160/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7920 - acc: 0.7478 - val_loss: 0.7887 - val_acc: 0.7423
Epoch 161/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7887 - acc: 0.7503 - val_loss: 0.7392 - val_acc: 0.7664
Epoch 162/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8063 - acc: 0.7450 - val_loss: 1.0207 - val_acc: 0.6778
Epoch 163/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8027 - acc: 0.7458 - val_loss: 0.9223 - val_acc: 0.7200
Epoch 164/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8107 - acc: 0.7444 - val_loss: 0.9642 - val_acc: 0.7060
Epoch 165/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8242 - acc: 0.7383 - val_loss: 0.9241 - val_acc: 0.7191
Epoch 166/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8167 - acc: 0.7411 - val_loss: 0.9396 - val_acc: 0.7139
Epoch 167/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8306 - acc: 0.7391 - val_loss: 0.9059 - val_acc: 0.7039
Epoch 168/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8368 - acc: 0.7377 - val_loss: 0.8922 - val_acc: 0.7308
Epoch 169/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8488 - acc: 0.7310 - val_loss: 0.9288 - val_acc: 0.7160
Epoch 170/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8611 - acc: 0.7301 - val_loss: 0.8390 - val_acc: 0.7259
Epoch 171/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8668 - acc: 0.7274 - val_loss: 0.9507 - val_acc: 0.6912
Epoch 172/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8652 - acc: 0.7258 - val_loss: 0.9619 - val_acc: 0.7216
Epoch 173/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8756 - acc: 0.7224 - val_loss: 0.8233 - val_acc: 0.7444
Epoch 174/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8820 - acc: 0.7211 - val_loss: 0.7449 - val_acc: 0.7586
Epoch 175/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8864 - acc: 0.7229 - val_loss: 0.8631 - val_acc: 0.7223
Epoch 176/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8947 - acc: 0.7178 - val_loss: 0.9967 - val_acc: 0.6882
Epoch 177/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9037 - acc: 0.7155 - val_loss: 0.8722 - val_acc: 0.7269
Epoch 178/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9231 - acc: 0.7116 - val_loss: 0.8205 - val_acc: 0.7349
Epoch 179/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9329 - acc: 0.7089 - val_loss: 0.8434 - val_acc: 0.7103
Epoch 180/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9388 - acc: 0.7047 - val_loss: 0.8886 - val_acc: 0.7240
Epoch 181/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9426 - acc: 0.7051 - val_loss: 0.8819 - val_acc: 0.7121
Epoch 182/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9510 - acc: 0.7011 - val_loss: 1.0046 - val_acc: 0.6887
Epoch 183/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9598 - acc: 0.7008 - val_loss: 0.8566 - val_acc: 0.7325
Epoch 184/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9749 - acc: 0.6979 - val_loss: 1.0115 - val_acc: 0.7009
Epoch 185/200
50000/50000 [==============================] - 64s 1ms/step - loss: 0.9812 - acc: 0.6903 - val_loss: 0.9432 - val_acc: 0.6993
Epoch 186/200
50000/50000 [==============================] - 63s 1ms/step - loss: 0.9821 - acc: 0.6918 - val_loss: 0.8205 - val_acc: 0.7397
Epoch 187/200
50000/50000 [==============================] - 63s 1ms/step - loss: 1.0078 - acc: 0.6827 - val_loss: 1.1344 - val_acc: 0.6733
Epoch 188/200
50000/50000 [==============================] - 63s 1ms/step - loss: 1.0165 - acc: 0.6823 - val_loss: 1.0071 - val_acc: 0.6637
Epoch 189/200
50000/50000 [==============================] - 63s 1ms/step - loss: 1.0242 - acc: 0.6793 - val_loss: 1.1800 - val_acc: 0.6498
Epoch 190/200
50000/50000 [==============================] - 63s 1ms/step - loss: 1.0402 - acc: 0.6732 - val_loss: 1.1132 - val_acc: 0.6317
Epoch 191/200
50000/50000 [==============================] - 65s 1ms/step - loss: 1.0433 - acc: 0.6737 - val_loss: 1.1406 - val_acc: 0.6701
Epoch 192/200
50000/50000 [==============================] - 64s 1ms/step - loss: 1.0441 - acc: 0.6746 - val_loss: 1.0199 - val_acc: 0.6929
Epoch 193/200
50000/50000 [==============================] - 65s 1ms/step - loss: 1.0518 - acc: 0.6716 - val_loss: 0.9363 - val_acc: 0.6963
Epoch 194/200
50000/50000 [==============================] - 65s 1ms/step - loss: 1.0581 - acc: 0.6710 - val_loss: 1.0933 - val_acc: 0.6657
Epoch 195/200
50000/50000 [==============================] - 65s 1ms/step - loss: 1.0832 - acc: 0.6630 - val_loss: 1.0265 - val_acc: 0.6638
Epoch 196/200
50000/50000 [==============================] - 65s 1ms/step - loss: 1.0811 - acc: 0.6619 - val_loss: 0.9917 - val_acc: 0.6865
Epoch 197/200
50000/50000 [==============================] - 64s 1ms/step - loss: 1.0982 - acc: 0.6583 - val_loss: 1.1057 - val_acc: 0.6768
Epoch 198/200
50000/50000 [==============================] - 64s 1ms/step - loss: 1.0940 - acc: 0.6583 - val_loss: 1.0699 - val_acc: 0.6927
Epoch 199/200
50000/50000 [==============================] - 65s 1ms/step - loss: 1.0978 - acc: 0.6544 - val_loss: 1.2466 - val_acc: 0.6230
Epoch 200/200
50000/50000 [==============================] - 64s 1ms/step - loss: 1.1230 - acc: 0.6500 - val_loss: 1.0537 - val_acc: 0.6314
Saved trained model at E:\Master\Semester 2\ML\Homeworks\Pure Code\saved_models\keras_cifar10_trained_model.h5
10000/10000 [==============================] - 3s 326us/step
Test loss: 1.053674542236328
Test accuracy: 0.6314

#################################################

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 32, 32, 32)        896
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 32, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 16, 32)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496
_________________________________________________________________
activation_1 (Activation)    (None, 16, 16, 64)        0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928
_________________________________________________________________
activation_2 (Activation)    (None, 14, 14, 64)        0
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0
_________________________________________________________________
dropout_3 (Dropout)          (None, 7, 7, 64)          0
_________________________________________________________________
flatten_1 (Flatten)          (None, 3136)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               1606144
_________________________________________________________________
dropout_4 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_2 (Dense)              (None, 10)                5130
=================================================================
Total params: 1,676,842
Trainable params: 1,676,842
Non-trainable params: 0
_________________________________________________________________
None
Augmented Data Training.
e:\Master\Semester 2\ML\Homeworks\Pure Code\ML\FinalProject\CNN3.PY:114: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.
  callbacks = [fmon]
e:\Master\Semester 2\ML\Homeworks\Pure Code\ML\FinalProject\CNN3.PY:114: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., verbose=1, callbacks=[<kerutils..., steps_per_epoch=1562, epochs=400)`
  callbacks = [fmon]
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train begin: 2019-07-07 18:46:14
Stop file: stop_training_file.keras (create this file to stop training gracefully)
Pause file: pause_training_file.keras (create this file to pause training and view graphs)
do_validation = True
epochs = 400
metrics = ['loss', 'acc', 'val_loss', 'val_acc']
steps = 1562
verbose = 1
Epoch 1/400
2019-07-07 18:46:14.995284: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-07-07 18:46:16.009437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:03:00.0
totalMemory: 4.00GiB freeMemory: 3.35GiB
2019-07-07 18:46:16.013036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-07-07 18:46:17.123537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-07 18:46:17.127529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-07-07 18:46:17.129781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-07-07 18:46:17.132548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with
3058 MB memory) -> physical GPU (device: 0, name: GeForce 840M, pci bus id: 0000:03:00.0, compute capability: 5.0)
2019-07-07 18:46:18.097389: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
1562/1562 [==============================] - 88s 56ms/step - loss: 1.7557 - acc: 0.3552 - val_loss: 1.4263 - val_acc: 0.4785
Epoch 2/400
1562/1562 [==============================] - 84s 54ms/step - loss: 1.4070 - acc: 0.4911 - val_loss: 1.2125 - val_acc: 0.5640
Epoch 3/400
1562/1562 [==============================] - 85s 54ms/step - loss: 1.2620 - acc: 0.5460 - val_loss: 1.0488 - val_acc: 0.6303
Epoch 4/400
1562/1562 [==============================] - 85s 54ms/step - loss: 1.1673 - acc: 0.5824 - val_loss: 0.9693 - val_acc: 0.6633
Epoch 5/400
1562/1562 [==============================] - 85s 55ms/step - loss: 1.0767 - acc: 0.6170 - val_loss: 0.9056 - val_acc: 0.6789
.Epoch 6/400
1562/1562 [==============================] - 86s 55ms/step - loss: 1.0214 - acc: 0.6386 - val_loss: 0.8680 - val_acc: 0.6955
Epoch 7/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.9657 - acc: 0.6586 - val_loss: 0.7848 - val_acc: 0.7297
Epoch 8/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.9272 - acc: 0.6735 - val_loss: 0.7697 - val_acc: 0.7346
Epoch 9/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.8913 - acc: 0.6859 - val_loss: 0.7341 - val_acc: 0.7451
.Epoch 10/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.8631 - acc: 0.6979 - val_loss: 0.7072 - val_acc: 0.7554
Epoch 11/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.8368 - acc: 0.7020 - val_loss: 0.6894 - val_acc: 0.7609
Epoch 12/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.8106 - acc: 0.7133 - val_loss: 0.6857 - val_acc: 0.7620
Epoch 13/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.7831 - acc: 0.7264 - val_loss: 0.6511 - val_acc: 0.7734
.Epoch 14/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.7707 - acc: 0.7308 - val_loss: 0.6382 - val_acc: 0.7831
Epoch 15/400
1562/1562 [==============================] - 97s 62ms/step - loss: 0.7499 - acc: 0.7358 - val_loss: 0.6417 - val_acc: 0.7819
Epoch 16/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.7333 - acc: 0.7425 - val_loss: 0.6393 - val_acc: 0.7823
Epoch 17/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.7188 - acc: 0.7465 - val_loss: 0.6643 - val_acc: 0.7697
.Epoch 18/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.7008 - acc: 0.7545 - val_loss: 0.6020 - val_acc: 0.7955
Epoch 19/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.6948 - acc: 0.7559 - val_loss: 0.5921 - val_acc: 0.7987
Epoch 20/400
1562/1562 [==============================] - 89s 57ms/step - loss: 0.6809 - acc: 0.7621 - val_loss: 0.5776 - val_acc: 0.8057
Epoch 21/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.6665 - acc: 0.7672 - val_loss: 0.6114 - val_acc: 0.7929
.05% epoch=20, acc=0.767111, loss=0.666641, val_acc=0.792900, val_loss=0.611374, time=0.505 hours
Epoch 22/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.6570 - acc: 0.7693 - val_loss: 0.5720 - val_acc: 0.8048
Epoch 23/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.6448 - acc: 0.7732 - val_loss: 0.6068 - val_acc: 0.7988
Epoch 24/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.6372 - acc: 0.7775 - val_loss: 0.5737 - val_acc: 0.8055
Epoch 25/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.6242 - acc: 0.7828 - val_loss: 0.6063 - val_acc: 0.7987
.Epoch 26/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.6218 - acc: 0.7820 - val_loss: 0.5828 - val_acc: 0.8105
Epoch 27/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.6128 - acc: 0.7860 - val_loss: 0.5602 - val_acc: 0.8111
Epoch 28/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.6003 - acc: 0.7915 - val_loss: 0.5634 - val_acc: 0.8107
Epoch 29/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.5940 - acc: 0.7923 - val_loss: 0.5716 - val_acc: 0.8120
.Epoch 30/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.5869 - acc: 0.7949 - val_loss: 0.5510 - val_acc: 0.8156
Epoch 31/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.5827 - acc: 0.7964 - val_loss: 0.5385 - val_acc: 0.8195
Epoch 32/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.5724 - acc: 0.7999 - val_loss: 0.5627 - val_acc: 0.8116
Epoch 33/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.5677 - acc: 0.8019 - val_loss: 0.5394 - val_acc: 0.8205
.Epoch 34/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.5617 - acc: 0.8025 - val_loss: 0.5201 - val_acc: 0.8244
Epoch 35/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.5599 - acc: 0.8041 - val_loss: 0.4947 - val_acc: 0.8324
Epoch 36/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.5407 - acc: 0.8116 - val_loss: 0.5171 - val_acc: 0.8289
Epoch 37/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.5395 - acc: 0.8126 - val_loss: 0.5400 - val_acc: 0.8222
.Epoch 38/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.5387 - acc: 0.8099 - val_loss: 0.5128 - val_acc: 0.8284
Epoch 39/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.5345 - acc: 0.8117 - val_loss: 0.5137 - val_acc: 0.8274
Epoch 40/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.5285 - acc: 0.8149 - val_loss: 0.5053 - val_acc: 0.8327
Epoch 41/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.5208 - acc: 0.8173 - val_loss: 0.5093 - val_acc: 0.8295
.10% epoch=40, acc=0.817363, loss=0.520696, val_acc=0.829500, val_loss=0.509332, time=0.979 hours
Epoch 42/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.5183 - acc: 0.8164 - val_loss: 0.5049 - val_acc: 0.8322
Epoch 43/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.5122 - acc: 0.8205 - val_loss: 0.4839 - val_acc: 0.8373
Epoch 44/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.5044 - acc: 0.8231 - val_loss: 0.4872 - val_acc: 0.8353
Epoch 45/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.5022 - acc: 0.8235 - val_loss: 0.5026 - val_acc: 0.8351
.Epoch 46/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.5020 - acc: 0.8226 - val_loss: 0.5030 - val_acc: 0.8315
Epoch 47/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4908 - acc: 0.8268 - val_loss: 0.4990 - val_acc: 0.8346
Epoch 48/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.4838 - acc: 0.8283 - val_loss: 0.5110 - val_acc: 0.8344
Epoch 49/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.4820 - acc: 0.8302 - val_loss: 0.4871 - val_acc: 0.8411
.Epoch 50/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4773 - acc: 0.8327 - val_loss: 0.4905 - val_acc: 0.8338
Epoch 51/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.4729 - acc: 0.8337 - val_loss: 0.4832 - val_acc: 0.8403
Epoch 52/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4712 - acc: 0.8334 - val_loss: 0.4756 - val_acc: 0.8419
Epoch 53/400
1562/1562 [==============================] - 88s 57ms/step - loss: 0.4629 - acc: 0.8352 - val_loss: 0.4938 - val_acc: 0.8366
.Epoch 54/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.4634 - acc: 0.8375 - val_loss: 0.4899 - val_acc: 0.8362
Epoch 55/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4589 - acc: 0.8367 - val_loss: 0.4770 - val_acc: 0.8409
Epoch 56/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4519 - acc: 0.8418 - val_loss: 0.4850 - val_acc: 0.8407
Epoch 57/400
1562/1562 [==============================] - 93s 60ms/step - loss: 0.4545 - acc: 0.8410 - val_loss: 0.4825 - val_acc: 0.8413
.Epoch 58/400
1562/1562 [==============================] - 99s 63ms/step - loss: 0.4416 - acc: 0.8436 - val_loss: 0.4950 - val_acc: 0.8396
Epoch 59/400
1562/1562 [==============================] - 94s 60ms/step - loss: 0.4450 - acc: 0.8434 - val_loss: 0.4848 - val_acc: 0.8419
Epoch 60/400
1562/1562 [==============================] - 92s 59ms/step - loss: 0.4402 - acc: 0.8443 - val_loss: 0.4941 - val_acc: 0.8405
Epoch 61/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.4440 - acc: 0.8426 - val_loss: 0.5088 - val_acc: 0.8353
.15% epoch=60, acc=0.842619, loss=0.443988, val_acc=0.835300, val_loss=0.508759, time=1.467 hours
Epoch 62/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.4275 - acc: 0.8481 - val_loss: 0.4876 - val_acc: 0.8411
Epoch 63/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4293 - acc: 0.8493 - val_loss: 0.4728 - val_acc: 0.8454
Epoch 64/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.4296 - acc: 0.8481 - val_loss: 0.5253 - val_acc: 0.8356
Epoch 65/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4298 - acc: 0.8488 - val_loss: 0.4908 - val_acc: 0.8405
.Epoch 66/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4229 - acc: 0.8506 - val_loss: 0.4674 - val_acc: 0.8479
Epoch 67/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4274 - acc: 0.8504 - val_loss: 0.4817 - val_acc: 0.8456
Epoch 68/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4183 - acc: 0.8539 - val_loss: 0.4690 - val_acc: 0.8468
Epoch 69/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4172 - acc: 0.8518 - val_loss: 0.4695 - val_acc: 0.8458
.Epoch 70/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4097 - acc: 0.8555 - val_loss: 0.4937 - val_acc: 0.8464
Epoch 71/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4128 - acc: 0.8552 - val_loss: 0.4770 - val_acc: 0.8474
Epoch 72/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4079 - acc: 0.8557 - val_loss: 0.4853 - val_acc: 0.8459
Epoch 73/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4022 - acc: 0.8574 - val_loss: 0.4572 - val_acc: 0.8504
.Epoch 74/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.4017 - acc: 0.8592 - val_loss: 0.4840 - val_acc: 0.8447
Epoch 75/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3972 - acc: 0.8596 - val_loss: 0.4775 - val_acc: 0.8454
Epoch 76/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3989 - acc: 0.8610 - val_loss: 0.4555 - val_acc: 0.8518
Epoch 77/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3912 - acc: 0.8601 - val_loss: 0.4666 - val_acc: 0.8475
.Epoch 78/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3923 - acc: 0.8618 - val_loss: 0.4950 - val_acc: 0.8417
Epoch 79/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3911 - acc: 0.8614 - val_loss: 0.4623 - val_acc: 0.8509
Epoch 80/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3870 - acc: 0.8642 - val_loss: 0.4785 - val_acc: 0.8466
Epoch 81/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3886 - acc: 0.8638 - val_loss: 0.4925 - val_acc: 0.8425
.20% epoch=80, acc=0.863773, loss=0.388592, val_acc=0.842500, val_loss=0.492517, time=1.945 hours
Epoch 82/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3799 - acc: 0.8663 - val_loss: 0.4585 - val_acc: 0.8523
Epoch 83/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3827 - acc: 0.8646 - val_loss: 0.4774 - val_acc: 0.8452
Epoch 84/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3718 - acc: 0.8679 - val_loss: 0.4695 - val_acc: 0.8511
Epoch 85/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3784 - acc: 0.8677 - val_loss: 0.4695 - val_acc: 0.8541
.Epoch 86/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3773 - acc: 0.8682 - val_loss: 0.4622 - val_acc: 0.8535
Epoch 87/400
1562/1562 [==============================] - 89s 57ms/step - loss: 0.3740 - acc: 0.8679 - val_loss: 0.4652 - val_acc: 0.8489
Epoch 88/400
1562/1562 [==============================] - 89s 57ms/step - loss: 0.3735 - acc: 0.8664 - val_loss: 0.4630 - val_acc: 0.8525
Epoch 89/400
1562/1562 [==============================] - 89s 57ms/step - loss: 0.3694 - acc: 0.8696 - val_loss: 0.4464 - val_acc: 0.8566
.Epoch 90/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3682 - acc: 0.8704 - val_loss: 0.4601 - val_acc: 0.8554
Epoch 91/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3661 - acc: 0.8705 - val_loss: 0.4650 - val_acc: 0.8525
Epoch 92/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3675 - acc: 0.8712 - val_loss: 0.4628 - val_acc: 0.8526
Epoch 93/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3577 - acc: 0.8730 - val_loss: 0.4560 - val_acc: 0.8554
.Epoch 94/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3624 - acc: 0.8717 - val_loss: 0.4602 - val_acc: 0.8539
Epoch 95/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3601 - acc: 0.8740 - val_loss: 0.4723 - val_acc: 0.8539
Epoch 96/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3549 - acc: 0.8747 - val_loss: 0.4529 - val_acc: 0.8566
Epoch 97/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3479 - acc: 0.8772 - val_loss: 0.4545 - val_acc: 0.8555
.Epoch 98/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3574 - acc: 0.8740 - val_loss: 0.4679 - val_acc: 0.8524
Epoch 99/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3562 - acc: 0.8736 - val_loss: 0.4489 - val_acc: 0.8527
Epoch 100/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3485 - acc: 0.8758 - val_loss: 0.4752 - val_acc: 0.8500
Epoch 101/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3494 - acc: 0.8759 - val_loss: 0.4598 - val_acc: 0.8551
.25% epoch=100, acc=0.875901, loss=0.349303, val_acc=0.855100, val_loss=0.459750, time=2.430 hours
Epoch 102/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3506 - acc: 0.8774 - val_loss: 0.4543 - val_acc: 0.8573
Epoch 103/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3427 - acc: 0.8777 - val_loss: 0.4839 - val_acc: 0.8512
Epoch 104/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3409 - acc: 0.8790 - val_loss: 0.4778 - val_acc: 0.8538
Epoch 105/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3457 - acc: 0.8784 - val_loss: 0.4560 - val_acc: 0.8565
.Epoch 106/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3392 - acc: 0.8805 - val_loss: 0.4624 - val_acc: 0.8558
Epoch 107/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3418 - acc: 0.8794 - val_loss: 0.4527 - val_acc: 0.8563
Epoch 108/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3402 - acc: 0.8803 - val_loss: 0.4496 - val_acc: 0.8621
Epoch 109/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3370 - acc: 0.8805 - val_loss: 0.4609 - val_acc: 0.8570
.Epoch 110/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3329 - acc: 0.8820 - val_loss: 0.4631 - val_acc: 0.8574
Epoch 111/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3289 - acc: 0.8826 - val_loss: 0.4712 - val_acc: 0.8572
Epoch 112/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3303 - acc: 0.8822 - val_loss: 0.4636 - val_acc: 0.8594
Epoch 113/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3253 - acc: 0.8846 - val_loss: 0.4496 - val_acc: 0.8614
.Epoch 114/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3282 - acc: 0.8844 - val_loss: 0.4401 - val_acc: 0.8627
Epoch 115/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3347 - acc: 0.8809 - val_loss: 0.4603 - val_acc: 0.8570
Epoch 116/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3228 - acc: 0.8861 - val_loss: 0.4421 - val_acc: 0.8616
Epoch 117/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3231 - acc: 0.8856 - val_loss: 0.4446 - val_acc: 0.8618
.Epoch 118/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3305 - acc: 0.8839 - val_loss: 0.4609 - val_acc: 0.8585
Epoch 119/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3232 - acc: 0.8855 - val_loss: 0.4787 - val_acc: 0.8512
Epoch 120/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3196 - acc: 0.8855 - val_loss: 0.4766 - val_acc: 0.8544
Epoch 121/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3251 - acc: 0.8845 - val_loss: 0.4505 - val_acc: 0.8610
.30% epoch=120, acc=0.884506, loss=0.325085, val_acc=0.861000, val_loss=0.450464, time=2.911 hours
Epoch 122/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3170 - acc: 0.8875 - val_loss: 0.4494 - val_acc: 0.8590
Epoch 123/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3192 - acc: 0.8868 - val_loss: 0.4548 - val_acc: 0.8621
Epoch 124/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3202 - acc: 0.8869 - val_loss: 0.4496 - val_acc: 0.8625
Epoch 125/400
1562/1562 [==============================] - 89s 57ms/step - loss: 0.3203 - acc: 0.8871 - val_loss: 0.4503 - val_acc: 0.8590
.Epoch 126/400
1562/1562 [==============================] - 89s 57ms/step - loss: 0.3126 - acc: 0.8899 - val_loss: 0.4679 - val_acc: 0.8573
Epoch 127/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3107 - acc: 0.8913 - val_loss: 0.4451 - val_acc: 0.8622
Epoch 128/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3098 - acc: 0.8898 - val_loss: 0.4458 - val_acc: 0.8617
Epoch 129/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3136 - acc: 0.8885 - val_loss: 0.4569 - val_acc: 0.8583
.Epoch 130/400
1562/1562 [==============================] - 91s 59ms/step - loss: 0.3151 - acc: 0.8891 - val_loss: 0.4606 - val_acc: 0.8581
Epoch 131/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3111 - acc: 0.8891 - val_loss: 0.4501 - val_acc: 0.8613
Epoch 132/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3063 - acc: 0.8925 - val_loss: 0.4555 - val_acc: 0.8589
Epoch 133/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.3025 - acc: 0.8937 - val_loss: 0.4524 - val_acc: 0.8614
.Epoch 134/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3084 - acc: 0.8915 - val_loss: 0.4463 - val_acc: 0.8598
Epoch 135/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3030 - acc: 0.8947 - val_loss: 0.4579 - val_acc: 0.8594
Epoch 136/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3088 - acc: 0.8905 - val_loss: 0.4538 - val_acc: 0.8609
Epoch 137/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3029 - acc: 0.8932 - val_loss: 0.4528 - val_acc: 0.8623
.Epoch 138/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2970 - acc: 0.8935 - val_loss: 0.4528 - val_acc: 0.8583
Epoch 139/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.3046 - acc: 0.8921 - val_loss: 0.4465 - val_acc: 0.8638
Epoch 140/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.3001 - acc: 0.8941 - val_loss: 0.4504 - val_acc: 0.8610
Epoch 141/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.3005 - acc: 0.8939 - val_loss: 0.4507 - val_acc: 0.8622
.35% epoch=140, acc=0.893952, loss=0.300437, val_acc=0.862200, val_loss=0.450740, time=3.397 hours
Epoch 142/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.2992 - acc: 0.8952 - val_loss: 0.4734 - val_acc: 0.8555
Epoch 143/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2947 - acc: 0.8954 - val_loss: 0.4471 - val_acc: 0.8604
Epoch 144/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2916 - acc: 0.8962 - val_loss: 0.4439 - val_acc: 0.8641
Epoch 145/400
1562/1562 [==============================] - 1608s 1s/step - loss: 0.3002 - acc: 0.8928 - val_loss: 0.4440 - val_acc: 0.8624
.Epoch 146/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2971 - acc: 0.8959 - val_loss: 0.4534 - val_acc: 0.8585
Epoch 147/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2876 - acc: 0.8983 - val_loss: 0.4687 - val_acc: 0.8549
Epoch 148/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2918 - acc: 0.8964 - val_loss: 0.4561 - val_acc: 0.8587
Epoch 149/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2935 - acc: 0.8949 - val_loss: 0.4466 - val_acc: 0.8634
.Epoch 150/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2892 - acc: 0.8977 - val_loss: 0.4472 - val_acc: 0.8623
Epoch 151/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2881 - acc: 0.8975 - val_loss: 0.4722 - val_acc: 0.8575
Epoch 152/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2895 - acc: 0.8968 - val_loss: 0.4494 - val_acc: 0.8623
Epoch 153/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2849 - acc: 0.8995 - val_loss: 0.4480 - val_acc: 0.8618
.Epoch 154/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2884 - acc: 0.8984 - val_loss: 0.4599 - val_acc: 0.8593
Epoch 155/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2860 - acc: 0.8982 - val_loss: 0.4471 - val_acc: 0.8609
Epoch 156/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2929 - acc: 0.8962 - val_loss: 0.4483 - val_acc: 0.8613
Epoch 157/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2869 - acc: 0.8998 - val_loss: 0.4351 - val_acc: 0.8667
.Epoch 158/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2842 - acc: 0.9003 - val_loss: 0.4603 - val_acc: 0.8592
Epoch 159/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2780 - acc: 0.8997 - val_loss: 0.4590 - val_acc: 0.8590
Epoch 160/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2814 - acc: 0.8999 - val_loss: 0.4450 - val_acc: 0.8621
Epoch 161/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2781 - acc: 0.9020 - val_loss: 0.4401 - val_acc: 0.8654
.40% epoch=160, acc=0.902017, loss=0.278191, val_acc=0.865400, val_loss=0.440100, time=4.281 hours
Epoch 162/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2789 - acc: 0.9021 - val_loss: 0.4591 - val_acc: 0.8606
Epoch 163/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2850 - acc: 0.8992 - val_loss: 0.4572 - val_acc: 0.8598
Epoch 164/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2739 - acc: 0.9017 - val_loss: 0.4543 - val_acc: 0.8615
Epoch 165/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2798 - acc: 0.9017 - val_loss: 0.4656 - val_acc: 0.8591
.Epoch 166/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2725 - acc: 0.9041 - val_loss: 0.4517 - val_acc: 0.8604
Epoch 167/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2710 - acc: 0.9032 - val_loss: 0.4484 - val_acc: 0.8636
Epoch 168/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2744 - acc: 0.9036 - val_loss: 0.4436 - val_acc: 0.8665
Epoch 169/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2751 - acc: 0.9026 - val_loss: 0.4559 - val_acc: 0.8639
.Epoch 170/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2751 - acc: 0.9017 - val_loss: 0.4743 - val_acc: 0.8568
Epoch 171/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2700 - acc: 0.9040 - val_loss: 0.4535 - val_acc: 0.8645
Epoch 172/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2746 - acc: 0.9028 - val_loss: 0.4508 - val_acc: 0.8651
Epoch 173/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2719 - acc: 0.9036 - val_loss: 0.4490 - val_acc: 0.8612
.Epoch 174/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2686 - acc: 0.9053 - val_loss: 0.4591 - val_acc: 0.8620
Epoch 175/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2699 - acc: 0.9022 - val_loss: 0.4713 - val_acc: 0.8594
Epoch 176/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2699 - acc: 0.9034 - val_loss: 0.4545 - val_acc: 0.8642
Epoch 177/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2674 - acc: 0.9056 - val_loss: 0.4639 - val_acc: 0.8611
.Epoch 178/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2671 - acc: 0.9065 - val_loss: 0.4691 - val_acc: 0.8607
Epoch 179/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2609 - acc: 0.9071 - val_loss: 0.4464 - val_acc: 0.8666
Epoch 180/400
1562/1562 [==============================] - 84s 53ms/step - loss: 0.2617 - acc: 0.9070 - val_loss: 0.4692 - val_acc: 0.8574
Epoch 181/400
1562/1562 [==============================] - 84s 53ms/step - loss: 0.2683 - acc: 0.9065 - val_loss: 0.4680 - val_acc: 0.8602
.45% epoch=180, acc=0.906500, loss=0.268305, val_acc=0.860200, val_loss=0.468040, time=4.744 hours
Epoch 182/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2656 - acc: 0.9062 - val_loss: 0.4572 - val_acc: 0.8661
Epoch 183/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2626 - acc: 0.9072 - val_loss: 0.4794 - val_acc: 0.8555
Epoch 184/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2649 - acc: 0.9073 - val_loss: 0.4568 - val_acc: 0.8636
Epoch 185/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2564 - acc: 0.9087 - val_loss: 0.4582 - val_acc: 0.8630
.Epoch 186/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2580 - acc: 0.9079 - val_loss: 0.4428 - val_acc: 0.8655
Epoch 187/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2628 - acc: 0.9062 - val_loss: 0.4554 - val_acc: 0.8668
Epoch 188/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2614 - acc: 0.9081 - val_loss: 0.4614 - val_acc: 0.8632
Epoch 189/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2637 - acc: 0.9068 - val_loss: 0.4555 - val_acc: 0.8635
.Epoch 190/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2609 - acc: 0.9081 - val_loss: 0.4362 - val_acc: 0.8670
Epoch 191/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2559 - acc: 0.9099 - val_loss: 0.4519 - val_acc: 0.8650
Epoch 192/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2576 - acc: 0.9083 - val_loss: 0.4585 - val_acc: 0.8623
Epoch 193/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2629 - acc: 0.9070 - val_loss: 0.4384 - val_acc: 0.8645
.Epoch 194/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2601 - acc: 0.9070 - val_loss: 0.4548 - val_acc: 0.8620
Epoch 195/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2561 - acc: 0.9093 - val_loss: 0.4581 - val_acc: 0.8654
Epoch 196/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2571 - acc: 0.9077 - val_loss: 0.4594 - val_acc: 0.8626
Epoch 197/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2552 - acc: 0.9091 - val_loss: 0.4597 - val_acc: 0.8628
.Epoch 198/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2483 - acc: 0.9116 - val_loss: 0.4568 - val_acc: 0.8630
Epoch 199/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2542 - acc: 0.9090 - val_loss: 0.4515 - val_acc: 0.8661
Epoch 200/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2520 - acc: 0.9112 - val_loss: 0.4626 - val_acc: 0.8625
Epoch 201/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2536 - acc: 0.9112 - val_loss: 0.4698 - val_acc: 0.8605
.50% epoch=200, acc=0.911243, loss=0.253602, val_acc=0.860500, val_loss=0.469830, time=5.214 hours
Epoch 202/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2513 - acc: 0.9106 - val_loss: 0.4690 - val_acc: 0.8615
Epoch 203/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2505 - acc: 0.9116 - val_loss: 0.4648 - val_acc: 0.8597
Epoch 204/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2507 - acc: 0.9109 - val_loss: 0.4490 - val_acc: 0.8648
Epoch 205/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2491 - acc: 0.9118 - val_loss: 0.4512 - val_acc: 0.8669
.Epoch 206/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2567 - acc: 0.9089 - val_loss: 0.4586 - val_acc: 0.8656
Epoch 207/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2499 - acc: 0.9111 - val_loss: 0.4555 - val_acc: 0.8639
Epoch 208/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2489 - acc: 0.9130 - val_loss: 0.4630 - val_acc: 0.8667
Epoch 209/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2502 - acc: 0.9100 - val_loss: 0.4588 - val_acc: 0.8637
.Epoch 210/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2525 - acc: 0.9110 - val_loss: 0.4534 - val_acc: 0.8681
Epoch 211/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.2438 - acc: 0.9122 - val_loss: 0.4592 - val_acc: 0.8666
Epoch 212/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.2477 - acc: 0.9130 - val_loss: 0.4412 - val_acc: 0.8685
Epoch 213/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2488 - acc: 0.9126 - val_loss: 0.4611 - val_acc: 0.8655
.Epoch 214/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2429 - acc: 0.9154 - val_loss: 0.4495 - val_acc: 0.8684
Epoch 215/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2449 - acc: 0.9130 - val_loss: 0.4572 - val_acc: 0.8643
Epoch 216/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2461 - acc: 0.9135 - val_loss: 0.4498 - val_acc: 0.8687
Epoch 217/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2451 - acc: 0.9139 - val_loss: 0.4527 - val_acc: 0.8662
.Epoch 218/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2427 - acc: 0.9138 - val_loss: 0.4625 - val_acc: 0.8642
Epoch 219/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2436 - acc: 0.9131 - val_loss: 0.4584 - val_acc: 0.8645
Epoch 220/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2454 - acc: 0.9132 - val_loss: 0.4470 - val_acc: 0.8668
Epoch 221/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.2411 - acc: 0.9151 - val_loss: 0.4452 - val_acc: 0.8688
.55% epoch=220, acc=0.915146, loss=0.241119, val_acc=0.868800, val_loss=0.445199, time=5.688 hours
Epoch 222/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2456 - acc: 0.9139 - val_loss: 0.4599 - val_acc: 0.8639
Epoch 223/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2429 - acc: 0.9147 - val_loss: 0.4795 - val_acc: 0.8597
Epoch 224/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2411 - acc: 0.9150 - val_loss: 0.4639 - val_acc: 0.8638
Epoch 225/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2412 - acc: 0.9142 - val_loss: 0.4492 - val_acc: 0.8659
.Epoch 226/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2437 - acc: 0.9138 - val_loss: 0.4559 - val_acc: 0.8654
Epoch 227/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2398 - acc: 0.9138 - val_loss: 0.4540 - val_acc: 0.8668
Epoch 228/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2431 - acc: 0.9139 - val_loss: 0.4438 - val_acc: 0.8675
Epoch 229/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2413 - acc: 0.9148 - val_loss: 0.4498 - val_acc: 0.8662
.Epoch 230/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2389 - acc: 0.9166 - val_loss: 0.4541 - val_acc: 0.8645
Epoch 231/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2390 - acc: 0.9149 - val_loss: 0.4603 - val_acc: 0.8632
Epoch 232/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2415 - acc: 0.9146 - val_loss: 0.4412 - val_acc: 0.8693
Epoch 233/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2369 - acc: 0.9155 - val_loss: 0.4513 - val_acc: 0.8677
.Epoch 234/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.2353 - acc: 0.9173 - val_loss: 0.4518 - val_acc: 0.8687
Epoch 235/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2398 - acc: 0.9129 - val_loss: 0.4462 - val_acc: 0.8699
Epoch 236/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2373 - acc: 0.9165 - val_loss: 0.4357 - val_acc: 0.8726
Epoch 237/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2364 - acc: 0.9160 - val_loss: 0.4445 - val_acc: 0.8692
.Epoch 238/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.2376 - acc: 0.9161 - val_loss: 0.4504 - val_acc: 0.8689
Epoch 239/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2372 - acc: 0.9165 - val_loss: 0.4558 - val_acc: 0.8662
Epoch 240/400
1562/1562 [==============================] - 92s 59ms/step - loss: 0.2325 - acc: 0.9163 - val_loss: 0.4481 - val_acc: 0.8689
Epoch 241/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2316 - acc: 0.9180 - val_loss: 0.4444 - val_acc: 0.8709
.60% epoch=240, acc=0.917988, loss=0.231617, val_acc=0.870900, val_loss=0.444441, time=6.163 hours
Epoch 242/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2355 - acc: 0.9178 - val_loss: 0.4604 - val_acc: 0.8656
Epoch 243/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.2308 - acc: 0.9183 - val_loss: 0.4592 - val_acc: 0.8660
Epoch 244/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.2374 - acc: 0.9157 - val_loss: 0.4535 - val_acc: 0.8664
Epoch 245/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.2315 - acc: 0.9175 - val_loss: 0.4616 - val_acc: 0.8641
.Epoch 246/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.2304 - acc: 0.9176 - val_loss: 0.4723 - val_acc: 0.8623
Epoch 247/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.2328 - acc: 0.9162 - val_loss: 0.4508 - val_acc: 0.8679
Epoch 248/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2348 - acc: 0.9160 - val_loss: 0.4502 - val_acc: 0.8677
Epoch 249/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2339 - acc: 0.9174 - val_loss: 0.4479 - val_acc: 0.8702
.Epoch 250/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.2311 - acc: 0.9186 - val_loss: 0.4600 - val_acc: 0.8678
Epoch 251/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2343 - acc: 0.9169 - val_loss: 0.4409 - val_acc: 0.8686
Epoch 252/400
1562/1562 [==============================] - 87s 55ms/step - loss: 0.2340 - acc: 0.9171 - val_loss: 0.4488 - val_acc: 0.8680
Epoch 253/400
1562/1562 [==============================] - 88s 56ms/step - loss: 0.2332 - acc: 0.9184 - val_loss: 0.4571 - val_acc: 0.8630
.Epoch 254/400
1562/1562 [==============================] - 86s 55ms/step - loss: 0.2300 - acc: 0.9181 - val_loss: 0.4424 - val_acc: 0.8699
Epoch 255/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.2295 - acc: 0.9189 - val_loss: 0.4653 - val_acc: 0.8633
Epoch 256/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.2289 - acc: 0.9184 - val_loss: 0.4722 - val_acc: 0.8650
Epoch 257/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.2338 - acc: 0.9166 - val_loss: 0.4600 - val_acc: 0.8685
.Epoch 258/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.2248 - acc: 0.9201 - val_loss: 0.4659 - val_acc: 0.8635
Epoch 259/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2258 - acc: 0.9199 - val_loss: 0.4556 - val_acc: 0.8670
Epoch 260/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2226 - acc: 0.9209 - val_loss: 0.4600 - val_acc: 0.8665
Epoch 261/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2297 - acc: 0.9185 - val_loss: 0.4598 - val_acc: 0.8655
.65% epoch=260, acc=0.918528, loss=0.229588, val_acc=0.865500, val_loss=0.459795, time=6.640 hours
Epoch 262/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2293 - acc: 0.9198 - val_loss: 0.4587 - val_acc: 0.8657
Epoch 263/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2252 - acc: 0.9210 - val_loss: 0.4606 - val_acc: 0.8679
Epoch 264/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2298 - acc: 0.9193 - val_loss: 0.4595 - val_acc: 0.8672
Epoch 265/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2232 - acc: 0.9210 - val_loss: 0.4578 - val_acc: 0.8665
.Epoch 266/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2206 - acc: 0.9218 - val_loss: 0.4645 - val_acc: 0.8655
Epoch 267/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2209 - acc: 0.9212 - val_loss: 0.4487 - val_acc: 0.8678
Epoch 268/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2228 - acc: 0.9209 - val_loss: 0.4632 - val_acc: 0.8659
Epoch 269/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2248 - acc: 0.9199 - val_loss: 0.4491 - val_acc: 0.8690
.Epoch 270/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2236 - acc: 0.9205 - val_loss: 0.4688 - val_acc: 0.8657
Epoch 271/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2186 - acc: 0.9226 - val_loss: 0.4552 - val_acc: 0.8680
Epoch 272/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2248 - acc: 0.9205 - val_loss: 0.4698 - val_acc: 0.8621
Epoch 273/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2248 - acc: 0.9205 - val_loss: 0.4497 - val_acc: 0.8697
.Epoch 274/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2216 - acc: 0.9218 - val_loss: 0.4412 - val_acc: 0.8699
Epoch 275/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2201 - acc: 0.9221 - val_loss: 0.4538 - val_acc: 0.8701
Epoch 276/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2187 - acc: 0.9223 - val_loss: 0.4470 - val_acc: 0.8694
Epoch 277/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2185 - acc: 0.9233 - val_loss: 0.4602 - val_acc: 0.8684
.Epoch 278/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2235 - acc: 0.9207 - val_loss: 0.4514 - val_acc: 0.8693
Epoch 279/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2204 - acc: 0.9219 - val_loss: 0.4637 - val_acc: 0.8647
Epoch 280/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2194 - acc: 0.9227 - val_loss: 0.4583 - val_acc: 0.8674
Epoch 281/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2222 - acc: 0.9224 - val_loss: 0.4492 - val_acc: 0.8676
.70% epoch=280, acc=0.922410, loss=0.222202, val_acc=0.867600, val_loss=0.449180, time=7.110 hours
Epoch 282/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.2189 - acc: 0.9224 - val_loss: 0.4576 - val_acc: 0.8678
Epoch 283/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2180 - acc: 0.9227 - val_loss: 0.4671 - val_acc: 0.8661
Epoch 284/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2173 - acc: 0.9219 - val_loss: 0.4461 - val_acc: 0.8706
Epoch 285/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2193 - acc: 0.9219 - val_loss: 0.4528 - val_acc: 0.8680
.Epoch 286/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2138 - acc: 0.9237 - val_loss: 0.4658 - val_acc: 0.8657
Epoch 287/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2212 - acc: 0.9228 - val_loss: 0.4436 - val_acc: 0.8707
Epoch 288/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2180 - acc: 0.9227 - val_loss: 0.4470 - val_acc: 0.8695
Epoch 289/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2142 - acc: 0.9246 - val_loss: 0.4518 - val_acc: 0.8701
.Epoch 290/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2177 - acc: 0.9226 - val_loss: 0.4377 - val_acc: 0.8715
Epoch 291/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2165 - acc: 0.9238 - val_loss: 0.4520 - val_acc: 0.8670
Epoch 292/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2170 - acc: 0.9215 - val_loss: 0.4576 - val_acc: 0.8646
Epoch 293/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2162 - acc: 0.9223 - val_loss: 0.4950 - val_acc: 0.8591
.Epoch 294/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2205 - acc: 0.9219 - val_loss: 0.4545 - val_acc: 0.8698
Epoch 295/400
1562/1562 [==============================] - 87s 56ms/step - loss: 0.2111 - acc: 0.9263 - val_loss: 0.4582 - val_acc: 0.8688
Epoch 296/400
1562/1562 [==============================] - 89s 57ms/step - loss: 0.2187 - acc: 0.9216 - val_loss: 0.4513 - val_acc: 0.8683
Epoch 297/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2160 - acc: 0.9235 - val_loss: 0.4582 - val_acc: 0.8655
.Epoch 298/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2137 - acc: 0.9237 - val_loss: 0.4450 - val_acc: 0.8672
Epoch 299/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2167 - acc: 0.9231 - val_loss: 0.4507 - val_acc: 0.8677
Epoch 300/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2144 - acc: 0.9242 - val_loss: 0.4471 - val_acc: 0.8681
Epoch 301/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2071 - acc: 0.9256 - val_loss: 0.4559 - val_acc: 0.8701
.75% epoch=300, acc=0.925632, loss=0.206977, val_acc=0.870100, val_loss=0.455912, time=7.582 hours
Epoch 302/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2174 - acc: 0.9222 - val_loss: 0.4514 - val_acc: 0.8669
Epoch 303/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2117 - acc: 0.9252 - val_loss: 0.4441 - val_acc: 0.8670
Epoch 304/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2127 - acc: 0.9245 - val_loss: 0.4685 - val_acc: 0.8646
Epoch 305/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2090 - acc: 0.9264 - val_loss: 0.4474 - val_acc: 0.8683
.Epoch 306/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2090 - acc: 0.9244 - val_loss: 0.4687 - val_acc: 0.8662
Epoch 307/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2087 - acc: 0.9257 - val_loss: 0.4614 - val_acc: 0.8676
Epoch 308/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2167 - acc: 0.9230 - val_loss: 0.4472 - val_acc: 0.8704
Epoch 309/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2139 - acc: 0.9240 - val_loss: 0.4503 - val_acc: 0.8742
.Epoch 310/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2119 - acc: 0.9266 - val_loss: 0.4637 - val_acc: 0.8676
Epoch 311/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2094 - acc: 0.9256 - val_loss: 0.4764 - val_acc: 0.8649
Epoch 312/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2088 - acc: 0.9258 - val_loss: 0.4544 - val_acc: 0.8696
Epoch 313/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2102 - acc: 0.9257 - val_loss: 0.4565 - val_acc: 0.8679
.Epoch 314/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2062 - acc: 0.9271 - val_loss: 0.4634 - val_acc: 0.8667
Epoch 315/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2141 - acc: 0.9235 - val_loss: 0.4511 - val_acc: 0.8688
Epoch 316/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2152 - acc: 0.9241 - val_loss: 0.4652 - val_acc: 0.8659
Epoch 317/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2102 - acc: 0.9246 - val_loss: 0.4636 - val_acc: 0.8656
.Epoch 318/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2085 - acc: 0.9273 - val_loss: 0.4661 - val_acc: 0.8657
Epoch 319/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2096 - acc: 0.9261 - val_loss: 0.4522 - val_acc: 0.8673
Epoch 320/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2094 - acc: 0.9257 - val_loss: 0.4627 - val_acc: 0.8675
Epoch 321/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2075 - acc: 0.9265 - val_loss: 0.4649 - val_acc: 0.8684
.80% epoch=320, acc=0.926433, loss=0.207558, val_acc=0.868400, val_loss=0.464921, time=8.050 hours
Epoch 322/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2052 - acc: 0.9272 - val_loss: 0.4615 - val_acc: 0.8711
Epoch 323/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2092 - acc: 0.9265 - val_loss: 0.4479 - val_acc: 0.8711
Epoch 324/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2090 - acc: 0.9254 - val_loss: 0.4772 - val_acc: 0.8643
Epoch 325/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2058 - acc: 0.9268 - val_loss: 0.4545 - val_acc: 0.8693
.Epoch 326/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2074 - acc: 0.9258 - val_loss: 0.4499 - val_acc: 0.8702
Epoch 327/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2093 - acc: 0.9256 - val_loss: 0.4609 - val_acc: 0.8645
Epoch 328/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2069 - acc: 0.9260 - val_loss: 0.4642 - val_acc: 0.8678
Epoch 329/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2054 - acc: 0.9269 - val_loss: 0.4678 - val_acc: 0.8664
.Epoch 330/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2028 - acc: 0.9283 - val_loss: 0.4679 - val_acc: 0.8658
Epoch 331/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2075 - acc: 0.9269 - val_loss: 0.4646 - val_acc: 0.8662
Epoch 332/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1998 - acc: 0.9291 - val_loss: 0.4743 - val_acc: 0.8700
Epoch 333/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2076 - acc: 0.9261 - val_loss: 0.4647 - val_acc: 0.8670
.Epoch 334/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2038 - acc: 0.9280 - val_loss: 0.4553 - val_acc: 0.8686
Epoch 335/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2016 - acc: 0.9290 - val_loss: 0.4708 - val_acc: 0.8664
Epoch 336/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2060 - acc: 0.9269 - val_loss: 0.4835 - val_acc: 0.8638
Epoch 337/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2023 - acc: 0.9279 - val_loss: 0.4692 - val_acc: 0.8696
.Epoch 338/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2045 - acc: 0.9276 - val_loss: 0.4564 - val_acc: 0.8712
Epoch 339/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2022 - acc: 0.9289 - val_loss: 0.4576 - val_acc: 0.8688
Epoch 340/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2050 - acc: 0.9275 - val_loss: 0.4538 - val_acc: 0.8721
Epoch 341/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2056 - acc: 0.9286 - val_loss: 0.4562 - val_acc: 0.8713
.85% epoch=340, acc=0.928654, loss=0.205616, val_acc=0.871300, val_loss=0.456213, time=8.519 hours
Epoch 342/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2003 - acc: 0.9292 - val_loss: 0.4751 - val_acc: 0.8673
Epoch 343/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1987 - acc: 0.9299 - val_loss: 0.4552 - val_acc: 0.8692
Epoch 344/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2071 - acc: 0.9277 - val_loss: 0.4507 - val_acc: 0.8719
Epoch 345/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2007 - acc: 0.9291 - val_loss: 0.4494 - val_acc: 0.8712
.Epoch 346/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2032 - acc: 0.9278 - val_loss: 0.4799 - val_acc: 0.8623
Epoch 347/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1975 - acc: 0.9292 - val_loss: 0.4548 - val_acc: 0.8719
Epoch 348/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1986 - acc: 0.9286 - val_loss: 0.4544 - val_acc: 0.8736
Epoch 349/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1997 - acc: 0.9294 - val_loss: 0.4565 - val_acc: 0.8702
.Epoch 350/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2031 - acc: 0.9279 - val_loss: 0.4618 - val_acc: 0.8680
Epoch 351/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1992 - acc: 0.9282 - val_loss: 0.4482 - val_acc: 0.8750
Epoch 352/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1995 - acc: 0.9291 - val_loss: 0.4613 - val_acc: 0.8685
Epoch 353/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1987 - acc: 0.9289 - val_loss: 0.4728 - val_acc: 0.8676
.Epoch 354/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1985 - acc: 0.9293 - val_loss: 0.4671 - val_acc: 0.8669
Epoch 355/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2000 - acc: 0.9298 - val_loss: 0.4539 - val_acc: 0.8717
Epoch 356/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1967 - acc: 0.9290 - val_loss: 0.4559 - val_acc: 0.8719
Epoch 357/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1983 - acc: 0.9299 - val_loss: 0.4585 - val_acc: 0.8689
.Epoch 358/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1973 - acc: 0.9295 - val_loss: 0.4727 - val_acc: 0.8704
Epoch 359/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2025 - acc: 0.9268 - val_loss: 0.4601 - val_acc: 0.8710
Epoch 360/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1984 - acc: 0.9294 - val_loss: 0.4726 - val_acc: 0.8697
Epoch 361/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2014 - acc: 0.9278 - val_loss: 0.4538 - val_acc: 0.8705
.90% epoch=360, acc=0.927794, loss=0.201429, val_acc=0.870500, val_loss=0.453819, time=8.989 hours
Epoch 362/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2076 - acc: 0.9249 - val_loss: 0.4633 - val_acc: 0.8684
Epoch 363/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1935 - acc: 0.9326 - val_loss: 0.4713 - val_acc: 0.8690
Epoch 364/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1942 - acc: 0.9304 - val_loss: 0.4645 - val_acc: 0.8680
Epoch 365/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1994 - acc: 0.9289 - val_loss: 0.4723 - val_acc: 0.8662
.Epoch 366/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1927 - acc: 0.9325 - val_loss: 0.4610 - val_acc: 0.8695
Epoch 367/400
1562/1562 [==============================] - 85s 55ms/step - loss: 0.1987 - acc: 0.9287 - val_loss: 0.4771 - val_acc: 0.8669
Epoch 368/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1974 - acc: 0.9299 - val_loss: 0.4686 - val_acc: 0.8704
Epoch 369/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1917 - acc: 0.9333 - val_loss: 0.4631 - val_acc: 0.8689
.Epoch 370/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1954 - acc: 0.9305 - val_loss: 0.4694 - val_acc: 0.8683
Epoch 371/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1963 - acc: 0.9296 - val_loss: 0.4608 - val_acc: 0.8703
Epoch 372/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1959 - acc: 0.9310 - val_loss: 0.4628 - val_acc: 0.8694
Epoch 373/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1957 - acc: 0.9304 - val_loss: 0.4602 - val_acc: 0.8739
.Epoch 374/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1952 - acc: 0.9307 - val_loss: 0.4730 - val_acc: 0.8663
Epoch 375/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1966 - acc: 0.9313 - val_loss: 0.4604 - val_acc: 0.8707
Epoch 376/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1933 - acc: 0.9318 - val_loss: 0.4719 - val_acc: 0.8683
Epoch 377/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1924 - acc: 0.9319 - val_loss: 0.4686 - val_acc: 0.8685
.Epoch 378/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1940 - acc: 0.9317 - val_loss: 0.4659 - val_acc: 0.8713
Epoch 379/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2004 - acc: 0.9289 - val_loss: 0.4679 - val_acc: 0.8715
Epoch 380/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1987 - acc: 0.9294 - val_loss: 0.4703 - val_acc: 0.8669
Epoch 381/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1959 - acc: 0.9311 - val_loss: 0.4700 - val_acc: 0.8693
.95% epoch=380, acc=0.931096, loss=0.195970, val_acc=0.869300, val_loss=0.470046, time=9.458 hours
Epoch 382/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1945 - acc: 0.9304 - val_loss: 0.4595 - val_acc: 0.8708
Epoch 383/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1914 - acc: 0.9313 - val_loss: 0.4607 - val_acc: 0.8706
Epoch 384/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1932 - acc: 0.9315 - val_loss: 0.4582 - val_acc: 0.8684
Epoch 385/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1920 - acc: 0.9332 - val_loss: 0.4589 - val_acc: 0.8737
.Epoch 386/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1904 - acc: 0.9319 - val_loss: 0.4709 - val_acc: 0.8653
Epoch 387/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1897 - acc: 0.9335 - val_loss: 0.4499 - val_acc: 0.8747
Epoch 388/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1952 - acc: 0.9311 - val_loss: 0.4570 - val_acc: 0.8669
Epoch 389/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1987 - acc: 0.9304 - val_loss: 0.4613 - val_acc: 0.8708
.Epoch 390/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1943 - acc: 0.9306 - val_loss: 0.4575 - val_acc: 0.8713
Epoch 391/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1893 - acc: 0.9317 - val_loss: 0.4638 - val_acc: 0.8691
Epoch 392/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1942 - acc: 0.9314 - val_loss: 0.4616 - val_acc: 0.8729
Epoch 393/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1868 - acc: 0.9336 - val_loss: 0.4552 - val_acc: 0.8741
.Epoch 394/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1937 - acc: 0.9321 - val_loss: 0.4547 - val_acc: 0.8712
Epoch 395/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1908 - acc: 0.9328 - val_loss: 0.4546 - val_acc: 0.8703
Epoch 396/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1927 - acc: 0.9311 - val_loss: 0.4751 - val_acc: 0.8681
Epoch 397/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.1931 - acc: 0.9317 - val_loss: 0.4625 - val_acc: 0.8708
.Epoch 398/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1910 - acc: 0.9318 - val_loss: 0.4756 - val_acc: 0.8684
Epoch 399/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1959 - acc: 0.9319 - val_loss: 0.4568 - val_acc: 0.8686
Epoch 400/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.1921 - acc: 0.9327 - val_loss: 0.4601 - val_acc: 0.8698
 99% epoch=399 acc=0.932717 loss=0.192157
Train end: 2019-07-08 04:40:31
Total run time: 9.905 hours
max_acc = 0.933598  epoch = 392
max_val_acc = 0.875000  epoch = 350
Training: accuracy   = 0.999160 loss = 0.013974
Validation: accuracy = 0.873000 loss = 0.441008

#########################


EPOCH # 132
         --> Step 0, Minibatch Loss= 2.1389, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.1490, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.1937, Training Accuracy= 0.180
         --> Step 300, Minibatch Loss= 2.2314, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9747, Training Accuracy= 0.270
EPOCH # 133
         --> Step 0, Minibatch Loss= 2.2746, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1279, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1551, Training Accuracy= 0.170
         --> Step 300, Minibatch Loss= 2.1707, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9617, Training Accuracy= 0.280
EPOCH # 134
         --> Step 0, Minibatch Loss= 2.2114, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.1654, Training Accuracy= 0.120
         --> Step 200, Minibatch Loss= 2.1181, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.2325, Training Accuracy= 0.160
         --> Step 400, Minibatch Loss= 2.0502, Training Accuracy= 0.250
EPOCH # 135
         --> Step 0, Minibatch Loss= 2.1437, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.1149, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.1257, Training Accuracy= 0.180
         --> Step 300, Minibatch Loss= 2.1801, Training Accuracy= 0.140
         --> Step 400, Minibatch Loss= 1.9898, Training Accuracy= 0.270
EPOCH # 136
         --> Step 0, Minibatch Loss= 2.1982, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1425, Training Accuracy= 0.140
         --> Step 200, Minibatch Loss= 2.1134, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.2197, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9539, Training Accuracy= 0.270
EPOCH # 137
         --> Step 0, Minibatch Loss= 2.1864, Training Accuracy= 0.150
         --> Step 100, Minibatch Loss= 2.1932, Training Accuracy= 0.160
         --> Step 200, Minibatch Loss= 2.0475, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.2239, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 2.0394, Training Accuracy= 0.280
EPOCH # 138
         --> Step 0, Minibatch Loss= 2.1508, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.1093, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.0683, Training Accuracy= 0.190
         --> Step 300, Minibatch Loss= 2.2095, Training Accuracy= 0.190
         --> Step 400, Minibatch Loss= 2.0651, Training Accuracy= 0.250
EPOCH # 139
         --> Step 0, Minibatch Loss= 2.2255, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1152, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0601, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1726, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 2.0483, Training Accuracy= 0.280
EPOCH # 140
         --> Step 0, Minibatch Loss= 2.1874, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.1721, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.0964, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.2619, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 2.0466, Training Accuracy= 0.260
EPOCH # 141
         --> Step 0, Minibatch Loss= 2.1473, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1284, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0512, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1467, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 2.0329, Training Accuracy= 0.280
EPOCH # 142
         --> Step 0, Minibatch Loss= 2.2023, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1789, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.0599, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1796, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9966, Training Accuracy= 0.300
EPOCH # 143
         --> Step 0, Minibatch Loss= 2.2430, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1067, Training Accuracy= 0.150
         --> Step 200, Minibatch Loss= 2.0605, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.2085, Training Accuracy= 0.190
         --> Step 400, Minibatch Loss= 2.0681, Training Accuracy= 0.220
EPOCH # 144
         --> Step 0, Minibatch Loss= 2.1747, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.2320, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0971, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.2028, Training Accuracy= 0.190
         --> Step 400, Minibatch Loss= 2.0421, Training Accuracy= 0.250
EPOCH # 145
         --> Step 0, Minibatch Loss= 2.1256, Training Accuracy= 0.270
         --> Step 100, Minibatch Loss= 2.1290, Training Accuracy= 0.180
         --> Step 200, Minibatch Loss= 2.1129, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1755, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9937, Training Accuracy= 0.300
EPOCH # 146
         --> Step 0, Minibatch Loss= 2.1634, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.1467, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.0730, Training Accuracy= 0.280
         --> Step 300, Minibatch Loss= 2.1727, Training Accuracy= 0.150
         --> Step 400, Minibatch Loss= 2.0271, Training Accuracy= 0.250
EPOCH # 147
         --> Step 0, Minibatch Loss= 2.2199, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1483, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0381, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1869, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9303, Training Accuracy= 0.330
EPOCH # 148
         --> Step 0, Minibatch Loss= 2.1618, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.0693, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0758, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1420, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9565, Training Accuracy= 0.300
EPOCH # 149
         --> Step 0, Minibatch Loss= 2.2191, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1583, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0462, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1602, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9481, Training Accuracy= 0.280
EPOCH # 150
         --> Step 0, Minibatch Loss= 2.2300, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1251, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0396, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1650, Training Accuracy= 0.150
         --> Step 400, Minibatch Loss= 1.9397, Training Accuracy= 0.280
EPOCH # 151
         --> Step 0, Minibatch Loss= 2.3112, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1149, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.0892, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1703, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 2.0106, Training Accuracy= 0.280
EPOCH # 152
         --> Step 0, Minibatch Loss= 2.2340, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1540, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1494, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1256, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 2.0576, Training Accuracy= 0.260
EPOCH # 153
         --> Step 0, Minibatch Loss= 2.1498, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.1360, Training Accuracy= 0.270
         --> Step 200, Minibatch Loss= 2.1039, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1589, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 2.0372, Training Accuracy= 0.210
EPOCH # 154
         --> Step 0, Minibatch Loss= 2.1771, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.1456, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.0461, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1807, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 2.0468, Training Accuracy= 0.250
EPOCH # 155
         --> Step 0, Minibatch Loss= 2.2066, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.1565, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0266, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1364, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9560, Training Accuracy= 0.240
EPOCH # 156
         --> Step 0, Minibatch Loss= 2.2179, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1008, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.0532, Training Accuracy= 0.310
         --> Step 300, Minibatch Loss= 2.1695, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9951, Training Accuracy= 0.260
EPOCH # 157
         --> Step 0, Minibatch Loss= 2.2306, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1473, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0072, Training Accuracy= 0.300
         --> Step 300, Minibatch Loss= 2.1371, Training Accuracy= 0.270
         --> Step 400, Minibatch Loss= 1.9442, Training Accuracy= 0.270
EPOCH # 158
         --> Step 0, Minibatch Loss= 2.1722, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1722, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.0654, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1227, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9545, Training Accuracy= 0.290
EPOCH # 159
         --> Step 0, Minibatch Loss= 2.1642, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1510, Training Accuracy= 0.180
         --> Step 200, Minibatch Loss= 2.1149, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1791, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9730, Training Accuracy= 0.310
EPOCH # 160
         --> Step 0, Minibatch Loss= 2.1409, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1611, Training Accuracy= 0.120
         --> Step 200, Minibatch Loss= 2.0628, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1662, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9389, Training Accuracy= 0.270
EPOCH # 161
         --> Step 0, Minibatch Loss= 2.2001, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1116, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.1044, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1553, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9483, Training Accuracy= 0.290
EPOCH # 162
         --> Step 0, Minibatch Loss= 2.1695, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1055, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.0730, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1995, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 2.0131, Training Accuracy= 0.250
EPOCH # 163
         --> Step 0, Minibatch Loss= 2.1079, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1030, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0857, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1250, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9299, Training Accuracy= 0.220
EPOCH # 164
         --> Step 0, Minibatch Loss= 2.1249, Training Accuracy= 0.170
         --> Step 100, Minibatch Loss= 2.1429, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0826, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1550, Training Accuracy= 0.300
         --> Step 400, Minibatch Loss= 2.0602, Training Accuracy= 0.250
EPOCH # 165
         --> Step 0, Minibatch Loss= 2.1761, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.1730, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0753, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1437, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 2.0409, Training Accuracy= 0.310
EPOCH # 166
         --> Step 0, Minibatch Loss= 2.2116, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1143, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0494, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1086, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 2.0081, Training Accuracy= 0.250
EPOCH # 167
         --> Step 0, Minibatch Loss= 2.1641, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.1907, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.0917, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1544, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 2.0323, Training Accuracy= 0.200
EPOCH # 168
         --> Step 0, Minibatch Loss= 2.1650, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1190, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0448, Training Accuracy= 0.180
         --> Step 300, Minibatch Loss= 2.1612, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 2.0124, Training Accuracy= 0.260
EPOCH # 169
         --> Step 0, Minibatch Loss= 2.2305, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.0617, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.0861, Training Accuracy= 0.190
         --> Step 300, Minibatch Loss= 2.0978, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9888, Training Accuracy= 0.280
EPOCH # 170
         --> Step 0, Minibatch Loss= 2.2979, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.0576, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0887, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1668, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9522, Training Accuracy= 0.330
EPOCH # 171
         --> Step 0, Minibatch Loss= 2.1303, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.1930, Training Accuracy= 0.160
         --> Step 200, Minibatch Loss= 2.0696, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1537, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 2.0019, Training Accuracy= 0.270
EPOCH # 172
         --> Step 0, Minibatch Loss= 2.1358, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1545, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0474, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.2150, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9280, Training Accuracy= 0.280
EPOCH # 173
         --> Step 0, Minibatch Loss= 2.1864, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.2022, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.0267, Training Accuracy= 0.280
         --> Step 300, Minibatch Loss= 2.1707, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9920, Training Accuracy= 0.240
EPOCH # 174
         --> Step 0, Minibatch Loss= 2.1570, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1832, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0317, Training Accuracy= 0.320
         --> Step 300, Minibatch Loss= 2.1178, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9618, Training Accuracy= 0.290
EPOCH # 175
         --> Step 0, Minibatch Loss= 2.1694, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1530, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0675, Training Accuracy= 0.260
         --> Step 300, Minibatch Loss= 2.1888, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9643, Training Accuracy= 0.290
EPOCH # 176
         --> Step 0, Minibatch Loss= 2.2448, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1247, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0843, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1900, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9431, Training Accuracy= 0.330
EPOCH # 177
         --> Step 0, Minibatch Loss= 2.1965, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1374, Training Accuracy= 0.150
         --> Step 200, Minibatch Loss= 2.0461, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.2219, Training Accuracy= 0.180
         --> Step 400, Minibatch Loss= 1.9839, Training Accuracy= 0.240
EPOCH # 178
         --> Step 0, Minibatch Loss= 2.1476, Training Accuracy= 0.150
         --> Step 100, Minibatch Loss= 2.0806, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0290, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.2275, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 2.0214, Training Accuracy= 0.250
EPOCH # 179
         --> Step 0, Minibatch Loss= 2.2131, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1413, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.0653, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.2135, Training Accuracy= 0.180
         --> Step 400, Minibatch Loss= 1.9296, Training Accuracy= 0.260
EPOCH # 180
         --> Step 0, Minibatch Loss= 2.0755, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.0475, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.1129, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1706, Training Accuracy= 0.280
         --> Step 400, Minibatch Loss= 1.9209, Training Accuracy= 0.280
EPOCH # 181
         --> Step 0, Minibatch Loss= 2.1827, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1332, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0716, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1367, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9093, Training Accuracy= 0.270
EPOCH # 182
         --> Step 0, Minibatch Loss= 2.1344, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.0658, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0363, Training Accuracy= 0.310
         --> Step 300, Minibatch Loss= 2.1564, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9183, Training Accuracy= 0.270
EPOCH # 183
         --> Step 0, Minibatch Loss= 2.0915, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.0926, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.1081, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1912, Training Accuracy= 0.190
         --> Step 400, Minibatch Loss= 1.9093, Training Accuracy= 0.300
EPOCH # 184
         --> Step 0, Minibatch Loss= 2.1209, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.1896, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0925, Training Accuracy= 0.260
         --> Step 300, Minibatch Loss= 2.1616, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9653, Training Accuracy= 0.290
EPOCH # 185
         --> Step 0, Minibatch Loss= 2.1263, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.2363, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.1321, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1716, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9251, Training Accuracy= 0.290
EPOCH # 186
         --> Step 0, Minibatch Loss= 2.1358, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.1231, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.1241, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1839, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9554, Training Accuracy= 0.300
EPOCH # 187
         --> Step 0, Minibatch Loss= 2.2028, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1147, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.1621, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1483, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9667, Training Accuracy= 0.220
EPOCH # 188
         --> Step 0, Minibatch Loss= 2.1726, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1059, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.1160, Training Accuracy= 0.190
         --> Step 300, Minibatch Loss= 2.1690, Training Accuracy= 0.300
         --> Step 400, Minibatch Loss= 1.9325, Training Accuracy= 0.340
EPOCH # 189
         --> Step 0, Minibatch Loss= 2.1436, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.0703, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.1506, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1828, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9440, Training Accuracy= 0.300
EPOCH # 190
         --> Step 0, Minibatch Loss= 2.0985, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.0968, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.1199, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1547, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9436, Training Accuracy= 0.260
EPOCH # 191
         --> Step 0, Minibatch Loss= 2.0727, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.0887, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.0678, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1757, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9174, Training Accuracy= 0.310
EPOCH # 192
         --> Step 0, Minibatch Loss= 2.1430, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1672, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0943, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.2052, Training Accuracy= 0.190
         --> Step 400, Minibatch Loss= 1.9282, Training Accuracy= 0.300
EPOCH # 193
         --> Step 0, Minibatch Loss= 2.1855, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1352, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0993, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1431, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 2.0550, Training Accuracy= 0.280
EPOCH # 194
         --> Step 0, Minibatch Loss= 2.1873, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.2291, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1500, Training Accuracy= 0.160
         --> Step 300, Minibatch Loss= 2.1964, Training Accuracy= 0.170
         --> Step 400, Minibatch Loss= 1.9744, Training Accuracy= 0.270
EPOCH # 195
         --> Step 0, Minibatch Loss= 2.2098, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.1899, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0892, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1963, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9577, Training Accuracy= 0.330
EPOCH # 196
         --> Step 0, Minibatch Loss= 2.1236, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1546, Training Accuracy= 0.140
         --> Step 200, Minibatch Loss= 2.0769, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.2207, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9595, Training Accuracy= 0.320
EPOCH # 197
         --> Step 0, Minibatch Loss= 2.0715, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.1960, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0821, Training Accuracy= 0.190
         --> Step 300, Minibatch Loss= 2.1375, Training Accuracy= 0.280
         --> Step 400, Minibatch Loss= 1.9979, Training Accuracy= 0.230
EPOCH # 198
         --> Step 0, Minibatch Loss= 2.1314, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1871, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0983, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1494, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9406, Training Accuracy= 0.280
EPOCH # 199
         --> Step 0, Minibatch Loss= 2.1118, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1052, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0506, Training Accuracy= 0.290
         --> Step 300, Minibatch Loss= 2.1600, Training Accuracy= 0.270
         --> Step 400, Minibatch Loss= 1.9486, Training Accuracy= 0.290
EPOCH # 200
         --> Step 0, Minibatch Loss= 2.0857, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1056, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0414, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1790, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9142, Training Accuracy= 0.290
EPOCH # 201
         --> Step 0, Minibatch Loss= 2.0626, Training Accuracy= 0.270
         --> Step 100, Minibatch Loss= 2.1564, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0338, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1148, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9617, Training Accuracy= 0.240
EPOCH # 202
         --> Step 0, Minibatch Loss= 2.1066, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.0550, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.0561, Training Accuracy= 0.280
         --> Step 300, Minibatch Loss= 2.1309, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9636, Training Accuracy= 0.250
EPOCH # 203
         --> Step 0, Minibatch Loss= 2.0596, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1819, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.0386, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1406, Training Accuracy= 0.280
         --> Step 400, Minibatch Loss= 1.9912, Training Accuracy= 0.300
EPOCH # 204
         --> Step 0, Minibatch Loss= 2.1225, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1395, Training Accuracy= 0.160
         --> Step 200, Minibatch Loss= 2.0426, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1182, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9531, Training Accuracy= 0.320
EPOCH # 205
         --> Step 0, Minibatch Loss= 2.1838, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1782, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1234, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1131, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9677, Training Accuracy= 0.280
EPOCH # 206
         --> Step 0, Minibatch Loss= 2.1897, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1633, Training Accuracy= 0.160
         --> Step 200, Minibatch Loss= 2.0776, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1006, Training Accuracy= 0.300
         --> Step 400, Minibatch Loss= 1.9756, Training Accuracy= 0.300
EPOCH # 207
         --> Step 0, Minibatch Loss= 2.1235, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1564, Training Accuracy= 0.140
         --> Step 200, Minibatch Loss= 2.0190, Training Accuracy= 0.260
         --> Step 300, Minibatch Loss= 2.1104, Training Accuracy= 0.270
         --> Step 400, Minibatch Loss= 1.9630, Training Accuracy= 0.330
EPOCH # 208
         --> Step 0, Minibatch Loss= 2.1327, Training Accuracy= 0.260
         --> Step 100, Minibatch Loss= 2.1770, Training Accuracy= 0.140
         --> Step 200, Minibatch Loss= 2.0704, Training Accuracy= 0.190
         --> Step 300, Minibatch Loss= 2.1009, Training Accuracy= 0.270
         --> Step 400, Minibatch Loss= 1.9591, Training Accuracy= 0.270
EPOCH # 209
         --> Step 0, Minibatch Loss= 2.1593, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.1714, Training Accuracy= 0.140
         --> Step 200, Minibatch Loss= 2.0713, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.0602, Training Accuracy= 0.290
         --> Step 400, Minibatch Loss= 1.9899, Training Accuracy= 0.260
EPOCH # 210
         --> Step 0, Minibatch Loss= 2.2352, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.0632, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0517, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.0630, Training Accuracy= 0.320
         --> Step 400, Minibatch Loss= 2.0091, Training Accuracy= 0.330
EPOCH # 211
         --> Step 0, Minibatch Loss= 2.1819, Training Accuracy= 0.150
         --> Step 100, Minibatch Loss= 2.0782, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0424, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.0946, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9642, Training Accuracy= 0.270
EPOCH # 212
         --> Step 0, Minibatch Loss= 2.1444, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.1069, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.1115, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1368, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9749, Training Accuracy= 0.320
EPOCH # 213
         --> Step 0, Minibatch Loss= 2.1658, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1434, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.0729, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.0915, Training Accuracy= 0.270
         --> Step 400, Minibatch Loss= 1.9971, Training Accuracy= 0.240
EPOCH # 214
         --> Step 0, Minibatch Loss= 2.1746, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1506, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0556, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1000, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9340, Training Accuracy= 0.320
EPOCH # 215
         --> Step 0, Minibatch Loss= 2.1488, Training Accuracy= 0.160
         --> Step 100, Minibatch Loss= 2.1167, Training Accuracy= 0.280
         --> Step 200, Minibatch Loss= 2.0631, Training Accuracy= 0.190
         --> Step 300, Minibatch Loss= 2.1285, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9291, Training Accuracy= 0.290
EPOCH # 216
         --> Step 0, Minibatch Loss= 2.2381, Training Accuracy= 0.160
         --> Step 100, Minibatch Loss= 2.1221, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0959, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1126, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9113, Training Accuracy= 0.320
EPOCH # 217
         --> Step 0, Minibatch Loss= 2.2490, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.0741, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.0978, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1222, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9736, Training Accuracy= 0.360
EPOCH # 218
         --> Step 0, Minibatch Loss= 2.2004, Training Accuracy= 0.170
         --> Step 100, Minibatch Loss= 2.1160, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.1925, Training Accuracy= 0.160
         --> Step 300, Minibatch Loss= 2.1535, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9826, Training Accuracy= 0.320
EPOCH # 219
         --> Step 0, Minibatch Loss= 2.1952, Training Accuracy= 0.170
         --> Step 100, Minibatch Loss= 2.0793, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.1032, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1736, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9944, Training Accuracy= 0.200
EPOCH # 220
         --> Step 0, Minibatch Loss= 2.2299, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.0794, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1300, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1088, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9521, Training Accuracy= 0.330
EPOCH # 221
         --> Step 0, Minibatch Loss= 2.1262, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1262, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1258, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1631, Training Accuracy= 0.180
         --> Step 400, Minibatch Loss= 1.9532, Training Accuracy= 0.250
EPOCH # 222
         --> Step 0, Minibatch Loss= 2.2104, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1609, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.1456, Training Accuracy= 0.160
         --> Step 300, Minibatch Loss= 2.1305, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9541, Training Accuracy= 0.260
EPOCH # 223
         --> Step 0, Minibatch Loss= 2.1946, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.0962, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.1661, Training Accuracy= 0.150
         --> Step 300, Minibatch Loss= 2.1404, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9575, Training Accuracy= 0.290
EPOCH # 224
         --> Step 0, Minibatch Loss= 2.1591, Training Accuracy= 0.170
         --> Step 100, Minibatch Loss= 2.1172, Training Accuracy= 0.180
         --> Step 200, Minibatch Loss= 2.1608, Training Accuracy= 0.130
         --> Step 300, Minibatch Loss= 2.1263, Training Accuracy= 0.280
         --> Step 400, Minibatch Loss= 1.9424, Training Accuracy= 0.260
EPOCH # 225
         --> Step 0, Minibatch Loss= 2.1859, Training Accuracy= 0.160
         --> Step 100, Minibatch Loss= 2.1007, Training Accuracy= 0.280
         --> Step 200, Minibatch Loss= 2.1268, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1603, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9552, Training Accuracy= 0.290
EPOCH # 226
         --> Step 0, Minibatch Loss= 2.1524, Training Accuracy= 0.150
         --> Step 100, Minibatch Loss= 2.1395, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.1263, Training Accuracy= 0.170
         --> Step 300, Minibatch Loss= 2.1276, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9125, Training Accuracy= 0.280
EPOCH # 227
         --> Step 0, Minibatch Loss= 2.1697, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.0787, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0964, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1191, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9357, Training Accuracy= 0.300
EPOCH # 228
         --> Step 0, Minibatch Loss= 2.1043, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.0880, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0705, Training Accuracy= 0.160
         --> Step 300, Minibatch Loss= 2.1923, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9093, Training Accuracy= 0.310
EPOCH # 229
         --> Step 0, Minibatch Loss= 2.1565, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1711, Training Accuracy= 0.150
         --> Step 200, Minibatch Loss= 2.1062, Training Accuracy= 0.190
         --> Step 300, Minibatch Loss= 2.1578, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9476, Training Accuracy= 0.260
EPOCH # 230
         --> Step 0, Minibatch Loss= 2.2122, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.1496, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.1172, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1915, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9988, Training Accuracy= 0.260
EPOCH # 231
         --> Step 0, Minibatch Loss= 2.2208, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.0964, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.1209, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1869, Training Accuracy= 0.180
         --> Step 400, Minibatch Loss= 1.9753, Training Accuracy= 0.290
EPOCH # 232
         --> Step 0, Minibatch Loss= 2.1506, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.1134, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0512, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1775, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9697, Training Accuracy= 0.260
EPOCH # 233
         --> Step 0, Minibatch Loss= 2.1903, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1173, Training Accuracy= 0.180
         --> Step 200, Minibatch Loss= 2.1529, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1682, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 2.0116, Training Accuracy= 0.290
EPOCH # 234
         --> Step 0, Minibatch Loss= 2.2471, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1201, Training Accuracy= 0.270
         --> Step 200, Minibatch Loss= 2.1537, Training Accuracy= 0.130
         --> Step 300, Minibatch Loss= 2.1869, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 2.0066, Training Accuracy= 0.310
EPOCH # 235
         --> Step 0, Minibatch Loss= 2.1475, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1433, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.1175, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1754, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9490, Training Accuracy= 0.340
EPOCH # 236
         --> Step 0, Minibatch Loss= 2.2065, Training Accuracy= 0.160
         --> Step 100, Minibatch Loss= 2.0334, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.0979, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.2415, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9730, Training Accuracy= 0.330
EPOCH # 237
         --> Step 0, Minibatch Loss= 2.1200, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.0832, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.0887, Training Accuracy= 0.260
         --> Step 300, Minibatch Loss= 2.1566, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9743, Training Accuracy= 0.350
EPOCH # 238
         --> Step 0, Minibatch Loss= 2.1174, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.0916, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.1211, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1824, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9522, Training Accuracy= 0.320
EPOCH # 239
         --> Step 0, Minibatch Loss= 2.1652, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1438, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0218, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1897, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9712, Training Accuracy= 0.300
EPOCH # 240
         --> Step 0, Minibatch Loss= 2.1796, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.1912, Training Accuracy= 0.180
         --> Step 200, Minibatch Loss= 2.1077, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.2581, Training Accuracy= 0.270
         --> Step 400, Minibatch Loss= 1.9456, Training Accuracy= 0.350
EPOCH # 241
         --> Step 0, Minibatch Loss= 2.1942, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1048, Training Accuracy= 0.260
         --> Step 200, Minibatch Loss= 2.0739, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1747, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9848, Training Accuracy= 0.290
EPOCH # 242
         --> Step 0, Minibatch Loss= 2.1986, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1781, Training Accuracy= 0.150
         --> Step 200, Minibatch Loss= 2.0441, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1596, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.8850, Training Accuracy= 0.320
EPOCH # 243
         --> Step 0, Minibatch Loss= 2.1821, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1273, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0772, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1658, Training Accuracy= 0.310
         --> Step 400, Minibatch Loss= 1.9206, Training Accuracy= 0.280
EPOCH # 244
         --> Step 0, Minibatch Loss= 2.1618, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1755, Training Accuracy= 0.140
         --> Step 200, Minibatch Loss= 2.1139, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1529, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9203, Training Accuracy= 0.320
EPOCH # 245
         --> Step 0, Minibatch Loss= 2.1823, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1184, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0281, Training Accuracy= 0.260
         --> Step 300, Minibatch Loss= 2.1616, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9603, Training Accuracy= 0.280
EPOCH # 246
         --> Step 0, Minibatch Loss= 2.2619, Training Accuracy= 0.130
         --> Step 100, Minibatch Loss= 2.1863, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0922, Training Accuracy= 0.260
         --> Step 300, Minibatch Loss= 2.2160, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9588, Training Accuracy= 0.240
EPOCH # 247
         --> Step 0, Minibatch Loss= 2.2156, Training Accuracy= 0.150
         --> Step 100, Minibatch Loss= 2.1461, Training Accuracy= 0.270
         --> Step 200, Minibatch Loss= 2.0925, Training Accuracy= 0.260
         --> Step 300, Minibatch Loss= 2.1838, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9238, Training Accuracy= 0.310
EPOCH # 248
         --> Step 0, Minibatch Loss= 2.2005, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1489, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0704, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1891, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9372, Training Accuracy= 0.300
EPOCH # 249
         --> Step 0, Minibatch Loss= 2.2417, Training Accuracy= 0.180
         --> Step 100, Minibatch Loss= 2.0928, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.0541, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.2170, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.8982, Training Accuracy= 0.370
EPOCH # 250
         --> Step 0, Minibatch Loss= 2.1960, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1087, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0462, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1829, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9451, Training Accuracy= 0.280
EPOCH # 251
         --> Step 0, Minibatch Loss= 2.1398, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.0614, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.1032, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.2033, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9998, Training Accuracy= 0.240
EPOCH # 252
         --> Step 0, Minibatch Loss= 2.1403, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.0822, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.0738, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.2044, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9767, Training Accuracy= 0.250
EPOCH # 253
         --> Step 0, Minibatch Loss= 2.1443, Training Accuracy= 0.270
         --> Step 100, Minibatch Loss= 2.1056, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1209, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1773, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 2.0218, Training Accuracy= 0.230
EPOCH # 254
         --> Step 0, Minibatch Loss= 2.1822, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.0464, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.1263, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1393, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9611, Training Accuracy= 0.280
EPOCH # 255
         --> Step 0, Minibatch Loss= 2.1640, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.0817, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.1075, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1248, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9815, Training Accuracy= 0.280
EPOCH # 256
         --> Step 0, Minibatch Loss= 2.1660, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1076, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1277, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1262, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9411, Training Accuracy= 0.260
EPOCH # 257
         --> Step 0, Minibatch Loss= 2.2365, Training Accuracy= 0.150
         --> Step 100, Minibatch Loss= 2.0716, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.2024, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1774, Training Accuracy= 0.190
         --> Step 400, Minibatch Loss= 2.0394, Training Accuracy= 0.240
EPOCH # 258
         --> Step 0, Minibatch Loss= 2.2075, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.0596, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.1351, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1561, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9599, Training Accuracy= 0.320
EPOCH # 259
         --> Step 0, Minibatch Loss= 2.1636, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.0600, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.1309, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1630, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9111, Training Accuracy= 0.290
EPOCH # 260
         --> Step 0, Minibatch Loss= 2.1008, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.0811, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1417, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1194, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 2.0498, Training Accuracy= 0.280
EPOCH # 261
         --> Step 0, Minibatch Loss= 2.1874, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.1408, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.1348, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.0967, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 2.0891, Training Accuracy= 0.280
EPOCH # 262
         --> Step 0, Minibatch Loss= 2.1956, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.0901, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.1013, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1616, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9811, Training Accuracy= 0.280
EPOCH # 263
         --> Step 0, Minibatch Loss= 2.1938, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.0465, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.1100, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1944, Training Accuracy= 0.180
         --> Step 400, Minibatch Loss= 1.9333, Training Accuracy= 0.270
EPOCH # 264
         --> Step 0, Minibatch Loss= 2.1275, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1075, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0900, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1306, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9286, Training Accuracy= 0.310
EPOCH # 265
         --> Step 0, Minibatch Loss= 2.1504, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.0637, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.1166, Training Accuracy= 0.260
         --> Step 300, Minibatch Loss= 2.1265, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9755, Training Accuracy= 0.280
EPOCH # 266
         --> Step 0, Minibatch Loss= 2.1639, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1101, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0479, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1416, Training Accuracy= 0.330
         --> Step 400, Minibatch Loss= 1.9710, Training Accuracy= 0.280
EPOCH # 267
         --> Step 0, Minibatch Loss= 2.1119, Training Accuracy= 0.270
         --> Step 100, Minibatch Loss= 2.0429, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0435, Training Accuracy= 0.260
         --> Step 300, Minibatch Loss= 2.0813, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9581, Training Accuracy= 0.320
EPOCH # 268
         --> Step 0, Minibatch Loss= 2.1502, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1130, Training Accuracy= 0.150
         --> Step 200, Minibatch Loss= 2.0764, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.0750, Training Accuracy= 0.270
         --> Step 400, Minibatch Loss= 1.9565, Training Accuracy= 0.260
EPOCH # 269
         --> Step 0, Minibatch Loss= 2.1053, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.1191, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.0834, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1188, Training Accuracy= 0.190
         --> Step 400, Minibatch Loss= 1.9872, Training Accuracy= 0.250
EPOCH # 270
         --> Step 0, Minibatch Loss= 2.1096, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.1126, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0581, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1307, Training Accuracy= 0.200
         --> Step 400, Minibatch Loss= 1.9509, Training Accuracy= 0.240
EPOCH # 271
         --> Step 0, Minibatch Loss= 2.0848, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.0524, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0786, Training Accuracy= 0.230
         --> Step 300, Minibatch Loss= 2.1261, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9638, Training Accuracy= 0.230
EPOCH # 272
         --> Step 0, Minibatch Loss= 2.0858, Training Accuracy= 0.210
         --> Step 100, Minibatch Loss= 2.0592, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.1265, Training Accuracy= 0.160
         --> Step 300, Minibatch Loss= 2.1616, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9178, Training Accuracy= 0.330
EPOCH # 273
         --> Step 0, Minibatch Loss= 2.0881, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.0779, Training Accuracy= 0.260
         --> Step 200, Minibatch Loss= 2.0838, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1099, Training Accuracy= 0.280
         --> Step 400, Minibatch Loss= 1.9356, Training Accuracy= 0.260
EPOCH # 274
         --> Step 0, Minibatch Loss= 2.0552, Training Accuracy= 0.270
         --> Step 100, Minibatch Loss= 2.1090, Training Accuracy= 0.180
         --> Step 200, Minibatch Loss= 2.1003, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1269, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9355, Training Accuracy= 0.300
EPOCH # 275
         --> Step 0, Minibatch Loss= 2.0675, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.0925, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1111, Training Accuracy= 0.180
         --> Step 300, Minibatch Loss= 2.1307, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9252, Training Accuracy= 0.270
EPOCH # 276
         --> Step 0, Minibatch Loss= 2.1015, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1081, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 1.9739, Training Accuracy= 0.290
         --> Step 300, Minibatch Loss= 2.1286, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.8945, Training Accuracy= 0.290
EPOCH # 277
         --> Step 0, Minibatch Loss= 2.1087, Training Accuracy= 0.240
         --> Step 100, Minibatch Loss= 2.1070, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0824, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.0972, Training Accuracy= 0.270
         --> Step 400, Minibatch Loss= 1.8635, Training Accuracy= 0.300
EPOCH # 278
         --> Step 0, Minibatch Loss= 2.1233, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.0792, Training Accuracy= 0.150
         --> Step 200, Minibatch Loss= 2.0677, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1312, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.8851, Training Accuracy= 0.300
EPOCH # 279
         --> Step 0, Minibatch Loss= 2.0622, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.0914, Training Accuracy= 0.250
         --> Step 200, Minibatch Loss= 2.0651, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.0976, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9545, Training Accuracy= 0.250
EPOCH # 280
         --> Step 0, Minibatch Loss= 2.1760, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1239, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.0944, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.0767, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9570, Training Accuracy= 0.250
EPOCH # 281
         --> Step 0, Minibatch Loss= 2.0919, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.0632, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.0811, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1697, Training Accuracy= 0.210
         --> Step 400, Minibatch Loss= 1.9447, Training Accuracy= 0.320
EPOCH # 282
         --> Step 0, Minibatch Loss= 2.1560, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1278, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0224, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1112, Training Accuracy= 0.190
         --> Step 400, Minibatch Loss= 1.9293, Training Accuracy= 0.260
EPOCH # 283
         --> Step 0, Minibatch Loss= 2.1189, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1287, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1575, Training Accuracy= 0.190
         --> Step 300, Minibatch Loss= 2.1392, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9979, Training Accuracy= 0.240
EPOCH # 284
         --> Step 0, Minibatch Loss= 2.1726, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.1048, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0919, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1592, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9414, Training Accuracy= 0.280
EPOCH # 285
         --> Step 0, Minibatch Loss= 2.1138, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.0932, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.0826, Training Accuracy= 0.180
         --> Step 300, Minibatch Loss= 2.0903, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 2.0330, Training Accuracy= 0.280
EPOCH # 286
         --> Step 0, Minibatch Loss= 2.1931, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.1908, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.1133, Training Accuracy= 0.180
         --> Step 300, Minibatch Loss= 2.1369, Training Accuracy= 0.190
         --> Step 400, Minibatch Loss= 1.9367, Training Accuracy= 0.300
EPOCH # 287
         --> Step 0, Minibatch Loss= 2.1559, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.0984, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.1336, Training Accuracy= 0.170
         --> Step 300, Minibatch Loss= 2.0986, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9652, Training Accuracy= 0.300
EPOCH # 288
         --> Step 0, Minibatch Loss= 2.1245, Training Accuracy= 0.200
         --> Step 100, Minibatch Loss= 2.0841, Training Accuracy= 0.230
         --> Step 200, Minibatch Loss= 2.0625, Training Accuracy= 0.180
         --> Step 300, Minibatch Loss= 2.0498, Training Accuracy= 0.320
         --> Step 400, Minibatch Loss= 1.9838, Training Accuracy= 0.300
EPOCH # 289
         --> Step 0, Minibatch Loss= 2.0856, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.1366, Training Accuracy= 0.130
         --> Step 200, Minibatch Loss= 2.0266, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1537, Training Accuracy= 0.250
         --> Step 400, Minibatch Loss= 1.9277, Training Accuracy= 0.290
EPOCH # 290
         --> Step 0, Minibatch Loss= 2.1198, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.1347, Training Accuracy= 0.160
         --> Step 200, Minibatch Loss= 2.0877, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.1538, Training Accuracy= 0.290
         --> Step 400, Minibatch Loss= 1.9943, Training Accuracy= 0.260
EPOCH # 291
         --> Step 0, Minibatch Loss= 2.0832, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.0497, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.1309, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1317, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9402, Training Accuracy= 0.320
EPOCH # 292
         --> Step 0, Minibatch Loss= 2.1151, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1232, Training Accuracy= 0.210
         --> Step 200, Minibatch Loss= 2.0966, Training Accuracy= 0.210
         --> Step 300, Minibatch Loss= 2.1170, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9290, Training Accuracy= 0.270
EPOCH # 293
         --> Step 0, Minibatch Loss= 2.1021, Training Accuracy= 0.230
         --> Step 100, Minibatch Loss= 2.1269, Training Accuracy= 0.190
         --> Step 200, Minibatch Loss= 2.1683, Training Accuracy= 0.200
         --> Step 300, Minibatch Loss= 2.1446, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9560, Training Accuracy= 0.260
EPOCH # 294
         --> Step 0, Minibatch Loss= 2.1194, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.1196, Training Accuracy= 0.160
         --> Step 200, Minibatch Loss= 2.0589, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1719, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9845, Training Accuracy= 0.270
EPOCH # 295
         --> Step 0, Minibatch Loss= 2.0696, Training Accuracy= 0.300
         --> Step 100, Minibatch Loss= 2.0960, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.0575, Training Accuracy= 0.280
         --> Step 300, Minibatch Loss= 2.1305, Training Accuracy= 0.220
         --> Step 400, Minibatch Loss= 1.9981, Training Accuracy= 0.250
EPOCH # 296
         --> Step 0, Minibatch Loss= 2.0993, Training Accuracy= 0.190
         --> Step 100, Minibatch Loss= 2.1328, Training Accuracy= 0.200
         --> Step 200, Minibatch Loss= 2.0890, Training Accuracy= 0.240
         --> Step 300, Minibatch Loss= 2.0727, Training Accuracy= 0.230
         --> Step 400, Minibatch Loss= 1.9961, Training Accuracy= 0.260
EPOCH # 297
         --> Step 0, Minibatch Loss= 2.0415, Training Accuracy= 0.220
         --> Step 100, Minibatch Loss= 2.0849, Training Accuracy= 0.220
         --> Step 200, Minibatch Loss= 2.1315, Training Accuracy= 0.250
         --> Step 300, Minibatch Loss= 2.1040, Training Accuracy= 0.280
         --> Step 400, Minibatch Loss= 1.9729, Training Accuracy= 0.240
EPOCH # 298
         --> Step 0, Minibatch Loss= 2.1015, Training Accuracy= 0.170
         --> Step 100, Minibatch Loss= 2.0622, Training Accuracy= 0.240
         --> Step 200, Minibatch Loss= 2.0969, Training Accuracy= 0.220
         --> Step 300, Minibatch Loss= 2.1605, Training Accuracy= 0.260
         --> Step 400, Minibatch Loss= 1.9537, Training Accuracy= 0.250
EPOCH # 299
         --> Step 0, Minibatch Loss= 2.0911, Training Accuracy= 0.250
         --> Step 100, Minibatch Loss= 2.0908, Training Accuracy= 0.170
         --> Step 200, Minibatch Loss= 2.0396, Training Accuracy= 0.270
         --> Step 300, Minibatch Loss= 2.1135, Training Accuracy= 0.240
         --> Step 400, Minibatch Loss= 1.9371, Training Accuracy= 0.270
Optimization Finished!

=======>  Test Accuracy: 0.222

#######################################

Train end: 2019-07-09 09:13:15
Total run time: 9.253 hours
max_acc = 0.934238  epoch = 399
max_val_acc = 0.871800  epoch = 322
Saving model4 to "model4.h5"
Saving history dict to pickle file: hist4.p
Training: accuracy = 0.994020  ;  loss = 0.030406
Validation: accuracy = 0.866900  ;  loss = 0.473657

#########################################

WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train begin: 2019-07-11 13:47:46
Stop file: stop_training_file.keras (create this file to stop training gracefully)
Pause file: pause_training_file.keras (create this file to pause training and view graphs)
do_validation = True
epochs = 120
metrics = ['loss', 'acc', 'val_loss', 'val_acc']
steps = 1562
verbose = 1
Epoch 1/120
2019-07-11 13:47:46.225237: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-07-11 13:47:47.191017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:03:00.0
totalMemory: 4.00GiB freeMemory: 3.35GiB
2019-07-11 13:47:47.196984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-07-11 13:47:48.439158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-11 13:47:48.442625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-07-11 13:47:48.444811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-07-11 13:47:48.447155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with
3058 MB memory) -> physical GPU (device: 0, name: GeForce 840M, pci bus id: 0000:03:00.0, compute capability: 5.0)
2019-07-11 13:47:49.268988: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
1562/1562 [==============================] - 82s 52ms/step - loss: 1.8764 - acc: 0.3080 - val_loss: 1.5509 - val_acc: 0.4337
Epoch 2/120
1562/1562 [==============================] - 77s 49ms/step - loss: 1.5551 - acc: 0.4352 - val_loss: 1.3596 - val_acc: 0.5051
Epoch 3/120
1562/1562 [==============================] - 77s 49ms/step - loss: 1.3810 - acc: 0.4987 - val_loss: 1.1132 - val_acc: 0.6076
.Epoch 4/120
1562/1562 [==============================] - 78s 50ms/step - loss: 1.2718 - acc: 0.5442 - val_loss: 1.0609 - val_acc: 0.6200
.Epoch 5/120
1562/1562 [==============================] - 81s 52ms/step - loss: 1.1950 - acc: 0.5718 - val_loss: 0.9854 - val_acc: 0.6526
.Epoch 6/120
1562/1562 [==============================] - 80s 51ms/step - loss: 1.1454 - acc: 0.5910 - val_loss: 0.9141 - val_acc: 0.6770
.Epoch 7/120
1562/1562 [==============================] - 78s 50ms/step - loss: 1.0998 - acc: 0.6119 - val_loss: 0.8981 - val_acc: 0.6851
.05% epoch=6, acc=0.611832, loss=1.099750, val_acc=0.685100, val_loss=0.898143, time=0.154 hours
Epoch 8/120
1562/1562 [==============================] - 77s 50ms/step - loss: 1.0602 - acc: 0.6243 - val_loss: 0.8657 - val_acc: 0.6948
Epoch 9/120
1562/1562 [==============================] - 78s 50ms/step - loss: 1.0392 - acc: 0.6297 - val_loss: 0.8229 - val_acc: 0.7150
.Epoch 10/120
1562/1562 [==============================] - 81s 52ms/step - loss: 0.9997 - acc: 0.6462 - val_loss: 0.8372 - val_acc: 0.7079
.Epoch 11/120
1562/1562 [==============================] - 81s 52ms/step - loss: 0.9795 - acc: 0.6531 - val_loss: 0.7766 - val_acc: 0.7345
.Epoch 12/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.9536 - acc: 0.6656 - val_loss: 0.7573 - val_acc: 0.7414
.Epoch 13/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.9279 - acc: 0.6739 - val_loss: 0.7416 - val_acc: 0.7411
.10% epoch=12, acc=0.673891, loss=0.927889, val_acc=0.741100, val_loss=0.741562, time=0.285 hours
Epoch 14/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.9127 - acc: 0.6772 - val_loss: 0.7235 - val_acc: 0.7466
Epoch 15/120
1562/1562 [==============================] - 79s 51ms/step - loss: 0.8977 - acc: 0.6835 - val_loss: 0.7336 - val_acc: 0.7461
.Epoch 16/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.8775 - acc: 0.6919 - val_loss: 0.7110 - val_acc: 0.7502
.Epoch 17/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.8702 - acc: 0.6929 - val_loss: 0.7353 - val_acc: 0.7453
.Epoch 18/120
1562/1562 [==============================] - 79s 51ms/step - loss: 0.8532 - acc: 0.6978 - val_loss: 0.6824 - val_acc: 0.7650
.Epoch 19/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.8431 - acc: 0.7038 - val_loss: 0.6822 - val_acc: 0.7647
.15% epoch=18, acc=0.703810, loss=0.843112, val_acc=0.764700, val_loss=0.682170, time=0.417 hours
Epoch 20/120
1562/1562 [==============================] - 103s 66ms/step - loss: 0.8305 - acc: 0.7102 - val_loss: 0.6795 - val_acc: 0.7651
Epoch 21/120
1562/1562 [==============================] - 77s 49ms/step - loss: 0.8199 - acc: 0.7110 - val_loss: 0.6662 - val_acc: 0.7690
.Epoch 22/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.8115 - acc: 0.7160 - val_loss: 0.6712 - val_acc: 0.7645
.Epoch 23/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.8003 - acc: 0.7185 - val_loss: 0.6769 - val_acc: 0.7693
.Epoch 24/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7945 - acc: 0.7194 - val_loss: 0.6464 - val_acc: 0.7770
.Epoch 25/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7884 - acc: 0.7230 - val_loss: 0.6623 - val_acc: 0.7736
.20% epoch=24, acc=0.723003, loss=0.788353, val_acc=0.773600, val_loss=0.662268, time=0.553 hours
Epoch 26/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7790 - acc: 0.7268 - val_loss: 0.6532 - val_acc: 0.7748
Epoch 27/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7743 - acc: 0.7285 - val_loss: 0.6418 - val_acc: 0.7768
.Epoch 28/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7674 - acc: 0.7302 - val_loss: 0.6417 - val_acc: 0.7756
.Epoch 29/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7597 - acc: 0.7346 - val_loss: 0.6467 - val_acc: 0.7759
.Epoch 30/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7521 - acc: 0.7350 - val_loss: 0.6261 - val_acc: 0.7825
.Epoch 31/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7480 - acc: 0.7364 - val_loss: 0.6138 - val_acc: 0.7845
.25% epoch=30, acc=0.736371, loss=0.748075, val_acc=0.784500, val_loss=0.613838, time=0.683 hours
Epoch 32/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7386 - acc: 0.7410 - val_loss: 0.6147 - val_acc: 0.7844
Epoch 33/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7365 - acc: 0.7427 - val_loss: 0.6219 - val_acc: 0.7837
.Epoch 34/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7284 - acc: 0.7441 - val_loss: 0.6371 - val_acc: 0.7784
.Epoch 35/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7233 - acc: 0.7447 - val_loss: 0.6133 - val_acc: 0.7878
.Epoch 36/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7182 - acc: 0.7479 - val_loss: 0.6159 - val_acc: 0.7844
.Epoch 37/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7152 - acc: 0.7485 - val_loss: 0.6142 - val_acc: 0.7872
.30% epoch=36, acc=0.748479, loss=0.715238, val_acc=0.787200, val_loss=0.614232, time=0.813 hours
Epoch 38/120
1562/1562 [==============================] - 79s 51ms/step - loss: 0.7145 - acc: 0.7481 - val_loss: 0.6102 - val_acc: 0.7909
Epoch 39/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7099 - acc: 0.7510 - val_loss: 0.6012 - val_acc: 0.7927
.Epoch 40/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.7045 - acc: 0.7516 - val_loss: 0.5884 - val_acc: 0.7940
.Epoch 41/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6951 - acc: 0.7560 - val_loss: 0.5982 - val_acc: 0.7931
.Epoch 42/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6903 - acc: 0.7576 - val_loss: 0.5939 - val_acc: 0.7934
.Epoch 43/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6946 - acc: 0.7564 - val_loss: 0.5735 - val_acc: 0.8013
.35% epoch=42, acc=0.756344, loss=0.694582, val_acc=0.801300, val_loss=0.573536, time=0.944 hours
Epoch 44/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6907 - acc: 0.7561 - val_loss: 0.5825 - val_acc: 0.7987
Epoch 45/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.6846 - acc: 0.7598 - val_loss: 0.5786 - val_acc: 0.7998
.Epoch 46/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6817 - acc: 0.7604 - val_loss: 0.5670 - val_acc: 0.8028
.Epoch 47/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6738 - acc: 0.7618 - val_loss: 0.5828 - val_acc: 0.7993
.Epoch 48/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6777 - acc: 0.7599 - val_loss: 0.5721 - val_acc: 0.8011
.Epoch 49/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6720 - acc: 0.7618 - val_loss: 0.5831 - val_acc: 0.8004
.40% epoch=48, acc=0.761748, loss=0.671995, val_acc=0.800400, val_loss=0.583149, time=1.074 hours
Epoch 50/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6684 - acc: 0.7663 - val_loss: 0.5865 - val_acc: 0.7967
Epoch 51/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6661 - acc: 0.7683 - val_loss: 0.5609 - val_acc: 0.8048
.Epoch 52/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6625 - acc: 0.7658 - val_loss: 0.5592 - val_acc: 0.8067
.Epoch 53/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6641 - acc: 0.7672 - val_loss: 0.5819 - val_acc: 0.7988
.Epoch 54/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6546 - acc: 0.7722 - val_loss: 0.5586 - val_acc: 0.8048
.Epoch 55/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6560 - acc: 0.7710 - val_loss: 0.5683 - val_acc: 0.8046
.45% epoch=54, acc=0.771013, loss=0.656047, val_acc=0.804600, val_loss=0.568251, time=1.204 hours
Epoch 56/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6510 - acc: 0.7733 - val_loss: 0.5629 - val_acc: 0.8028
Epoch 57/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.6544 - acc: 0.7696 - val_loss: 0.5739 - val_acc: 0.8025
.Epoch 58/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.6513 - acc: 0.7723 - val_loss: 0.5499 - val_acc: 0.8103
.Epoch 59/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6411 - acc: 0.7747 - val_loss: 0.5589 - val_acc: 0.8072
.Epoch 60/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6487 - acc: 0.7723 - val_loss: 0.5568 - val_acc: 0.8088
.Epoch 61/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6432 - acc: 0.7717 - val_loss: 0.5739 - val_acc: 0.7995
.50% epoch=60, acc=0.771754, loss=0.643305, val_acc=0.799500, val_loss=0.573898, time=1.335 hours
Epoch 62/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6388 - acc: 0.7746 - val_loss: 0.5540 - val_acc: 0.8078
Epoch 63/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6326 - acc: 0.7777 - val_loss: 0.5526 - val_acc: 0.8104
.Epoch 64/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6330 - acc: 0.7764 - val_loss: 0.5529 - val_acc: 0.8092
.Epoch 65/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6338 - acc: 0.7766 - val_loss: 0.5619 - val_acc: 0.8076
.Epoch 66/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6246 - acc: 0.7793 - val_loss: 0.5528 - val_acc: 0.8086
.Epoch 67/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6294 - acc: 0.7788 - val_loss: 0.5482 - val_acc: 0.8101
.55% epoch=66, acc=0.778838, loss=0.629361, val_acc=0.810100, val_loss=0.548153, time=1.464 hours
Epoch 68/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6259 - acc: 0.7817 - val_loss: 0.5563 - val_acc: 0.8063
Epoch 69/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6249 - acc: 0.7770 - val_loss: 0.5667 - val_acc: 0.8006
.Epoch 70/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6180 - acc: 0.7806 - val_loss: 0.5403 - val_acc: 0.8099
.Epoch 71/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6255 - acc: 0.7814 - val_loss: 0.5595 - val_acc: 0.8074
.Epoch 72/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6245 - acc: 0.7804 - val_loss: 0.5570 - val_acc: 0.8077
.Epoch 73/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6155 - acc: 0.7822 - val_loss: 0.5407 - val_acc: 0.8139
.60% epoch=72, acc=0.782181, loss=0.615656, val_acc=0.813900, val_loss=0.540681, time=1.594 hours
Epoch 74/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6115 - acc: 0.7842 - val_loss: 0.5460 - val_acc: 0.8110
Epoch 75/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6097 - acc: 0.7843 - val_loss: 0.5561 - val_acc: 0.8063
.Epoch 76/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6076 - acc: 0.7848 - val_loss: 0.5477 - val_acc: 0.8129
.Epoch 77/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6080 - acc: 0.7847 - val_loss: 0.5560 - val_acc: 0.8088
.Epoch 78/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6084 - acc: 0.7861 - val_loss: 0.5477 - val_acc: 0.8114
.Epoch 79/120
1562/1562 [==============================] - 83s 53ms/step - loss: 0.6076 - acc: 0.7845 - val_loss: 0.5373 - val_acc: 0.8138
.65% epoch=78, acc=0.784522, loss=0.607679, val_acc=0.813800, val_loss=0.537302, time=1.726 hours
Epoch 80/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.6031 - acc: 0.7867 - val_loss: 0.5434 - val_acc: 0.8106
Epoch 81/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.6038 - acc: 0.7862 - val_loss: 0.5582 - val_acc: 0.8071
.Epoch 82/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5984 - acc: 0.7893 - val_loss: 0.5316 - val_acc: 0.8168
.Epoch 83/120
1562/1562 [==============================] - 79s 51ms/step - loss: 0.6012 - acc: 0.7869 - val_loss: 0.5509 - val_acc: 0.8110
.Epoch 84/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.6019 - acc: 0.7868 - val_loss: 0.5421 - val_acc: 0.8140
.Epoch 85/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5968 - acc: 0.7896 - val_loss: 0.5335 - val_acc: 0.8157
.70% epoch=84, acc=0.789585, loss=0.596851, val_acc=0.815700, val_loss=0.533532, time=1.857 hours
Epoch 86/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5968 - acc: 0.7892 - val_loss: 0.5535 - val_acc: 0.8072
Epoch 87/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5958 - acc: 0.7913 - val_loss: 0.5408 - val_acc: 0.8145
.Epoch 88/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5940 - acc: 0.7915 - val_loss: 0.5376 - val_acc: 0.8128
.Epoch 89/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5911 - acc: 0.7925 - val_loss: 0.5237 - val_acc: 0.8190
.Epoch 90/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5906 - acc: 0.7925 - val_loss: 0.5431 - val_acc: 0.8131
.Epoch 91/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5930 - acc: 0.7910 - val_loss: 0.5334 - val_acc: 0.8134
.75% epoch=90, acc=0.790966, loss=0.593030, val_acc=0.813400, val_loss=0.533417, time=1.990 hours
Epoch 92/120
1562/1562 [==============================] - 82s 52ms/step - loss: 0.5875 - acc: 0.7924 - val_loss: 0.5438 - val_acc: 0.8134
Epoch 93/120
1562/1562 [==============================] - 82s 52ms/step - loss: 0.5871 - acc: 0.7937 - val_loss: 0.5329 - val_acc: 0.8175
.Epoch 94/120
1562/1562 [==============================] - 83s 53ms/step - loss: 0.5853 - acc: 0.7935 - val_loss: 0.5336 - val_acc: 0.8160
.Epoch 95/120
1562/1562 [==============================] - 81s 52ms/step - loss: 0.5866 - acc: 0.7939 - val_loss: 0.5501 - val_acc: 0.8105
.Epoch 96/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5828 - acc: 0.7918 - val_loss: 0.5307 - val_acc: 0.8160
.Epoch 97/120
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5787 - acc: 0.7959 - val_loss: 0.5338 - val_acc: 0.8156
.80% epoch=96, acc=0.795869, loss=0.578636, val_acc=0.815600, val_loss=0.533762, time=2.125 hours
Epoch 98/120
1562/1562 [==============================] - 79s 51ms/step - loss: 0.5862 - acc: 0.7923 - val_loss: 0.5209 - val_acc: 0.8200
Epoch 99/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5858 - acc: 0.7943 - val_loss: 0.5272 - val_acc: 0.8199
.Epoch 100/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5841 - acc: 0.7940 - val_loss: 0.5253 - val_acc: 0.8192
.Epoch 101/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5763 - acc: 0.7956 - val_loss: 0.5253 - val_acc: 0.8176
.Epoch 102/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5797 - acc: 0.7959 - val_loss: 0.5341 - val_acc: 0.8151
.Epoch 103/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5797 - acc: 0.7950 - val_loss: 0.5320 - val_acc: 0.8153
.85% epoch=102, acc=0.794949, loss=0.579806, val_acc=0.815300, val_loss=0.531956, time=2.257 hours
Epoch 104/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5750 - acc: 0.7956 - val_loss: 0.5263 - val_acc: 0.8158
Epoch 105/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5750 - acc: 0.7961 - val_loss: 0.5173 - val_acc: 0.8216
.Epoch 106/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5749 - acc: 0.7984 - val_loss: 0.5244 - val_acc: 0.8195
.Epoch 107/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5748 - acc: 0.7963 - val_loss: 0.5238 - val_acc: 0.8196
.Epoch 108/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5728 - acc: 0.7957 - val_loss: 0.5237 - val_acc: 0.8186
.Epoch 109/120
1562/1562 [==============================] - 79s 50ms/step - loss: 0.5689 - acc: 0.8010 - val_loss: 0.5253 - val_acc: 0.8178
.90% epoch=108, acc=0.801013, loss=0.568950, val_acc=0.817800, val_loss=0.525303, time=2.388 hours
Epoch 110/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5711 - acc: 0.7962 - val_loss: 0.5334 - val_acc: 0.8167
Epoch 111/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5689 - acc: 0.8002 - val_loss: 0.5167 - val_acc: 0.8233
.Epoch 112/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5627 - acc: 0.8028 - val_loss: 0.5153 - val_acc: 0.8224
.Epoch 113/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5678 - acc: 0.8007 - val_loss: 0.5161 - val_acc: 0.8236
.Epoch 114/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5679 - acc: 0.8003 - val_loss: 0.5243 - val_acc: 0.8194
.Epoch 115/120
1562/1562 [==============================] - 77s 50ms/step - loss: 0.5643 - acc: 0.7990 - val_loss: 0.5460 - val_acc: 0.8132
.95% epoch=114, acc=0.799011, loss=0.564199, val_acc=0.813200, val_loss=0.546021, time=2.517 hours
Epoch 116/120
1562/1562 [==============================] - 78s 50ms/step - loss: 0.5623 - acc: 0.8007 - val_loss: 0.5266 - val_acc: 0.8207
Epoch 117/120
1562/1562 [==============================] - 77s 50ms/step - loss: 0.5599 - acc: 0.8026 - val_loss: 0.5156 - val_acc: 0.8227
.Epoch 118/120
1562/1562 [==============================] - 77s 50ms/step - loss: 0.5617 - acc: 0.8010 - val_loss: 0.5250 - val_acc: 0.8217
.Epoch 119/120
1562/1562 [==============================] - 77s 50ms/step - loss: 0.5648 - acc: 0.8008 - val_loss: 0.5279 - val_acc: 0.8199
.Epoch 120/120
1562/1562 [==============================] - 77s 50ms/step - loss: 0.5611 - acc: 0.8009 - val_loss: 0.5310 - val_acc: 0.8188
. 99% epoch=119 acc=0.800893 loss=0.561179
Train end: 2019-07-11 16:25:15
Total run time: 2.625 hours
max_acc = 0.802854  epoch = 111
max_val_acc = 0.823600  epoch = 112
Saving model3 to "model3.h5"
Saving history dict to pickle file: hist3.p
Confusion matrix, without normalization
[[885  11  27  11   7   2   8   6  29  14]
 [  8 943   3   3   1   0   8   0   7  27]
 [ 46   0 729  32  46  30  90  14   7   6]
 [ 21   4  56 591  41 133 116  27   8   3]
 [ 14   1  56  20 783  14  88  23   1   0]
 [ 12   1  22 140  35 709  54  26   1   0]
 [  7   1  16  15   8   8 938   3   3   1]
 [ 11   1  28  19  32  31  13 863   0   2]
 [ 59  16   8   6   2   3  10   4 878  14]
 [ 28  66   5  10   2   0   5   7   8 869]]
Medium CNN with Data Augmentation: 0.8188

#############################################

Using TensorFlow backend.
x_train shape: (50000, 32, 32, 3)
50000 train samples
10000 test samples
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Not using data augmentation.
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 50000 samples, validate on 10000 samples
Epoch 1/100
2019-07-11 18:08:18.761365: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-07-11 18:08:19.770092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:03:00.0
totalMemory: 4.00GiB freeMemory: 3.35GiB
2019-07-11 18:08:19.781528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-07-11 18:08:21.021167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-11 18:08:21.024116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-07-11 18:08:21.029672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-07-11 18:08:21.031407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with
3058 MB memory) -> physical GPU (device: 0, name: GeForce 840M, pci bus id: 0000:03:00.0, compute capability: 5.0)
2019-07-11 18:08:21.967971: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
50000/50000 [==============================] - 68s 1ms/step - loss: 1.8015 - acc: 0.3382 - val_loss: 1.5112 - val_acc: 0.4435
Epoch 2/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.4870 - acc: 0.4623 - val_loss: 1.3968 - val_acc: 0.4982
Epoch 3/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.3571 - acc: 0.5125 - val_loss: 1.2401 - val_acc: 0.5556
Epoch 4/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.2603 - acc: 0.5515 - val_loss: 1.1557 - val_acc: 0.5883
Epoch 5/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.1810 - acc: 0.5835 - val_loss: 1.0844 - val_acc: 0.6162
Epoch 6/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.1186 - acc: 0.6065 - val_loss: 1.0695 - val_acc: 0.6246
Epoch 7/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.0617 - acc: 0.6275 - val_loss: 1.0090 - val_acc: 0.6461
Epoch 8/100
50000/50000 [==============================] - 62s 1ms/step - loss: 1.0148 - acc: 0.6450 - val_loss: 0.9358 - val_acc: 0.6740
Epoch 9/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.9716 - acc: 0.6602 - val_loss: 0.9042 - val_acc: 0.6844
Epoch 10/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.9376 - acc: 0.6728 - val_loss: 0.9056 - val_acc: 0.6840
Epoch 11/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.9094 - acc: 0.6826 - val_loss: 0.8782 - val_acc: 0.6940
Epoch 12/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8803 - acc: 0.6938 - val_loss: 0.8329 - val_acc: 0.7123
Epoch 13/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8588 - acc: 0.7038 - val_loss: 0.8599 - val_acc: 0.7022
Epoch 14/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8374 - acc: 0.7103 - val_loss: 0.7945 - val_acc: 0.7275
Epoch 15/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.8168 - acc: 0.7148 - val_loss: 0.8319 - val_acc: 0.7156
Epoch 16/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.8018 - acc: 0.7224 - val_loss: 0.8031 - val_acc: 0.7235
Epoch 17/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.7911 - acc: 0.7265 - val_loss: 0.8278 - val_acc: 0.7172
Epoch 18/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7778 - acc: 0.7314 - val_loss: 0.7724 - val_acc: 0.7375
Epoch 19/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7643 - acc: 0.7373 - val_loss: 0.7714 - val_acc: 0.7377
Epoch 20/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7568 - acc: 0.7408 - val_loss: 0.7411 - val_acc: 0.7495
Epoch 21/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7469 - acc: 0.7450 - val_loss: 0.7146 - val_acc: 0.7566
Epoch 22/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7454 - acc: 0.7456 - val_loss: 0.7354 - val_acc: 0.7472
Epoch 23/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7342 - acc: 0.7496 - val_loss: 0.7101 - val_acc: 0.7552
Epoch 24/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7267 - acc: 0.7525 - val_loss: 0.7133 - val_acc: 0.7598
Epoch 25/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7190 - acc: 0.7556 - val_loss: 0.7051 - val_acc: 0.7595
Epoch 26/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7152 - acc: 0.7564 - val_loss: 0.6834 - val_acc: 0.7673
Epoch 27/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7111 - acc: 0.7588 - val_loss: 0.7158 - val_acc: 0.7585
Epoch 28/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.7051 - acc: 0.7589 - val_loss: 0.7306 - val_acc: 0.7503
Epoch 29/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6982 - acc: 0.7621 - val_loss: 0.6803 - val_acc: 0.7666
Epoch 30/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6977 - acc: 0.7656 - val_loss: 0.7200 - val_acc: 0.7685
Epoch 31/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6939 - acc: 0.7667 - val_loss: 0.6993 - val_acc: 0.7611
Epoch 32/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6886 - acc: 0.7664 - val_loss: 0.6649 - val_acc: 0.7760
Epoch 33/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6789 - acc: 0.7701 - val_loss: 0.6772 - val_acc: 0.7661
Epoch 34/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6802 - acc: 0.7694 - val_loss: 0.6675 - val_acc: 0.7766
Epoch 35/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6779 - acc: 0.7698 - val_loss: 0.6666 - val_acc: 0.7742
Epoch 36/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6727 - acc: 0.7732 - val_loss: 0.6737 - val_acc: 0.7758
Epoch 37/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6657 - acc: 0.7751 - val_loss: 0.6766 - val_acc: 0.7703
Epoch 38/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6675 - acc: 0.7772 - val_loss: 0.6682 - val_acc: 0.7770
Epoch 39/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6591 - acc: 0.7801 - val_loss: 0.6692 - val_acc: 0.7716
Epoch 40/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6579 - acc: 0.7781 - val_loss: 0.6713 - val_acc: 0.7776
Epoch 41/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6565 - acc: 0.7804 - val_loss: 0.6711 - val_acc: 0.7744
Epoch 42/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6459 - acc: 0.7834 - val_loss: 0.6796 - val_acc: 0.7753
Epoch 43/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6523 - acc: 0.7843 - val_loss: 0.6687 - val_acc: 0.7744
Epoch 44/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6464 - acc: 0.7834 - val_loss: 0.6499 - val_acc: 0.7850
Epoch 45/100
50000/50000 [==============================] - 61s 1ms/step - loss: 0.6497 - acc: 0.7817 - val_loss: 0.6373 - val_acc: 0.7885
Epoch 46/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6426 - acc: 0.7857 - val_loss: 0.6574 - val_acc: 0.7798
Epoch 47/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6411 - acc: 0.7861 - val_loss: 0.6650 - val_acc: 0.7747
Epoch 48/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6433 - acc: 0.7837 - val_loss: 0.6417 - val_acc: 0.7849
Epoch 49/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6340 - acc: 0.7884 - val_loss: 0.6528 - val_acc: 0.7808
Epoch 50/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6304 - acc: 0.7899 - val_loss: 0.6835 - val_acc: 0.7718
Epoch 51/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6333 - acc: 0.7888 - val_loss: 0.6643 - val_acc: 0.7811
Epoch 52/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6341 - acc: 0.7892 - val_loss: 0.6743 - val_acc: 0.7771
Epoch 53/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6292 - acc: 0.7901 - val_loss: 0.6718 - val_acc: 0.7791
Epoch 54/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6264 - acc: 0.7921 - val_loss: 0.7050 - val_acc: 0.7695
Epoch 55/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6306 - acc: 0.7889 - val_loss: 0.6449 - val_acc: 0.7827
Epoch 56/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6251 - acc: 0.7897 - val_loss: 0.6468 - val_acc: 0.7864
Epoch 57/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6241 - acc: 0.7925 - val_loss: 0.6533 - val_acc: 0.7873
Epoch 58/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6196 - acc: 0.7931 - val_loss: 0.6254 - val_acc: 0.7900
Epoch 59/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6182 - acc: 0.7932 - val_loss: 0.6798 - val_acc: 0.7825
Epoch 60/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6233 - acc: 0.7923 - val_loss: 0.6540 - val_acc: 0.7809
Epoch 61/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6145 - acc: 0.7929 - val_loss: 0.6315 - val_acc: 0.7888
Epoch 62/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6165 - acc: 0.7957 - val_loss: 0.7382 - val_acc: 0.7625
Epoch 63/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6197 - acc: 0.7922 - val_loss: 0.6395 - val_acc: 0.7863
Epoch 64/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6138 - acc: 0.7969 - val_loss: 0.6364 - val_acc: 0.7933
Epoch 65/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6149 - acc: 0.7933 - val_loss: 0.7006 - val_acc: 0.7765
Epoch 66/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6110 - acc: 0.7964 - val_loss: 0.6410 - val_acc: 0.7935
Epoch 67/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6167 - acc: 0.7959 - val_loss: 0.6971 - val_acc: 0.7735
Epoch 68/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6144 - acc: 0.7948 - val_loss: 0.6359 - val_acc: 0.7886
Epoch 69/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6081 - acc: 0.7953 - val_loss: 0.6760 - val_acc: 0.7806
Epoch 70/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6081 - acc: 0.7967 - val_loss: 0.8297 - val_acc: 0.7483
Epoch 71/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6126 - acc: 0.7954 - val_loss: 0.6333 - val_acc: 0.7862
Epoch 72/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6101 - acc: 0.7956 - val_loss: 0.6288 - val_acc: 0.7912
Epoch 73/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6093 - acc: 0.7954 - val_loss: 0.6757 - val_acc: 0.7826
Epoch 74/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6094 - acc: 0.7971 - val_loss: 0.6583 - val_acc: 0.7798
Epoch 75/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6035 - acc: 0.7990 - val_loss: 0.6672 - val_acc: 0.7907
Epoch 76/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6105 - acc: 0.7982 - val_loss: 0.6463 - val_acc: 0.7873
Epoch 77/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6057 - acc: 0.7982 - val_loss: 0.6547 - val_acc: 0.7851
Epoch 78/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6082 - acc: 0.7975 - val_loss: 0.6877 - val_acc: 0.7842
Epoch 79/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6132 - acc: 0.7960 - val_loss: 0.6699 - val_acc: 0.7885
Epoch 80/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6038 - acc: 0.8005 - val_loss: 0.6489 - val_acc: 0.7815
Epoch 81/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6131 - acc: 0.7955 - val_loss: 0.6733 - val_acc: 0.7831
Epoch 82/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6033 - acc: 0.7978 - val_loss: 0.6786 - val_acc: 0.7835
Epoch 83/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6070 - acc: 0.7964 - val_loss: 0.6717 - val_acc: 0.7785
Epoch 84/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6080 - acc: 0.7963 - val_loss: 0.6456 - val_acc: 0.7955
Epoch 85/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6059 - acc: 0.7995 - val_loss: 0.6863 - val_acc: 0.7740
Epoch 86/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6083 - acc: 0.7977 - val_loss: 0.6559 - val_acc: 0.7790
Epoch 87/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6088 - acc: 0.7984 - val_loss: 0.6856 - val_acc: 0.7777
Epoch 88/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6078 - acc: 0.7976 - val_loss: 0.6665 - val_acc: 0.7808
Epoch 89/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6043 - acc: 0.7991 - val_loss: 0.6526 - val_acc: 0.7859
Epoch 90/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6117 - acc: 0.7987 - val_loss: 0.6480 - val_acc: 0.7870
Epoch 91/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6083 - acc: 0.7971 - val_loss: 0.7220 - val_acc: 0.7675
Epoch 92/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6048 - acc: 0.7992 - val_loss: 0.6968 - val_acc: 0.7729
Epoch 93/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6052 - acc: 0.8002 - val_loss: 0.6943 - val_acc: 0.7800
Epoch 94/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6081 - acc: 0.7974 - val_loss: 0.7371 - val_acc: 0.7555
Epoch 95/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6029 - acc: 0.7993 - val_loss: 0.6745 - val_acc: 0.7852
Epoch 96/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6081 - acc: 0.7996 - val_loss: 0.6554 - val_acc: 0.7877
Epoch 97/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6065 - acc: 0.7992 - val_loss: 0.6737 - val_acc: 0.7834
Epoch 98/100
50000/50000 [==============================] - 62s 1ms/step - loss: 0.6076 - acc: 0.7992 - val_loss: 0.6841 - val_acc: 0.7816
Epoch 99/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6069 - acc: 0.7981 - val_loss: 0.6793 - val_acc: 0.7770
Epoch 100/100
50000/50000 [==============================] - 63s 1ms/step - loss: 0.6060 - acc: 0.7991 - val_loss: 0.6845 - val_acc: 0.7772
Saved trained model at E:\Master\Semester 2\ML\Homeworks\Pure Code\saved_models\simple_cnn.h5
Confusion matrix, without normalization
[[829  11  35   8  26  11   3  11  32  34]
 [ 17 899   0   3   2  10   3   2  10  54]
 [ 60   3 607  21 121 109  35  34   6   4]
 [ 31   6  48 446  73 315  29  36   7   9]
 [ 12   1  21  19 811  54  17  54   6   5]
 [ 10   1  19  41  41 841   3  38   0   6]
 [ 11   3  38  42  64  52 774   6   4   6]
 [ 10   0  14   6  45  88   3 829   0   5]
 [ 75  23   8   5   4  11   2   5 849  18]
 [ 19  50   1   4   6   9   2  10  12 887]]
Simple CNN Not using data augmentation: 0.7772

################################################
Using TensorFlow backend.
X_train shape: (50000, 32, 32, 3)
50000 training samples
10000 validation samples
CNN4.PY:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(32, 32, 3..., activation="relu", padding="same")`
  32, 32, 3), border_mode='same', activation='relu'))
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
CNN4.PY:64: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation="relu", padding="same")`
  model4.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))
CNN4.PY:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding="same")`
  model4.add(Convolution2D(64, 3, 3, border_mode='same'))
CNN4.PY:70: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`
  model4.add(Convolution2D(64, 3, 3))
CNN4.PY:76: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, activation="relu", kernel_constraint=<keras.con...)`
  model4.add(Dense(512, activation='relu', W_constraint=maxnorm(3)))
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 32, 32, 32)        896
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 32, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 16, 32)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496
_________________________________________________________________
activation_1 (Activation)    (None, 16, 16, 64)        0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928
_________________________________________________________________
activation_2 (Activation)    (None, 14, 14, 64)        0
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0
_________________________________________________________________
dropout_3 (Dropout)          (None, 7, 7, 64)          0
_________________________________________________________________
flatten_1 (Flatten)          (None, 3136)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               1606144
_________________________________________________________________
dropout_4 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_2 (Dense)              (None, 10)                5130
=================================================================
Total params: 1,676,842
Trainable params: 1,676,842
Non-trainable params: 0
_________________________________________________________________
None
Augmented Data Training.
CNN4.PY:121: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.
  callbacks=[fmon]
CNN4.PY:121: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., verbose=1, callbacks=[<kerutils..., steps_per_epoch=1562, epochs=400)`
  callbacks=[fmon]
WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train begin: 2019-07-12 11:11:16
Stop file: stop_training_file.keras (create this file to stop training gracefully)
Pause file: pause_training_file.keras (create this file to pause training and view graphs)
do_validation = True
epochs = 400
metrics = ['loss', 'acc', 'val_loss', 'val_acc']
steps = 1562
verbose = 1
Epoch 1/400
2019-07-12 11:11:16.113448: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-07-12 11:11:17.116458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:03:00.0
totalMemory: 4.00GiB freeMemory: 3.35GiB
2019-07-12 11:11:17.120310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-07-12 11:11:20.382789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-12 11:11:20.385825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-07-12 11:11:20.387679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-07-12 11:11:20.391366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3058 MB memory) -> physical GPU (device: 0, name: GeForce 840M, pci bus id: 0000:03:00.0, compute capability: 5.0)
2019-07-12 11:11:21.142122: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
1562/1562 [==============================] - 86s 55ms/step - loss: 1.7649 - acc: 0.3490 - val_loss: 1.4545 - val_acc: 0.4748
Epoch 2/400
1562/1562 [==============================] - 79s 50ms/step - loss: 1.4013 - acc: 0.4885 - val_loss: 1.2088 - val_acc: 0.5673
Epoch 3/400
1562/1562 [==============================] - 79s 50ms/step - loss: 1.2541 - acc: 0.5485 - val_loss: 1.0435 - val_acc: 0.6270
Epoch 4/400
1562/1562 [==============================] - 80s 51ms/step - loss: 1.1571 - acc: 0.5873 - val_loss: 0.9825 - val_acc: 0.6527
Epoch 5/400
1562/1562 [==============================] - 81s 52ms/step - loss: 1.0702 - acc: 0.6181 - val_loss: 0.8967 - val_acc: 0.6825
.Epoch 6/400
1562/1562 [==============================] - 80s 51ms/step - loss: 1.0138 - acc: 0.6396 - val_loss: 0.8378 - val_acc: 0.7080
Epoch 7/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.9636 - acc: 0.6613 - val_loss: 0.8227 - val_acc: 0.7112
Epoch 8/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.9121 - acc: 0.6762 - val_loss: 0.7522 - val_acc: 0.7392
Epoch 9/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.8869 - acc: 0.6881 - val_loss: 0.7217 - val_acc: 0.7551
.Epoch 10/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.8485 - acc: 0.6980 - val_loss: 0.7153 - val_acc: 0.7522
Epoch 11/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.8280 - acc: 0.7075 - val_loss: 0.6822 - val_acc: 0.7632
Epoch 12/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.8035 - acc: 0.7170 - val_loss: 0.6791 - val_acc: 0.7642
Epoch 13/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.7798 - acc: 0.7256 - val_loss: 0.6612 - val_acc: 0.7687
.Epoch 14/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.7622 - acc: 0.7333 - val_loss: 0.6286 - val_acc: 0.7848
Epoch 15/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.7485 - acc: 0.7376 - val_loss: 0.6176 - val_acc: 0.7869
Epoch 16/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.7309 - acc: 0.7454 - val_loss: 0.6478 - val_acc: 0.7758
Epoch 17/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.7192 - acc: 0.7494 - val_loss: 0.6524 - val_acc: 0.7760
.Epoch 18/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.6982 - acc: 0.7544 - val_loss: 0.6191 - val_acc: 0.7889
Epoch 19/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.6895 - acc: 0.7601 - val_loss: 0.5899 - val_acc: 0.7983
Epoch 20/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.6706 - acc: 0.7642 - val_loss: 0.6072 - val_acc: 0.7898
Epoch 21/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.6657 - acc: 0.7662 - val_loss: 0.6032 - val_acc: 0.7943
.05% epoch=20, acc=0.766210, loss=0.665716, val_acc=0.794300, val_loss=0.603220, time=0.464 hours
Epoch 22/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.6553 - acc: 0.7717 - val_loss: 0.5835 - val_acc: 0.8009
Epoch 23/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.6383 - acc: 0.7757 - val_loss: 0.5861 - val_acc: 0.8033
Epoch 24/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.6353 - acc: 0.7765 - val_loss: 0.5867 - val_acc: 0.8019
Epoch 25/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.6242 - acc: 0.7810 - val_loss: 0.5699 - val_acc: 0.8073
.Epoch 26/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.6131 - acc: 0.7856 - val_loss: 0.5711 - val_acc: 0.8080
Epoch 27/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.6062 - acc: 0.7870 - val_loss: 0.5813 - val_acc: 0.8053
Epoch 28/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.6027 - acc: 0.7871 - val_loss: 0.5707 - val_acc: 0.8074
Epoch 29/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5918 - acc: 0.7944 - val_loss: 0.5730 - val_acc: 0.8115
.Epoch 30/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5836 - acc: 0.7964 - val_loss: 0.5554 - val_acc: 0.8130
Epoch 31/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5737 - acc: 0.7988 - val_loss: 0.5518 - val_acc: 0.8159
Epoch 32/400
1562/1562 [==============================] - 80s 52ms/step - loss: 0.5742 - acc: 0.7998 - val_loss: 0.5461 - val_acc: 0.8177
Epoch 33/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.5590 - acc: 0.8048 - val_loss: 0.5335 - val_acc: 0.8216
.Epoch 34/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5574 - acc: 0.8058 - val_loss: 0.5394 - val_acc: 0.8253
Epoch 35/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.5484 - acc: 0.8084 - val_loss: 0.5086 - val_acc: 0.8307
Epoch 36/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5405 - acc: 0.8089 - val_loss: 0.5423 - val_acc: 0.8184
Epoch 37/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5318 - acc: 0.8144 - val_loss: 0.5379 - val_acc: 0.8254
.Epoch 38/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.5300 - acc: 0.8150 - val_loss: 0.5234 - val_acc: 0.8299
Epoch 39/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5283 - acc: 0.8127 - val_loss: 0.5345 - val_acc: 0.8269
Epoch 40/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.5200 - acc: 0.8177 - val_loss: 0.5319 - val_acc: 0.8246
Epoch 41/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.5140 - acc: 0.8209 - val_loss: 0.5082 - val_acc: 0.8305
.10% epoch=40, acc=0.820885, loss=0.513938, val_acc=0.830500, val_loss=0.508224, time=0.910 hours
Epoch 42/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.5127 - acc: 0.8200 - val_loss: 0.4979 - val_acc: 0.8353
Epoch 43/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.5035 - acc: 0.8228 - val_loss: 0.5021 - val_acc: 0.8338
Epoch 44/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.4919 - acc: 0.8283 - val_loss: 0.5020 - val_acc: 0.8356
Epoch 45/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4968 - acc: 0.8235 - val_loss: 0.5110 - val_acc: 0.8317
.Epoch 46/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4899 - acc: 0.8284 - val_loss: 0.5142 - val_acc: 0.8296
Epoch 47/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.4861 - acc: 0.8300 - val_loss: 0.5151 - val_acc: 0.8341
Epoch 48/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.4845 - acc: 0.8303 - val_loss: 0.5169 - val_acc: 0.8293
Epoch 49/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.4773 - acc: 0.8349 - val_loss: 0.4942 - val_acc: 0.8347
.Epoch 50/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.4765 - acc: 0.8335 - val_loss: 0.5074 - val_acc: 0.8317
Epoch 51/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.4723 - acc: 0.8328 - val_loss: 0.4753 - val_acc: 0.8417
Epoch 52/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4623 - acc: 0.8383 - val_loss: 0.4991 - val_acc: 0.8359
Epoch 53/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4621 - acc: 0.8375 - val_loss: 0.5029 - val_acc: 0.8365
.Epoch 54/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4586 - acc: 0.8381 - val_loss: 0.5146 - val_acc: 0.8346
Epoch 55/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.4565 - acc: 0.8396 - val_loss: 0.4937 - val_acc: 0.8397
Epoch 56/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.4487 - acc: 0.8431 - val_loss: 0.4720 - val_acc: 0.8436
Epoch 57/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4516 - acc: 0.8419 - val_loss: 0.4854 - val_acc: 0.8443
.Epoch 58/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.4432 - acc: 0.8448 - val_loss: 0.4890 - val_acc: 0.8398
Epoch 59/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.4432 - acc: 0.8444 - val_loss: 0.4875 - val_acc: 0.8413
Epoch 60/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.4372 - acc: 0.8457 - val_loss: 0.4801 - val_acc: 0.8415
Epoch 61/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.4375 - acc: 0.8466 - val_loss: 0.4829 - val_acc: 0.8412
.15% epoch=60, acc=0.846562, loss=0.437596, val_acc=0.841200, val_loss=0.482922, time=1.356 hours
Epoch 62/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4297 - acc: 0.8478 - val_loss: 0.4953 - val_acc: 0.8383
Epoch 63/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.4304 - acc: 0.8459 - val_loss: 0.4924 - val_acc: 0.8384
Epoch 64/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.4230 - acc: 0.8517 - val_loss: 0.4937 - val_acc: 0.8430
Epoch 65/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4246 - acc: 0.8497 - val_loss: 0.4858 - val_acc: 0.8429
.Epoch 66/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4210 - acc: 0.8532 - val_loss: 0.4812 - val_acc: 0.8440
Epoch 67/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4240 - acc: 0.8494 - val_loss: 0.4790 - val_acc: 0.8439
Epoch 68/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.4128 - acc: 0.8554 - val_loss: 0.4734 - val_acc: 0.8467
Epoch 69/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4147 - acc: 0.8547 - val_loss: 0.5163 - val_acc: 0.8358
.Epoch 70/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.4116 - acc: 0.8549 - val_loss: 0.5014 - val_acc: 0.8408
Epoch 71/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.4065 - acc: 0.8588 - val_loss: 0.4913 - val_acc: 0.8379
Epoch 72/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.4002 - acc: 0.8586 - val_loss: 0.4793 - val_acc: 0.8438
Epoch 73/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3977 - acc: 0.8585 - val_loss: 0.4772 - val_acc: 0.8460
.Epoch 74/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.3986 - acc: 0.8606 - val_loss: 0.4951 - val_acc: 0.8395
Epoch 75/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.3924 - acc: 0.8622 - val_loss: 0.4770 - val_acc: 0.8461
Epoch 76/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.3972 - acc: 0.8603 - val_loss: 0.4695 - val_acc: 0.8461
Epoch 77/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.3878 - acc: 0.8622 - val_loss: 0.4826 - val_acc: 0.8436
.Epoch 78/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3917 - acc: 0.8625 - val_loss: 0.4776 - val_acc: 0.8457
Epoch 79/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.3874 - acc: 0.8629 - val_loss: 0.4684 - val_acc: 0.8478
Epoch 80/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.3830 - acc: 0.8635 - val_loss: 0.4773 - val_acc: 0.8489
Epoch 81/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.3829 - acc: 0.8644 - val_loss: 0.4669 - val_acc: 0.8502
.20% epoch=80, acc=0.864373, loss=0.382921, val_acc=0.850200, val_loss=0.466869, time=1.799 hours
Epoch 82/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.3774 - acc: 0.8662 - val_loss: 0.4717 - val_acc: 0.8492
Epoch 83/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.3829 - acc: 0.8656 - val_loss: 0.4674 - val_acc: 0.8537
Epoch 84/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.3736 - acc: 0.8679 - val_loss: 0.4657 - val_acc: 0.8501
Epoch 85/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.3762 - acc: 0.8672 - val_loss: 0.4690 - val_acc: 0.8509
.Epoch 86/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.3688 - acc: 0.8702 - val_loss: 0.4561 - val_acc: 0.8517
Epoch 87/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3718 - acc: 0.8707 - val_loss: 0.4640 - val_acc: 0.8506
Epoch 88/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3667 - acc: 0.8721 - val_loss: 0.4843 - val_acc: 0.8437
Epoch 89/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.3646 - acc: 0.8713 - val_loss: 0.4711 - val_acc: 0.8494
.Epoch 90/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3696 - acc: 0.8700 - val_loss: 0.4534 - val_acc: 0.8575
Epoch 91/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3608 - acc: 0.8726 - val_loss: 0.4716 - val_acc: 0.8489
Epoch 92/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.3567 - acc: 0.8735 - val_loss: 0.4666 - val_acc: 0.8500
Epoch 93/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3637 - acc: 0.8718 - val_loss: 0.4654 - val_acc: 0.8492
.Epoch 94/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3532 - acc: 0.8750 - val_loss: 0.4752 - val_acc: 0.8479
Epoch 95/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3566 - acc: 0.8749 - val_loss: 0.4910 - val_acc: 0.8457
Epoch 96/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3504 - acc: 0.8766 - val_loss: 0.4625 - val_acc: 0.8562
Epoch 97/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3430 - acc: 0.8784 - val_loss: 0.4698 - val_acc: 0.8545
.Epoch 98/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3528 - acc: 0.8749 - val_loss: 0.4714 - val_acc: 0.8506
Epoch 99/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3472 - acc: 0.8785 - val_loss: 0.4550 - val_acc: 0.8553
Epoch 100/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3419 - acc: 0.8781 - val_loss: 0.4671 - val_acc: 0.8528
Epoch 101/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3474 - acc: 0.8776 - val_loss: 0.4624 - val_acc: 0.8528
.25% epoch=100, acc=0.877662, loss=0.347374, val_acc=0.852800, val_loss=0.462411, time=2.244 hours
Epoch 102/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.3484 - acc: 0.8760 - val_loss: 0.4625 - val_acc: 0.8538
Epoch 103/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.3376 - acc: 0.8785 - val_loss: 0.4742 - val_acc: 0.8476
Epoch 104/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.3420 - acc: 0.8797 - val_loss: 0.4587 - val_acc: 0.8505
Epoch 105/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.3406 - acc: 0.8786 - val_loss: 0.4604 - val_acc: 0.8553
.Epoch 106/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.3405 - acc: 0.8778 - val_loss: 0.4664 - val_acc: 0.8531
Epoch 107/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.3337 - acc: 0.8827 - val_loss: 0.4768 - val_acc: 0.8499
Epoch 108/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.3351 - acc: 0.8808 - val_loss: 0.4744 - val_acc: 0.8509
Epoch 109/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3362 - acc: 0.8804 - val_loss: 0.4583 - val_acc: 0.8541
.Epoch 110/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3324 - acc: 0.8814 - val_loss: 0.4572 - val_acc: 0.8534
Epoch 111/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3285 - acc: 0.8846 - val_loss: 0.4544 - val_acc: 0.8573
Epoch 112/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3329 - acc: 0.8823 - val_loss: 0.4619 - val_acc: 0.8549
Epoch 113/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3316 - acc: 0.8832 - val_loss: 0.4540 - val_acc: 0.8564
.Epoch 114/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.3267 - acc: 0.8845 - val_loss: 0.4641 - val_acc: 0.8526
Epoch 115/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3229 - acc: 0.8875 - val_loss: 0.4641 - val_acc: 0.8561
Epoch 116/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.3234 - acc: 0.8863 - val_loss: 0.4735 - val_acc: 0.8528
Epoch 117/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.3209 - acc: 0.8882 - val_loss: 0.4525 - val_acc: 0.8572
.Epoch 118/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.3203 - acc: 0.8859 - val_loss: 0.4816 - val_acc: 0.8538
Epoch 119/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3243 - acc: 0.8852 - val_loss: 0.4688 - val_acc: 0.8533
Epoch 120/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3182 - acc: 0.8852 - val_loss: 0.4634 - val_acc: 0.8556
Epoch 121/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.3233 - acc: 0.8858 - val_loss: 0.4587 - val_acc: 0.8588
.30% epoch=120, acc=0.885867, loss=0.323198, val_acc=0.858800, val_loss=0.458697, time=2.695 hours
Epoch 122/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.3206 - acc: 0.8857 - val_loss: 0.4501 - val_acc: 0.8589
Epoch 123/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3236 - acc: 0.8841 - val_loss: 0.4509 - val_acc: 0.8607
Epoch 124/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3151 - acc: 0.8878 - val_loss: 0.4563 - val_acc: 0.8578
Epoch 125/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3108 - acc: 0.8896 - val_loss: 0.4452 - val_acc: 0.8615
.Epoch 126/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3071 - acc: 0.8912 - val_loss: 0.4557 - val_acc: 0.8594
Epoch 127/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3089 - acc: 0.8881 - val_loss: 0.4575 - val_acc: 0.8554
Epoch 128/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3076 - acc: 0.8909 - val_loss: 0.4445 - val_acc: 0.8595
Epoch 129/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3142 - acc: 0.8884 - val_loss: 0.4599 - val_acc: 0.8550
.Epoch 130/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3021 - acc: 0.8928 - val_loss: 0.4509 - val_acc: 0.8590
Epoch 131/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3028 - acc: 0.8926 - val_loss: 0.4453 - val_acc: 0.8601
Epoch 132/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3040 - acc: 0.8925 - val_loss: 0.4664 - val_acc: 0.8558
Epoch 133/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2969 - acc: 0.8934 - val_loss: 0.4651 - val_acc: 0.8573
.Epoch 134/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.3067 - acc: 0.8921 - val_loss: 0.4533 - val_acc: 0.8611
Epoch 135/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2999 - acc: 0.8944 - val_loss: 0.4635 - val_acc: 0.8574
Epoch 136/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3049 - acc: 0.8912 - val_loss: 0.4526 - val_acc: 0.8602
Epoch 137/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.3021 - acc: 0.8917 - val_loss: 0.4587 - val_acc: 0.8571
.Epoch 138/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.3013 - acc: 0.8925 - val_loss: 0.4507 - val_acc: 0.8605
Epoch 139/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2984 - acc: 0.8935 - val_loss: 0.4565 - val_acc: 0.8578
Epoch 140/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2980 - acc: 0.8949 - val_loss: 0.4531 - val_acc: 0.8606
Epoch 141/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.2984 - acc: 0.8946 - val_loss: 0.4620 - val_acc: 0.8584
.35% epoch=140, acc=0.894653, loss=0.298440, val_acc=0.858400, val_loss=0.461964, time=3.144 hours
Epoch 142/400
1562/1562 [==============================] - 82s 52ms/step - loss: 0.2940 - acc: 0.8954 - val_loss: 0.4480 - val_acc: 0.8631
Epoch 143/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2892 - acc: 0.8983 - val_loss: 0.4616 - val_acc: 0.8607
Epoch 144/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2931 - acc: 0.8964 - val_loss: 0.4859 - val_acc: 0.8543
Epoch 145/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2944 - acc: 0.8947 - val_loss: 0.4667 - val_acc: 0.8528
.Epoch 146/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2900 - acc: 0.8988 - val_loss: 0.4557 - val_acc: 0.8592
Epoch 147/400
1562/1562 [==============================] - 84s 54ms/step - loss: 0.2879 - acc: 0.8983 - val_loss: 0.4644 - val_acc: 0.8575
Epoch 148/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2885 - acc: 0.8975 - val_loss: 0.4881 - val_acc: 0.8495
Epoch 149/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2838 - acc: 0.8994 - val_loss: 0.4592 - val_acc: 0.8599
.Epoch 150/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2879 - acc: 0.8987 - val_loss: 0.4563 - val_acc: 0.8593
Epoch 151/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2870 - acc: 0.8978 - val_loss: 0.4735 - val_acc: 0.8553
Epoch 152/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2831 - acc: 0.8983 - val_loss: 0.4559 - val_acc: 0.8581
Epoch 153/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2803 - acc: 0.9026 - val_loss: 0.4525 - val_acc: 0.8612
.Epoch 154/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2803 - acc: 0.8999 - val_loss: 0.4697 - val_acc: 0.8589
Epoch 155/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2805 - acc: 0.8998 - val_loss: 0.4703 - val_acc: 0.8570
Epoch 156/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2880 - acc: 0.8973 - val_loss: 0.4545 - val_acc: 0.8618
Epoch 157/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2776 - acc: 0.9017 - val_loss: 0.4814 - val_acc: 0.8561
.Epoch 158/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2807 - acc: 0.8995 - val_loss: 0.4701 - val_acc: 0.8558
Epoch 159/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2789 - acc: 0.9032 - val_loss: 0.4498 - val_acc: 0.8629
Epoch 160/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2779 - acc: 0.9019 - val_loss: 0.4796 - val_acc: 0.8541
Epoch 161/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2764 - acc: 0.9030 - val_loss: 0.4591 - val_acc: 0.8614
.40% epoch=160, acc=0.902998, loss=0.276511, val_acc=0.861400, val_loss=0.459108, time=3.592 hours
Epoch 162/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2757 - acc: 0.9020 - val_loss: 0.4684 - val_acc: 0.8570
Epoch 163/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2773 - acc: 0.9015 - val_loss: 0.4644 - val_acc: 0.8581
Epoch 164/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2699 - acc: 0.9037 - val_loss: 0.4731 - val_acc: 0.8554
Epoch 165/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2778 - acc: 0.9037 - val_loss: 0.4528 - val_acc: 0.8604
.Epoch 166/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2737 - acc: 0.9020 - val_loss: 0.4597 - val_acc: 0.8587
Epoch 167/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2728 - acc: 0.9027 - val_loss: 0.4770 - val_acc: 0.8540
Epoch 168/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2697 - acc: 0.9044 - val_loss: 0.4590 - val_acc: 0.8558
Epoch 169/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2677 - acc: 0.9056 - val_loss: 0.4448 - val_acc: 0.8630
.Epoch 170/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2694 - acc: 0.9050 - val_loss: 0.4578 - val_acc: 0.8621
Epoch 171/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2712 - acc: 0.9042 - val_loss: 0.4605 - val_acc: 0.8605
Epoch 172/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2640 - acc: 0.9063 - val_loss: 0.4665 - val_acc: 0.8606
Epoch 173/400
1562/1562 [==============================] - 80s 52ms/step - loss: 0.2693 - acc: 0.9038 - val_loss: 0.4625 - val_acc: 0.8624
.Epoch 174/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2659 - acc: 0.9058 - val_loss: 0.4587 - val_acc: 0.8590
Epoch 175/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2625 - acc: 0.9058 - val_loss: 0.4606 - val_acc: 0.8588
Epoch 176/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2688 - acc: 0.9049 - val_loss: 0.4714 - val_acc: 0.8561
Epoch 177/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2606 - acc: 0.9079 - val_loss: 0.4660 - val_acc: 0.8584
.Epoch 178/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2636 - acc: 0.9076 - val_loss: 0.4716 - val_acc: 0.8537
Epoch 179/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2627 - acc: 0.9069 - val_loss: 0.4479 - val_acc: 0.8632
Epoch 180/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2610 - acc: 0.9079 - val_loss: 0.4811 - val_acc: 0.8549
Epoch 181/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2614 - acc: 0.9070 - val_loss: 0.4512 - val_acc: 0.8595
.45% epoch=180, acc=0.906980, loss=0.261407, val_acc=0.859500, val_loss=0.451187, time=4.037 hours
Epoch 182/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2595 - acc: 0.9072 - val_loss: 0.4729 - val_acc: 0.8586
Epoch 183/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2616 - acc: 0.9080 - val_loss: 0.4561 - val_acc: 0.8594
Epoch 184/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2618 - acc: 0.9072 - val_loss: 0.4658 - val_acc: 0.8614
Epoch 185/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2632 - acc: 0.9073 - val_loss: 0.4612 - val_acc: 0.8645
.Epoch 186/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2566 - acc: 0.9084 - val_loss: 0.4547 - val_acc: 0.8630
Epoch 187/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2538 - acc: 0.9103 - val_loss: 0.4652 - val_acc: 0.8586
Epoch 188/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2579 - acc: 0.9104 - val_loss: 0.4514 - val_acc: 0.8654
Epoch 189/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2570 - acc: 0.9078 - val_loss: 0.4659 - val_acc: 0.8576
.Epoch 190/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2607 - acc: 0.9081 - val_loss: 0.4729 - val_acc: 0.8581
Epoch 191/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2558 - acc: 0.9089 - val_loss: 0.4510 - val_acc: 0.8623
Epoch 192/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2488 - acc: 0.9131 - val_loss: 0.4521 - val_acc: 0.8623
Epoch 193/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2553 - acc: 0.9092 - val_loss: 0.4572 - val_acc: 0.8594
.Epoch 194/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2563 - acc: 0.9107 - val_loss: 0.4518 - val_acc: 0.8600
Epoch 195/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2504 - acc: 0.9116 - val_loss: 0.4516 - val_acc: 0.8626
Epoch 196/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2560 - acc: 0.9099 - val_loss: 0.4422 - val_acc: 0.8651
Epoch 197/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2450 - acc: 0.9127 - val_loss: 0.4570 - val_acc: 0.8620
.Epoch 198/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2497 - acc: 0.9110 - val_loss: 0.4548 - val_acc: 0.8643
Epoch 199/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2540 - acc: 0.9098 - val_loss: 0.4707 - val_acc: 0.8587
Epoch 200/400
1562/1562 [==============================] - 85s 54ms/step - loss: 0.2465 - acc: 0.9127 - val_loss: 0.4628 - val_acc: 0.8611
Epoch 201/400
1562/1562 [==============================] - 84s 53ms/step - loss: 0.2501 - acc: 0.9104 - val_loss: 0.4653 - val_acc: 0.8587
.50% epoch=200, acc=0.910403, loss=0.250124, val_acc=0.858700, val_loss=0.465265, time=4.488 hours
Epoch 202/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2514 - acc: 0.9096 - val_loss: 0.4653 - val_acc: 0.8624
Epoch 203/400
1562/1562 [==============================] - 82s 53ms/step - loss: 0.2449 - acc: 0.9132 - val_loss: 0.4596 - val_acc: 0.8642
Epoch 204/400
1562/1562 [==============================] - 83s 53ms/step - loss: 0.2492 - acc: 0.9116 - val_loss: 0.4468 - val_acc: 0.8652
Epoch 205/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.2505 - acc: 0.9121 - val_loss: 0.4583 - val_acc: 0.8595
.Epoch 206/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2546 - acc: 0.9110 - val_loss: 0.4536 - val_acc: 0.8616
Epoch 207/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2476 - acc: 0.9128 - val_loss: 0.4636 - val_acc: 0.8624
Epoch 208/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2446 - acc: 0.9134 - val_loss: 0.4630 - val_acc: 0.8631
Epoch 209/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2447 - acc: 0.9128 - val_loss: 0.4724 - val_acc: 0.8597
.Epoch 210/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2445 - acc: 0.9124 - val_loss: 0.4604 - val_acc: 0.8615
Epoch 211/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2469 - acc: 0.9121 - val_loss: 0.4586 - val_acc: 0.8624
Epoch 212/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2481 - acc: 0.9126 - val_loss: 0.4530 - val_acc: 0.8637
Epoch 213/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2373 - acc: 0.9161 - val_loss: 0.4505 - val_acc: 0.8655
.Epoch 214/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2463 - acc: 0.9132 - val_loss: 0.4618 - val_acc: 0.8622
Epoch 215/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2386 - acc: 0.9151 - val_loss: 0.4676 - val_acc: 0.8607
Epoch 216/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2401 - acc: 0.9160 - val_loss: 0.4524 - val_acc: 0.8658
Epoch 217/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2456 - acc: 0.9128 - val_loss: 0.4439 - val_acc: 0.8661
.Epoch 218/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2415 - acc: 0.9148 - val_loss: 0.4450 - val_acc: 0.8688
Epoch 219/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2387 - acc: 0.9154 - val_loss: 0.4761 - val_acc: 0.8616
Epoch 220/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2371 - acc: 0.9152 - val_loss: 0.4474 - val_acc: 0.8661
Epoch 221/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2424 - acc: 0.9141 - val_loss: 0.4497 - val_acc: 0.8653
.55% epoch=220, acc=0.914145, loss=0.242391, val_acc=0.865300, val_loss=0.449750, time=4.934 hours
Epoch 222/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2371 - acc: 0.9165 - val_loss: 0.4525 - val_acc: 0.8657
Epoch 223/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2355 - acc: 0.9167 - val_loss: 0.4579 - val_acc: 0.8639
Epoch 224/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2363 - acc: 0.9161 - val_loss: 0.4677 - val_acc: 0.8624
Epoch 225/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2349 - acc: 0.9165 - val_loss: 0.4619 - val_acc: 0.8644
.Epoch 226/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2397 - acc: 0.9149 - val_loss: 0.4582 - val_acc: 0.8636
Epoch 227/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2334 - acc: 0.9161 - val_loss: 0.4689 - val_acc: 0.8620
Epoch 228/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2357 - acc: 0.9160 - val_loss: 0.4568 - val_acc: 0.8637
Epoch 229/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2343 - acc: 0.9178 - val_loss: 0.4591 - val_acc: 0.8636
.Epoch 230/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2346 - acc: 0.9167 - val_loss: 0.4658 - val_acc: 0.8631
Epoch 231/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2362 - acc: 0.9150 - val_loss: 0.4481 - val_acc: 0.8673
Epoch 232/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2359 - acc: 0.9154 - val_loss: 0.4517 - val_acc: 0.8637
Epoch 233/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2363 - acc: 0.9155 - val_loss: 0.4583 - val_acc: 0.8646
.Epoch 234/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2314 - acc: 0.9185 - val_loss: 0.4551 - val_acc: 0.8663
Epoch 235/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2409 - acc: 0.9152 - val_loss: 0.4572 - val_acc: 0.8637
Epoch 236/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2310 - acc: 0.9168 - val_loss: 0.4400 - val_acc: 0.8681
Epoch 237/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2312 - acc: 0.9192 - val_loss: 0.4476 - val_acc: 0.8678
.Epoch 238/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2295 - acc: 0.9190 - val_loss: 0.4554 - val_acc: 0.8616
Epoch 239/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2364 - acc: 0.9166 - val_loss: 0.4681 - val_acc: 0.8615
Epoch 240/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2307 - acc: 0.9172 - val_loss: 0.4557 - val_acc: 0.8645
Epoch 241/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2311 - acc: 0.9187 - val_loss: 0.4443 - val_acc: 0.8689
.60% epoch=240, acc=0.918668, loss=0.231144, val_acc=0.868900, val_loss=0.444280, time=5.375 hours
Epoch 242/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2278 - acc: 0.9197 - val_loss: 0.4581 - val_acc: 0.8626
Epoch 243/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2317 - acc: 0.9176 - val_loss: 0.4597 - val_acc: 0.8628
Epoch 244/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2311 - acc: 0.9180 - val_loss: 0.4513 - val_acc: 0.8643
Epoch 245/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2318 - acc: 0.9189 - val_loss: 0.4739 - val_acc: 0.8606
.Epoch 246/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2264 - acc: 0.9198 - val_loss: 0.4548 - val_acc: 0.8657
Epoch 247/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2280 - acc: 0.9198 - val_loss: 0.4620 - val_acc: 0.8597
Epoch 248/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2301 - acc: 0.9176 - val_loss: 0.4544 - val_acc: 0.8631
Epoch 249/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2317 - acc: 0.9182 - val_loss: 0.4607 - val_acc: 0.8626
.Epoch 250/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2261 - acc: 0.9195 - val_loss: 0.4566 - val_acc: 0.8637
Epoch 251/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2256 - acc: 0.9191 - val_loss: 0.4709 - val_acc: 0.8603
Epoch 252/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2334 - acc: 0.9180 - val_loss: 0.4597 - val_acc: 0.8620
Epoch 253/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2227 - acc: 0.9209 - val_loss: 0.4501 - val_acc: 0.8653
.Epoch 254/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2245 - acc: 0.9210 - val_loss: 0.4720 - val_acc: 0.8617
Epoch 255/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2221 - acc: 0.9222 - val_loss: 0.4595 - val_acc: 0.8643
Epoch 256/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2231 - acc: 0.9212 - val_loss: 0.4707 - val_acc: 0.8621
Epoch 257/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.2231 - acc: 0.9219 - val_loss: 0.4514 - val_acc: 0.8667
.Epoch 258/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2203 - acc: 0.9222 - val_loss: 0.4818 - val_acc: 0.8572
Epoch 259/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2198 - acc: 0.9212 - val_loss: 0.4659 - val_acc: 0.8608
Epoch 260/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2279 - acc: 0.9188 - val_loss: 0.4542 - val_acc: 0.8644
Epoch 261/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2267 - acc: 0.9210 - val_loss: 0.4651 - val_acc: 0.8637
.65% epoch=260, acc=0.921049, loss=0.226603, val_acc=0.863700, val_loss=0.465137, time=5.816 hours
Epoch 262/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2257 - acc: 0.9199 - val_loss: 0.4635 - val_acc: 0.8631
Epoch 263/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2230 - acc: 0.9209 - val_loss: 0.4740 - val_acc: 0.8596
Epoch 264/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2199 - acc: 0.9220 - val_loss: 0.4572 - val_acc: 0.8658
Epoch 265/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2155 - acc: 0.9227 - val_loss: 0.4670 - val_acc: 0.8654
.Epoch 266/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2176 - acc: 0.9219 - val_loss: 0.4820 - val_acc: 0.8634
Epoch 267/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2196 - acc: 0.9214 - val_loss: 0.4594 - val_acc: 0.8640
Epoch 268/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2216 - acc: 0.9207 - val_loss: 0.4837 - val_acc: 0.8570
Epoch 269/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2201 - acc: 0.9226 - val_loss: 0.4750 - val_acc: 0.8602
.Epoch 270/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2185 - acc: 0.9240 - val_loss: 0.4792 - val_acc: 0.8626
Epoch 271/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2208 - acc: 0.9218 - val_loss: 0.4612 - val_acc: 0.8639
Epoch 272/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2211 - acc: 0.9223 - val_loss: 0.4553 - val_acc: 0.8634
Epoch 273/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2142 - acc: 0.9241 - val_loss: 0.4646 - val_acc: 0.8641
.Epoch 274/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2130 - acc: 0.9231 - val_loss: 0.4686 - val_acc: 0.8646
Epoch 275/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2188 - acc: 0.9227 - val_loss: 0.4639 - val_acc: 0.8636
Epoch 276/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2154 - acc: 0.9236 - val_loss: 0.4552 - val_acc: 0.8681
Epoch 277/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2195 - acc: 0.9226 - val_loss: 0.4574 - val_acc: 0.8639
.Epoch 278/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2114 - acc: 0.9242 - val_loss: 0.4724 - val_acc: 0.8599
Epoch 279/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2187 - acc: 0.9212 - val_loss: 0.4797 - val_acc: 0.8624
Epoch 280/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2154 - acc: 0.9246 - val_loss: 0.4753 - val_acc: 0.8614
Epoch 281/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2145 - acc: 0.9246 - val_loss: 0.4640 - val_acc: 0.8618
.70% epoch=280, acc=0.924672, loss=0.214237, val_acc=0.861800, val_loss=0.464050, time=6.254 hours
Epoch 282/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2171 - acc: 0.9231 - val_loss: 0.4753 - val_acc: 0.8598
Epoch 283/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2175 - acc: 0.9212 - val_loss: 0.4716 - val_acc: 0.8601
Epoch 284/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2188 - acc: 0.9227 - val_loss: 0.4840 - val_acc: 0.8600
Epoch 285/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2119 - acc: 0.9245 - val_loss: 0.4654 - val_acc: 0.8620
.Epoch 286/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2192 - acc: 0.9228 - val_loss: 0.4699 - val_acc: 0.8607
Epoch 287/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2150 - acc: 0.9240 - val_loss: 0.4531 - val_acc: 0.8652
Epoch 288/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2085 - acc: 0.9265 - val_loss: 0.4613 - val_acc: 0.8644
Epoch 289/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2137 - acc: 0.9241 - val_loss: 0.4660 - val_acc: 0.8656
.Epoch 290/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2148 - acc: 0.9244 - val_loss: 0.4564 - val_acc: 0.8664
Epoch 291/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2141 - acc: 0.9254 - val_loss: 0.4640 - val_acc: 0.8640
Epoch 292/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2107 - acc: 0.9245 - val_loss: 0.4660 - val_acc: 0.8635
Epoch 293/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2116 - acc: 0.9255 - val_loss: 0.4715 - val_acc: 0.8648
.Epoch 294/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2131 - acc: 0.9246 - val_loss: 0.4571 - val_acc: 0.8647
Epoch 295/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2111 - acc: 0.9241 - val_loss: 0.4630 - val_acc: 0.8641
Epoch 296/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2147 - acc: 0.9246 - val_loss: 0.4580 - val_acc: 0.8651
Epoch 297/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2097 - acc: 0.9252 - val_loss: 0.4721 - val_acc: 0.8633
.Epoch 298/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2094 - acc: 0.9266 - val_loss: 0.4707 - val_acc: 0.8631
Epoch 299/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2081 - acc: 0.9265 - val_loss: 0.4638 - val_acc: 0.8677
Epoch 300/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2117 - acc: 0.9253 - val_loss: 0.4571 - val_acc: 0.8650
Epoch 301/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2048 - acc: 0.9286 - val_loss: 0.4552 - val_acc: 0.8676
.75% epoch=300, acc=0.928594, loss=0.204817, val_acc=0.867600, val_loss=0.455236, time=6.691 hours
Epoch 302/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2081 - acc: 0.9260 - val_loss: 0.4476 - val_acc: 0.8662
Epoch 303/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2074 - acc: 0.9266 - val_loss: 0.4598 - val_acc: 0.8647
Epoch 304/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2096 - acc: 0.9257 - val_loss: 0.4658 - val_acc: 0.8662
Epoch 305/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2082 - acc: 0.9257 - val_loss: 0.4706 - val_acc: 0.8622
.Epoch 306/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2086 - acc: 0.9266 - val_loss: 0.4711 - val_acc: 0.8653
Epoch 307/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2075 - acc: 0.9263 - val_loss: 0.4617 - val_acc: 0.8643
Epoch 308/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2031 - acc: 0.9282 - val_loss: 0.4610 - val_acc: 0.8663
Epoch 309/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2108 - acc: 0.9249 - val_loss: 0.4634 - val_acc: 0.8659
.Epoch 310/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2042 - acc: 0.9282 - val_loss: 0.4722 - val_acc: 0.8631
Epoch 311/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2065 - acc: 0.9260 - val_loss: 0.4723 - val_acc: 0.8632
Epoch 312/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2076 - acc: 0.9257 - val_loss: 0.4571 - val_acc: 0.8651
Epoch 313/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2076 - acc: 0.9272 - val_loss: 0.4756 - val_acc: 0.8621
.Epoch 314/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2025 - acc: 0.9271 - val_loss: 0.4692 - val_acc: 0.8644
Epoch 315/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2076 - acc: 0.9266 - val_loss: 0.4589 - val_acc: 0.8669
Epoch 316/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2044 - acc: 0.9276 - val_loss: 0.4611 - val_acc: 0.8661
Epoch 317/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2007 - acc: 0.9286 - val_loss: 0.4667 - val_acc: 0.8646
.Epoch 318/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1987 - acc: 0.9297 - val_loss: 0.4647 - val_acc: 0.8665
Epoch 319/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2071 - acc: 0.9275 - val_loss: 0.4705 - val_acc: 0.8637
Epoch 320/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2065 - acc: 0.9277 - val_loss: 0.4703 - val_acc: 0.8646
Epoch 321/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2046 - acc: 0.9282 - val_loss: 0.4659 - val_acc: 0.8667
.80% epoch=320, acc=0.928154, loss=0.204607, val_acc=0.866700, val_loss=0.465919, time=7.128 hours
Epoch 322/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1994 - acc: 0.9296 - val_loss: 0.4563 - val_acc: 0.8696
Epoch 323/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2051 - acc: 0.9278 - val_loss: 0.4520 - val_acc: 0.8667
Epoch 324/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2030 - acc: 0.9284 - val_loss: 0.4545 - val_acc: 0.8679
Epoch 325/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2007 - acc: 0.9309 - val_loss: 0.4649 - val_acc: 0.8651
.Epoch 326/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2023 - acc: 0.9267 - val_loss: 0.4656 - val_acc: 0.8650
Epoch 327/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2002 - acc: 0.9283 - val_loss: 0.4716 - val_acc: 0.8636
Epoch 328/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2019 - acc: 0.9288 - val_loss: 0.4545 - val_acc: 0.8694
Epoch 329/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1989 - acc: 0.9298 - val_loss: 0.4628 - val_acc: 0.8643
.Epoch 330/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1973 - acc: 0.9295 - val_loss: 0.4673 - val_acc: 0.8653
Epoch 331/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2064 - acc: 0.9256 - val_loss: 0.4678 - val_acc: 0.8638
Epoch 332/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2008 - acc: 0.9284 - val_loss: 0.4713 - val_acc: 0.8617
Epoch 333/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2001 - acc: 0.9289 - val_loss: 0.4621 - val_acc: 0.8650
.Epoch 334/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1977 - acc: 0.9296 - val_loss: 0.4609 - val_acc: 0.8654
Epoch 335/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.1970 - acc: 0.9302 - val_loss: 0.4667 - val_acc: 0.8653
Epoch 336/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2010 - acc: 0.9289 - val_loss: 0.4621 - val_acc: 0.8649
Epoch 337/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2004 - acc: 0.9290 - val_loss: 0.4697 - val_acc: 0.8631
.Epoch 338/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1991 - acc: 0.9287 - val_loss: 0.4693 - val_acc: 0.8654
Epoch 339/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2001 - acc: 0.9292 - val_loss: 0.4634 - val_acc: 0.8669
Epoch 340/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1977 - acc: 0.9298 - val_loss: 0.4662 - val_acc: 0.8681
Epoch 341/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2011 - acc: 0.9272 - val_loss: 0.4703 - val_acc: 0.8640
.85% epoch=340, acc=0.927133, loss=0.201122, val_acc=0.864000, val_loss=0.470282, time=7.566 hours
Epoch 342/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1938 - acc: 0.9315 - val_loss: 0.4692 - val_acc: 0.8669
Epoch 343/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1957 - acc: 0.9295 - val_loss: 0.4698 - val_acc: 0.8678
Epoch 344/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2021 - acc: 0.9292 - val_loss: 0.4575 - val_acc: 0.8678
Epoch 345/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1974 - acc: 0.9311 - val_loss: 0.4684 - val_acc: 0.8651
.Epoch 346/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1980 - acc: 0.9293 - val_loss: 0.4787 - val_acc: 0.8644
Epoch 347/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.2012 - acc: 0.9302 - val_loss: 0.4611 - val_acc: 0.8678
Epoch 348/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1922 - acc: 0.9321 - val_loss: 0.4662 - val_acc: 0.8667
Epoch 349/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1964 - acc: 0.9307 - val_loss: 0.4658 - val_acc: 0.8664
.Epoch 350/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1976 - acc: 0.9292 - val_loss: 0.4665 - val_acc: 0.8667
Epoch 351/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1973 - acc: 0.9298 - val_loss: 0.4648 - val_acc: 0.8665
Epoch 352/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.2001 - acc: 0.9297 - val_loss: 0.4696 - val_acc: 0.8643
Epoch 353/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1976 - acc: 0.9294 - val_loss: 0.4577 - val_acc: 0.8672
.Epoch 354/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1963 - acc: 0.9309 - val_loss: 0.4699 - val_acc: 0.8643
Epoch 355/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1971 - acc: 0.9306 - val_loss: 0.4646 - val_acc: 0.8655
Epoch 356/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1960 - acc: 0.9308 - val_loss: 0.4613 - val_acc: 0.8658
Epoch 357/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1939 - acc: 0.9306 - val_loss: 0.4604 - val_acc: 0.8688
.Epoch 358/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1893 - acc: 0.9329 - val_loss: 0.4676 - val_acc: 0.8682
Epoch 359/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1942 - acc: 0.9311 - val_loss: 0.4582 - val_acc: 0.8679
Epoch 360/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1975 - acc: 0.9301 - val_loss: 0.4743 - val_acc: 0.8663
Epoch 361/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1945 - acc: 0.9317 - val_loss: 0.4723 - val_acc: 0.8634
.90% epoch=360, acc=0.931716, loss=0.194514, val_acc=0.863400, val_loss=0.472329, time=8.005 hours
Epoch 362/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1956 - acc: 0.9299 - val_loss: 0.4651 - val_acc: 0.8678
Epoch 363/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1996 - acc: 0.9291 - val_loss: 0.4452 - val_acc: 0.8685
Epoch 364/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1928 - acc: 0.9316 - val_loss: 0.4655 - val_acc: 0.8667
Epoch 365/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1947 - acc: 0.9309 - val_loss: 0.4670 - val_acc: 0.8663
.Epoch 366/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1925 - acc: 0.9313 - val_loss: 0.4579 - val_acc: 0.8702
Epoch 367/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1925 - acc: 0.9318 - val_loss: 0.4728 - val_acc: 0.8672
Epoch 368/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1914 - acc: 0.9317 - val_loss: 0.4767 - val_acc: 0.8668
Epoch 369/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1919 - acc: 0.9316 - val_loss: 0.4635 - val_acc: 0.8666
.Epoch 370/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1903 - acc: 0.9326 - val_loss: 0.4660 - val_acc: 0.8666
Epoch 371/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1914 - acc: 0.9308 - val_loss: 0.4641 - val_acc: 0.8678
Epoch 372/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1943 - acc: 0.9298 - val_loss: 0.4728 - val_acc: 0.8677
Epoch 373/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.1948 - acc: 0.9310 - val_loss: 0.4655 - val_acc: 0.8657
.Epoch 374/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.1933 - acc: 0.9308 - val_loss: 0.4618 - val_acc: 0.8674
Epoch 375/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.1900 - acc: 0.9340 - val_loss: 0.4668 - val_acc: 0.8655
Epoch 376/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1847 - acc: 0.9335 - val_loss: 0.4649 - val_acc: 0.8668
Epoch 377/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1936 - acc: 0.9305 - val_loss: 0.4669 - val_acc: 0.8654
.Epoch 378/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1880 - acc: 0.9342 - val_loss: 0.4627 - val_acc: 0.8667
Epoch 379/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1969 - acc: 0.9294 - val_loss: 0.4624 - val_acc: 0.8691
Epoch 380/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1914 - acc: 0.9319 - val_loss: 0.4669 - val_acc: 0.8666
Epoch 381/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1919 - acc: 0.9310 - val_loss: 0.4646 - val_acc: 0.8661
.95% epoch=380, acc=0.931036, loss=0.191842, val_acc=0.866100, val_loss=0.464588, time=8.445 hours
Epoch 382/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1914 - acc: 0.9314 - val_loss: 0.4663 - val_acc: 0.8665
Epoch 383/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1876 - acc: 0.9335 - val_loss: 0.4561 - val_acc: 0.8690
Epoch 384/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1891 - acc: 0.9332 - val_loss: 0.4548 - val_acc: 0.8685
Epoch 385/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1870 - acc: 0.9340 - val_loss: 0.4602 - val_acc: 0.8695
.Epoch 386/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1861 - acc: 0.9336 - val_loss: 0.4713 - val_acc: 0.8660
Epoch 387/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.1928 - acc: 0.9316 - val_loss: 0.4661 - val_acc: 0.8670
Epoch 388/400
1562/1562 [==============================] - 80s 51ms/step - loss: 0.1911 - acc: 0.9316 - val_loss: 0.4571 - val_acc: 0.8705
Epoch 389/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1852 - acc: 0.9350 - val_loss: 0.4703 - val_acc: 0.8668
.Epoch 390/400
1562/1562 [==============================] - 81s 52ms/step - loss: 0.1871 - acc: 0.9339 - val_loss: 0.4713 - val_acc: 0.8672
Epoch 391/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1894 - acc: 0.9336 - val_loss: 0.4589 - val_acc: 0.8686
Epoch 392/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1887 - acc: 0.9335 - val_loss: 0.4745 - val_acc: 0.8658
Epoch 393/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1856 - acc: 0.9337 - val_loss: 0.4593 - val_acc: 0.8712
.Epoch 394/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1896 - acc: 0.9330 - val_loss: 0.4566 - val_acc: 0.8684
Epoch 395/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1857 - acc: 0.9350 - val_loss: 0.4719 - val_acc: 0.8660
Epoch 396/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1858 - acc: 0.9351 - val_loss: 0.4731 - val_acc: 0.8652
Epoch 397/400
1562/1562 [==============================] - 79s 50ms/step - loss: 0.1901 - acc: 0.9324 - val_loss: 0.4754 - val_acc: 0.8626
.Epoch 398/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1835 - acc: 0.9352 - val_loss: 0.4590 - val_acc: 0.8679
Epoch 399/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1861 - acc: 0.9349 - val_loss: 0.4700 - val_acc: 0.8671
Epoch 400/400
1562/1562 [==============================] - 79s 51ms/step - loss: 0.1850 - acc: 0.9341 - val_loss: 0.4645 - val_acc: 0.8696
 99% epoch=399 acc=0.934058 loss=0.185073
Train end: 2019-07-12 20:03:03
Total run time: 8.863 hours
max_acc = 0.935159  epoch = 397
max_val_acc = 0.871200  epoch = 392
Saving model4 to "model4.h5"
Confusion matrix, without normalization
[[904   9  19   7   5   1   6   8  30  11]
 [  2 955   3   2   0   1   1   1   5  30]
 [ 33   1 814  23  37  29  43  15   2   3]
 [ 14   2  38 709  38  96  67  23   6   7]
 [ 12   1  34  34 842  17  38  20   2   0]
 [  7   4  20 105  31 779  24  26   2   2]
 [  4   1  15  17   6   4 949   4   0   0]
 [  6   0   8  24  23  12   5 920   0   2]
 [ 43  10   4   6   1   0   2   1 918  15]
 [ 16  50   3   8   0   0   2   4  11 906]]
Large CNN with Data Augmentation: 0.8696

######################

Raw data
Confusion matrix, without normalization
[[485  16 109  20  73  20  31  11 221  14]
 [107 218  81  55 131  54  74  29 204  47]
 [ 84   0 384  61 244  57  94  16  55   5]
 [ 44   5 168 240 152 148 140  41  49  13]
 [ 58   4 216  51 457  48  85  25  54   2]
 [ 49   3 162 144 148 290 112  30  51  11]  
 [ 27   2 193  79 239  56 353  11  36   4]
 [ 64  12 143  62 201  66  86 294  56  16]
 [117  19  50  42  77  22  24  13 619  17]
 [109  57  78  69 112  37  76  56 207 199]]
1-NN Classifier: 0.3539
Confusion matrix, without normalization
[[537   4 117  13  59   7  26   5 225   7]
 [139 205 110  42 155  36  61  10 217  25]
 [107   3 452  52 226  34  66   8  49   3]
 [ 70   8 234 217 193 115  95  17  46   5]
 [ 64   1 262  35 514  21  41   7  53   2]
 [ 71   3 227 155 187 220  66  14  51   6]
 [ 27   2 273  68 314  37 248   2  28   1]
 [ 93  10 181  50 280  52  53 210  67   4]
 [141  14  52  38  54  16  14   8 655   8]
 [153  67  98  68 124  23  46  29 252 140]]
5-NN Classifier: 0.3398
Confusion matrix, without normalization
[[516   3 109  15  55   4  28   5 265   0]
 [113 183 132  40 174  21  74   8 236  19]
 [108   1 433  41 259  31  57   7  61   2]
 [ 57   4 238 197 223  95 122   9  52   3]
 [ 61   0 258  28 535  13  42  12  51   0]
 [ 49   3 217 134 214 220  88  12  57   6]
 [ 19   0 267  62 332  31 263   1  24   1]
 [ 78   6 185  53 293  54  50 200  72   9]
 [107   7  39  39  70  14  12   8 700   4]
 [135  42  99  47 136  19  54  20 297 151]]
9-NN Classifier: 0.3398
Confusion matrix, without normalization
[[494  20  39  10  84  34  50   9 200  60]
 [141 166  24  31  66  72 192  19 121 168]
 [225  24  83  15 292  48 209  21  54  29]
 [163  36  54  76 151 129 262  26  34  69]
 [ 86   8  57  26 417  38 265  22  50  31]
 [156  17  55  51 167 264 159  36  57  38]
 [106   2  60  18 228  46 467  15  19  39]
 [134  24  36  41 228  94 102 131  72 138]
 [168  41  18  17  56  83  39   8 471  99]
 [144  67  17  20  48  32 101  23 141 407]]
GaussianNB: 0.2976
Confusion matrix, without normalization
[[347  76  92  54  64  51  41  50 148  77]
 [ 77 270  65  79  66  61  41  72 101 168]
 [ 98  41 225 103 140 103 104  94  41  51]
 [ 64  58 126 169 107 131 129 104  53  59]
 [ 66  51 155  81 236 104 122 105  31  49]
 [ 53  54 109 150  95 220 101 101  65  52]
 [ 46  47 128 123 134  91 278  80  27  46]
 [ 69  74  99  94  99 103  66 267  47  82]
 [163 102  50  48  35  49  35  41 381  96]
 [ 85 170  68  69  40  67  49  77  93 282]]
DecisionTree : 0.2675
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  "of iterations.", ConvergenceWarning)
Confusion matrix, without normalization
[[484  46  56  41  22  30  22  50 176  73]
 [ 62 486  14  32  22  35  35  52  95 167]
 [121  44 272  85 112  93 136  68  48  21]
 [ 43  56  99 257  52 192 121  54  48  78]
 [ 65  23 133  58 289  95 148 125  33  31]
 [ 51  46  88 151  86 342  85  78  44  29]
 [ 12  33  74 117 100  87 485  40  21  31]
 [ 49  49  65  50  89  79  51 442  44  82]
 [172  77  18  27  10  51   8  16 520 101]
 [ 88 190  17  20  16  28  39  48 107 447]]
Logistic Regression : 0.4024
Confusion matrix, without normalization
[[336  35  50  42  33  35  38  53 302  76]
 [ 61 323  32  35  33  47  65  34 130 240]
 [ 80  72 121  71 185  90 196  81  74  30]
 [ 68  74  66 126  81 139 256  79  51  60]
 [ 49  42  81  44 258  77 299  85  39  26]
 [ 80  49  48 103  89 246 168  90  89  38]
 [ 10  45  48  77 119  55 560  40  22  24]
 [ 79  46  48  56 114  91 114 335  33  84]
 [105  67  23  16  24  35  21  20 521 168]
 [ 69 134  22  26  22  16  60  57 112 482]]
AdaBoost : 0.3308
##########################

PCA:

Confusion matrix, without normalization
[[282  16 119  52  93  52  31  39 275  41]
 [112 119 105 108 130  85  48  65 149  79]
 [ 96  13 247 121 165  93  58  50 137  20]
 [ 85  27 154 164 151 148  84  52 120  15]
 [ 87  13 171 112 286 114  65  42  98  12]
 [ 82  27 153 142 153 161  81  73 106  22]
 [ 70  22 160 121 207 112 151  47  88  22]
 [107  36 136 111 158 103  67 113 132  37]
 [180  29 122  69  94  51  33  41 335  46]
 [160  65  85  82  71  73  32  60 242 130]]
1-NN Classifier: 0.1988
Confusion matrix, without normalization
[[406  21 112  42  62  25  23  18 265  26]
 [179 139 129  93 115  77  39  23 163  43]
 [166  22 330  78 173  49  32  23 119   8]
 [163  35 208 139 176 118  37  31  83  10]
 [145  22 253  92 307  57  35  16  68   5]
 [125  42 211 152 182 135  49  39  58   7]
 [138  25 220 112 219  87  88  19  81  11]
 [198  36 204  93 140  73  43  58 131  24]
 [255  26 114  53  84  39  17  19 371  22]
 [261  71 108  65  51  43  18  34 264  85]]
5-NN Classifier: 0.2058
Confusion matrix, without normalization
[[368  13 116  43  80  24  14  12 305  25]
 [162 106 126 108 130  66  35  27 188  52]
 [155  13 339  74 187  61  24  23 118   6]
 [130  33 224 146 191 102  43  28  94   9]
 [136   8 244  81 365  57  28  10  68   3]
 [103  24 239 137 216 144  38  26  64   9]
 [115  22 214 131 271  71  74   8  85   9]
 [183  22 220 106 143  63  35  52 153  23]
 [220  29 128  59  71  36  19  20 399  19]
 [244  63 102  72  59  26  19  28 289  98]]
9-NN Classifier: 0.2091
Confusion matrix, without normalization
[[460   2 112   7 105  49  12  15   2 236]
 [166  42 107  20 108 188  53  32   2 282]
 [180  10 226  12 299 138  41  22   5  67]
 [164  19 126  21 218 264  67  21   6  94]
 [142  11 148  12 410 168  65   9   2  33]
 [107  16 155  27 229 328  60  29   6  43]
 [125   9 150   8 269 251 113   6   2  67]
 [235  17 148  15 165 184  53  37   7 139]
 [388  14 123   7 103  97  24  12   6 226]
 [240  26  82  13  60  68  21  21  10 459]]
GaussianNB: 0.2102
Confusion matrix, without normalization
[[223  79  94  75  71  66  67  70 137 118]
 [ 96 165  85  95  72  80  99  85 109 114]
 [ 75  63 163 118 146 109 101  91  81  53]
 [ 67  86  90 144 103 138  89 132  79  72]
 [ 70  68 139 104 185  90 124  92  76  52]
 [ 61  92 102 131 131 139 114  99  73  58]
 [ 70  90 112 109 115 106 158 112  65  63]
 [ 93 105  95 104 102 115  87 119  91  89]
 [136  87  79  81  62  84  65  91 204 111]
 [128 145  64  87  54  49  61  97 125 190]]
DecisionTree : 0.169
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  "of iterations.", ConvergenceWarning)
Confusion matrix, without normalization
[[446  71  40  18  28  37  50  52 101 157]
 [112 242  46  31  67  74 121  58  72 177]
 [112  78 159  49  82 129 165  81  82  63]
 [101 110  70  52  74 203 175  67  47 101]
 [ 80 101  88  40 145 179 191  77  43  56]
 [ 79  87  83  35  87 316 142  76  46  49]
 [ 64  88  74  44  90 128 359  27  43  83]
 [118  74  89  27  76 126 106 151  73 160]
 [259  60  37  17  33  75  56  59 197 207]
 [141 121  28  21  27  36  64  46 100 416]]
Logistic Regression : 0.2483
Confusion matrix, without normalization
[[301  30  80  18  76  43  35  40 197 180]
 [ 91 209  65  29  72 101  89  59  68 217]
 [ 71  55 200  37 210  87 126  57  93  64]
 [ 63  80 102  70 148 139 155  88  58  97]
 [ 64  50 147  24 287  92 187  52  46  51]
 [ 43  84 118  57 179 205 127  85  48  54]
 [ 48  48 126  41 179 113 281  38  54  72]
 [113  75 107  37 120 107  92 129  70 150]
 [210  44  83  30  71  56  54  41 245 166]
 [128 104  50  13  35  49  47  57  83 434]]
AdaBoost : 0.2361

############

Applied PCA-LDA to Dataset
Confusion matrix, without normalization
[[253  67 113  69  60  73  61  66 146  92]
 [ 92 167  65  90  98  85 108  97  85 113]
 [101  66 150 104 132 102 108  94  98  45]
 [ 86  85 123 129  93 109 129  91  87  68]
 [ 82  70 142 105 162  98 123 100  74  44]
 [ 66  82 102 137 128 146 120 107  69  43]
 [ 57  71 118 102 161 107 184  82  66  52]
 [ 88  69 104  89 103 107  86 136 111 107]
 [145  78  92  81  64  69  60  92 209 110]
 [105 133  61  84  65  67  75 101 112 197]]
1-NN Classifier: 0.1733
Confusion matrix, without normalization
[[404  69 118  55  54  44  33  41 127  55]
 [173 217  86  59  81  64  81  65  75  99]
 [172 107 206  88 126  69  73  66  57  36]
 [169  98 150 158  86  93 100  62  47  37]
 [146  98 189  98 184  80  98  43  44  20]
 [161 116 147 132 119 114  80  66  44  21]
 [113 101 165 123 126  90 164  46  39  33]
 [158 104 131  90 107  81  61 123  76  69]
 [254  83  98  68  49  42  44  60 239  63]
 [194 170  80  78  40  43  40  67 112 176]]
5-NN Classifier: 0.1985
Confusion matrix, without normalization
[[408  56 107  49  56  45  38  43 140  58]
 [125 213  96  88  79  67  90  64  80  98]
 [129  80 229 107 136  88  78  51  76  26]
 [128  81 161 179 117  90 102  56  49  37]
 [104  81 204  98 209  81 109  48  44  22]
 [107  78 156 151 138 160  82  63  39  26]
 [ 73  91 170 121 152  81 190  43  49  30]
 [146  91 132  98 116  75  67 136  70  69]
 [238  65  94  72  62  53  42  56 262  56]
 [157 147  81  77  43  46  38  56 122 233]]
9-NN Classifier: 0.2219
Confusion matrix, without normalization
[[371  42  58  19  92  24  42  35 225  92]
 [ 80 229  59  43  96  60 114  64  96 159]
 [ 89  51 202  32 236  74 109  62 104  41]
 [ 74  70 105  56 193 136 152  67  73  74]
 [ 46  49 130  23 372  80 137  53  73  37]
 [ 59  62 113  31 220 230 126  69  54  36]
 [ 41  57  97  30 237  80 303  36  62  57]
 [ 70  44 123  19 164 100  86 149 116 129]
 [160  34  63  20  89  46  55  60 392  81]
 [111 106  44  17  52  33  54  57 151 375]]
GaussianNB: 0.2679
Confusion matrix, without normalization
[[224  91  92  63  79  66  59 108 133  85]
 [ 95 183  75  89  81  84  83  96  96 118]
 [ 81  63 150 118 115 123 104  93  91  62]
 [ 86 104 105 129  95 116 114 103  76  72]
 [ 64  72 156 101 153 109 128  92  84  41]
 [ 88  87 116 119  98 153 107  95  78  59]
 [ 73  78 113 118 141 112 152  84  72  57]
 [ 80  86 108  96 109  92  94 132  99 104]
 [143  81  81  71  74  67  55  98 204 126]
 [117 126  71  66  57  65  54  99 112 233]]
DecisionTree : 0.1713
Confusion matrix, without normalization
[[427  79  49  25  31  38  40  70 109 132]
 [103 250  55  37  65  67 113  73  81 156]
 [ 98  80 179  79  85 111 137  96  79  56]
 [ 88 111  83  90  70 189 156  84  50  79]
 [ 62 106 116  60 171 151 163  90  39  42]
 [ 67  81 105  62  88 298 128  85  49  37]
 [ 52  84  86  72  94 112 353  41  47  59]
 [ 89  74  91  40  83 118 102 189  84 130]
 [227  73  44  29  30  76  49  77 228 167]
 [126 129  39  32  23  31  58  62 111 389]]
Logistic Regression : 0.2574
Confusion matrix, without normalization
[[354  44  64  27  56  27  57  51 200 120]
 [ 87 208  55  38  72  71 127  79  87 176]
 [ 89  54 232  44 153  68 156  67  88  49]
 [ 75  65 112  67 138 136 178  77  69  83]
 [ 54  62 151  36 274  91 143  89  66  34]
 [ 56  66 122  42 170 223 144  75  57  45]
 [ 50  75  89  36 159  97 339  49  51  55]
 [ 85  57 109  20 125  92 111 158 102 141]
 [204  45  67  20  53  53  63  75 299 121]
 [117 121  32  20  34  34  66  60 127 389]]
AdaBoost : 0.2543
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\svm\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
Confusion matrix, without normalization
[[396  46  86  32  36  26  46  62 168 102]
 [ 73 238  63  66  47  46 129  78  84 176]
 [ 92  57 261  77 131  74 101  79  81  47]
 [ 69  66 109 117 106 156 157  83  59  78]
 [ 52  70 171  52 260  89 144  71  57  34]
 [ 52  68 123  67 111 265 131  91  50  42]
 [ 38  70 132  53 130  89 322  51  59  56]
 [ 73  67 116  54  88 102  83 205  79 133]
 [153  37  68  41  51  58  51  79 352 110]
 [ 99 107  43  34  37  29  53  80 113 405]]
SVM: 0.2821

#######################
Applied LDA to Dataset
Confusion matrix, without normalization
[[389  47  83  51  61  57  30  54 168  60]
 [ 69 419  44  66  27  44  53  44  85 149]
 [ 88  39 279 105 124  95  98  92  45  35]
 [ 48  53 106 257  83 172 129  73  40  39]
 [ 47  25 154  69 312 106 126  88  36  37]
 [ 40  42  95 163  87 324  97  80  42  30]
 [ 29  45 113 102 110  96 402  57  18  28]
 [ 53  42  95  75  99  83  60 391  36  66]
 [142  80  54  49  24  40  27  37 452  95]
 [ 78 139  27  55  43  32  41  75  83 427]]
1-NN Classifier: 0.3652
Confusion matrix, without normalization
[[571  36  63  41  30  28  17  34 137  43]
 [ 89 511  52  59  21  42  28  22  69 107]
 [131  40 353 100 113  67  77  60  43  16]
 [ 77  56 137 341  80 115  97  49  21  27]
 [ 63  27 179  73 380  71  96  64  25  22]
 [ 75  45 131 169  87 331  59  60  30  13]
 [ 33  33 132 116 113  69 444  35  10  15]
 [ 75  48  88  63 112  83  37 422  29  43]
 [170  89  36  37  12  42  18  18 522  56]
 [100 165  30  49  36  27  30  49  76 438]]
5-NN Classifier: 0.4313
Confusion matrix, without normalization
[[549  39  58  44  35  28  13  34 156  44]
 [ 67 533  46  46  21  38  29  33  71 116]
 [ 98  29 392 105 114  61  85  60  41  15]
 [ 64  40 134 364  67 126 109  44  24  28]
 [ 64  26 163  69 408  63  95  70  25  17]
 [ 54  44 128 192  82 344  60  56  28  12]
 [ 17  27 143 101 115  63 479  29   8  18]
 [ 61  30  92  65 104  79  44 453  27  45]
 [135  78  32  41  13  34  17  17 570  63]
 [ 80 159  24  45  26  25  35  50  81 475]]
9-NN Classifier: 0.4567
Confusion matrix, without normalization
[[556  26  32  46  16  31  18  32 190  53]
 [ 56 526  34  50  18  35  33  34  88 126]
 [ 83  23 373 103 110  83 103  48  57  17]
 [ 47  31  82 407  58 170  97  24  38  46]
 [ 61  15 108  77 411  90 115  64  36  23]
 [ 40  27  83 158  73 452  62  42  40  23]
 [ 19  23  73 119  86  61 562  27  16  14]
 [ 52  28  73  66  84  93  32 472  45  55]
 [119  60  15  34  14  34   9  10 641  64]
 [ 59 125  19  34  20  25  34  43  89 552]]
GaussianNB: 0.4952
Confusion matrix, without normalization
[[379  55  92  39  70  56  26  63 160  60]
 [ 72 418  37  58  37  38  44  47  90 159]
 [ 80  34 265  98 128 105 117  76  60  37]
 [ 52  43  88 252  96 171 132  75  46  45]
 [ 53  28 153  86 305  84 122 102  28  39]
 [ 46  34 108 168  99 310  81  78  41  35]
 [ 28  31 117 125 124  99 375  40  22  39]
 [ 68  59  86  73 111  85  58 363  30  67]
 [146  79  53  55  35  44  20  28 430 110]
 [ 73 179  45  37  38  51  36  58  92 391]]
DecisionTree : 0.3488
Confusion matrix, without normalization
[[559  44  39  42  20  33  14  47 143  59]
 [ 49 563  28  36  15  29  37  47  59 137]
 [ 81  31 399  89  97  76 102  61  41  23]
 [ 37  44  93 390  57 159 100  42  26  52]
 [ 52  27 123  58 393  86 126  85  22  28]
 [ 35  35  95 148  55 454  67  57  28  26]
 [ 11  28  78  92  88  63 576  38   7  19]
 [ 35  47  72  52  65  78  39 519  36  57]
 [129  74  26  33   8  29   8  19 596  78]
 [ 54 151  24  29  21  25  33  49  64 550]]
Logistic Regression : 0.4999
Confusion matrix, without normalization
[[390  34  58  44  30  26  26  51 270  71]
 [ 31 525  27  45  17  38  54  42  89 132]
 [ 44  22 341  86 123  59 144  79  79  23]
 [ 34  36  87 357  74 154 138  46  37  37]
 [ 43  19 115  66 400  58 164  84  29  22]
 [ 36  32  93 114  93 394 101  65  49  23]
 [ 11  17  78  80 100  50 602  33  11  18]
 [ 28  32  59  60  89  71  64 492  40  65]
 [ 67  69  21  37  15  31  16  14 670  60]
 [ 33 130  23  34  19  27  41  46 105 542]]
AdaBoost : 0.4713
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\svm\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
Confusion matrix, without normalization
[[537  40  41  50  20  29  17  47 156  63]
 [ 48 548  21  52  17  31  35  43  65 140]
 [ 73  21 400 101 105  59 106  67  49  19]
 [ 35  37  91 426  69 124 111  41  27  39]
 [ 46  19 136  61 436  66 114  82  24  16]
 [ 30  27  95 164  72 428  80  57  26  21]
 [  7  24  82 104  90  53 584  29   7  20]
 [ 37  36  73  65  85  79  35 510  31  49]
 [110  64  24  39  11  25  10  17 621  79]
 [ 48 138  20  38  25  21  26  42  64 578]]
SVM: 0.5068

#################################

Applied PLS to Dataset
Confusion matrix, without normalization
[[406  57  94  47  58  39  41  50 152  56]
 [ 67 339  54  43  58  55  62  77  96 149]
 [ 97  30 254 100 150  89 115  79  38  48]
 [ 50  50 116 219 129 139 113  86  38  60]
 [ 50  28 154  96 266  86 133 121  32  34]
 [ 32  38 118 162  91 267 104  94  51  43]
 [ 22  31 122 100 152  92 337  80  20  44]
 [ 62  45  79  84 128  78  91 288  55  90]
 [148  94  46  41  50  42  37  43 376 123]
 [ 68 152  59  66  50  30  65 108 100 302]]
1-NN Classifier: 0.3054
Confusion matrix, without normalization
[[524  68  96  22  36  24  29  31 138  32]
 [101 428  58  47  41  27  46  38  95 119]
 [153  39 298  87 149  54 123  58  18  21]
 [ 84  63 163 218 108 121 117  64  25  37]
 [ 79  40 233  77 295  56 108  74  24  14]
 [ 71  43 165 172 104 252  81  64  25  23]
 [ 44  39 158  98 139  59 380  47   7  29]
 [ 89  57  90  89 122  58  89 293  44  69]
 [200 111  34  39  36  39  16  39 404  82]
 [116 178  42  51  38  26  42 108  89 310]]
5-NN Classifier: 0.3402
Confusion matrix, without normalization
[[526  59  88  19  37  19  31  36 145  40]
 [ 93 432  48  53  43  33  45  41  79 133]
 [133  36 310  90 159  52 129  48  21  22]
 [ 55  56 155 236 121 108 135  75  23  36]
 [ 66  33 221  70 332  44 116  74  30  14]
 [ 51  41 156 184 117 242  94  67  24  24]
 [ 29  29 163  81 151  55 421  38   5  28]
 [ 77  38  87  76 124  58 103 323  40  74]
 [174 100  32  51  38  39  19  40 429  78]
 [ 90 155  36  52  41  23  45  90 107 361]]
9-NN Classifier: 0.3612
Confusion matrix, without normalization
[[444  63  65  13  59  16  36  40 202  62]
 [ 53 401  59  43  47  28  68  24 101 176]
 [123  48 148  49 256  44 206  44  42  40]
 [ 62  75 112 115 167 151 165  60  21  72]
 [ 50  29  84  47 396  39 242  62  20  31]
 [ 33  56 106  93 131 310 131  64  38  38]
 [ 21  36  66  43 173  59 478  61  10  53]
 [ 38  51  74  64 173  44 146 209  45 156]
 [123  83  32  20  37  51  26  17 478 133]
 [ 36 128  41  18  43  20  58  66 137 453]]
GaussianNB: 0.3432
Confusion matrix, without normalization
[[343  60  94  46  61  45  38  62 174  77]
 [ 76 311  59  75  46  69  49  70 104 141]
 [102  53 197 102 128  95 136  88  51  48]
 [ 59  66  99 187 107 150 110  88  61  73]
 [ 72  40 134 110 247  81 126 103  45  42]
 [ 50  59 106 157  90 242 110  82  58  46]
 [ 38  64 100 104 129 106 284  98  25  52]
 [ 57  64  87  95 114  93  82 235  62 111]
 [155  98  55  48  55  52  36  56 322 123]
 [ 89 141  35  76  55  55  51 110 118 270]]
DecisionTree : 0.2638
Confusion matrix, without normalization
[[479  66  49  18  13  28  31  53 205  58]
 [ 65 439  41  28  56  28  58  40  90 155]
 [129  84 178  55 118 120 156  74  41  45]
 [ 51  92 104  96  91 236 126  98  25  81]
 [ 54  54 132  60 231  84 217  93  31  44]
 [ 39  77 105  55  79 365 104 102  47  27]
 [ 16  87  78  50  77 107 424  84  14  63]
 [ 46  51  78  47 120  91  89 287  53 138]
 [166 104  13  19  12  53   5  24 461 143]
 [ 49 143  25   7  33  19  47  91 140 446]]
Logistic Regression : 0.3406
Confusion matrix, without normalization
[[197  56  77  23  36  31  18  34 471  57]
 [ 56 406  44  34  42  49  64  21 149 135]
 [ 67  71 142  41 143 116 215  50 115  40]
 [ 56  95 100  93 117 198 154  75  47  65]
 [ 24  26 143  56 205  71 335  53  53  34]
 [ 41  77  93  74  86 323 133  65  75  33]
 [ 16  56  72  43  91  99 470  71  31  51]
 [ 33  63  87  57 111  88 157 208  79 117]
 [ 82  84  32  19  25  36  16  20 580 106]
 [ 34 135  29  12  49  22  68  65 235 351]]
AdaBoost : 0.2975
Confusion matrix, without normalization
[[110   7  16  17   7 708   6   9  59  61]
 [  9  55   2  10   5 781   5  11  19 103]
 [ 34   2 117  53  52 603  43  33   8  55]
 [  6   6  14  60  23 787  24  23   9  48]
 [ 13   5  60  61 103 585  50  54  16  53]
 [  3   0  16  42  12 848  12  18   9  40]
 [  6  11  44  64  51 622 116  27  10  49]
 [ 10   4   7  24  17 720  11  75  18 114]
 [ 23   7   8  14   7 684   0  11 122 124]
 [  8  18   3  17   6 647   1  21  18 261]]
SVM: 0.1867

##########################################
Applied PLS-LDA to Dataset
Confusion matrix, without normalization
[[392  57  98  39  59  41  33  69 155  57]
 [ 75 357  52  61  56  47  45  72  90 145]
 [ 87  37 251  98 157  93 113  87  35  42]
 [ 45  51 128 186 124 141 141  92  40  52]
 [ 57  30 155 106 260  80 142 108  30  32]
 [ 42  47 106 173  92 251  95  95  61  38]
 [ 19  34 152 120 113  93 331  77  17  44]
 [ 55  51  78  82 113  89  98 281  52 101]
 [147  94  49  53  37  47  31  52 366 124]
 [ 80 154  50  53  36  37  53 114 102 321]]
1-NN Classifier: 0.2996
Confusion matrix, without normalization
[[524  63  77  36  30  25  27  35 148  35]
 [102 471  49  42  34  30  34  37  65 136]
 [135  54 302  77 147  73 101  62  24  25]
 [ 81  76 168 212  94 142 103  66  26  32]
 [ 71  44 206  87 289  54 123  86  26  14]
 [ 64  54 145 175 102 257  82  79  31  11]
 [ 38  47 174  92 144  73 328  64  14  26]
 [ 85  68 104  80 115  79  72 296  34  67]
 [207 109  46  44  24  29  18  43 403  77]
 [107 174  46  38  35  29  38 104 102 327]]
5-NN Classifier: 0.3409
Confusion matrix, without normalization
[[509  59  92  27  36  20  22  39 159  37]
 [ 79 492  52  44  34  25  43  40  67 124]
 [117  47 308  82 154  64 114  66  23  25]
 [ 63  63 162 210 118 128 130  70  26  30]
 [ 62  29 205  75 320  51 130  81  30  17]
 [ 46  42 148 184 104 259 100  65  35  17]
 [ 19  39 168  85 132  61 407  51  12  26]
 [ 64  57  92  85 133  63  77 326  30  73]
 [173  89  42  52  26  32  17  40 435  94]
 [ 76 180  48  49  31  20  40 109 100 347]]
9-NN Classifier: 0.3613
Confusion matrix, without normalization
[[428  58  86  17  39  25  27  46 212  62]
 [ 49 433  63  27  39  29  68  29 106 157]
 [115  54 167  24 252  60 202  47  37  42]
 [ 45  81 113  77 157 173 203  58  26  67]
 [ 36  37 101  28 414  46 226  46  29  37]
 [ 37  55 106  64 134 317 144  74  41  28]
 [ 14  40  64  31 183  55 490  49  10  64]
 [ 32  49  76  34 151  67 145 249  56 141]
 [117  86  39  13  29  43  24  18 490 141]
 [ 28 126  41   7  41  21  60  73 151 452]]
GaussianNB: 0.3517
Confusion matrix, without normalization
[[357  62  94  48  59  40  42  55 168  75]
 [ 86 319  60  70  44  65  47  72  94 143]
 [ 81  50 199 114 137 102 126  81  49  61]
 [ 65  66 105 174  99 147 127 108  52  57]
 [ 52  50 130 108 214 108 138 114  37  49]
 [ 40  46  86 166  92 251 110 111  52  46]
 [ 28  55 127 128 120 105 274  89  21  53]
 [ 50  69  81  84 114 105  85 246  61 105]
 [149  91  71  49  43  51  17  69 337 123]
 [ 84 155  50  57  49  48  55  98 120 284]]
DecisionTree : 0.2655
Confusion matrix, without normalization
[[478  65  50  17  13  28  32  55 204  58]
 [ 65 434  41  28  58  27  59  40  91 157]
 [133  86 172  56 119 120 154  73  40  47]
 [ 53  93 105  97  90 233 126  99  25  79]
 [ 54  54 131  60 232  84 216  93  32  44]
 [ 40  75 105  52  81 364 105 102  51  25]
 [ 20  88  75  44  78 108 425  84  14  64]
 [ 44  50  77  46 120  92  90 288  54 139]
 [167 103  13  19  12  52   5  23 462 144]
 [ 48 141  26   9  32  20  47  90 142 445]]
Logistic Regression : 0.3397
Confusion matrix, without normalization
[[259  54  45  34  32  23  31  59 405  58]
 [ 70 413  41  33  43  32  64  46 134 124]
 [ 69  67 183  34 224  64 166  58 102  33]
 [ 40  90 112  56 183 183 148  89  44  55]
 [ 31  44 140  32 323  49 229  70  44  38]
 [ 40  65  99  63 138 298 106 100  70  21]
 [ 13  59  81  42 164  76 413  97  16  39]
 [ 33  58  76  32 151  78 124 265  66 117]
 [142  82  29  15  18  47  14  20 505 128]
 [ 57 137  29  13  28  17  65  84 197 373]]
AdaBoost : 0.3088
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\svm\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
Confusion matrix, without normalization
[[486  60  57  25  24  14  29  45 186  74]
 [ 35 539  32  41  17  27  36  40  85 148]
 [ 99  48 277  64 162  59 145  70  39  37]
 [ 34  48 112 185 113 166 164  90  36  52]
 [ 39  25 150  52 360  49 163  93  38  31]
 [ 24  39 108 137  84 337 108 100  41  22]
 [ 15  29  99  54 124  56 518  53  11  41]
 [ 34  37  59  46 124  62  97 394  40 107]
 [ 94  79  13  25  25  30  15  26 569 124]
 [ 40 146  22  32  21  23  43  77 116 480]]
SVM: 0.4145

#########################

Applied PCA-Whitening to Dataset
Confusion matrix, without normalization
[[282  16 119  52  93  52  31  39 275  41]
 [112 119 105 108 130  85  48  65 149  79]
 [ 96  13 247 121 165  93  58  50 137  20]
 [ 85  27 154 164 151 148  84  52 120  15]
 [ 87  13 171 112 286 114  65  42  98  12]
 [ 82  27 153 142 153 161  81  73 106  22]
 [ 70  22 160 121 207 112 151  47  88  22]
 [107  36 136 111 158 103  67 113 132  37]
 [180  29 122  69  94  51  33  41 335  46]
 [160  65  85  82  71  73  32  60 242 130]]
1-NN Classifier: 0.1988
Confusion matrix, without normalization
[[406  21 112  42  62  25  23  18 265  26]
 [179 139 129  93 115  77  39  23 163  43]
 [166  22 330  78 173  49  32  23 119   8]
 [163  35 208 139 176 118  37  31  83  10]
 [145  22 253  92 307  57  35  16  68   5]
 [125  42 211 152 182 135  49  39  58   7]
 [138  25 220 112 219  87  88  19  81  11]
 [198  36 204  93 140  73  43  58 131  24]
 [255  26 114  53  84  39  17  19 371  22]
 [261  71 108  65  51  43  18  34 264  85]]
5-NN Classifier: 0.2058
Confusion matrix, without normalization
[[368  13 116  43  80  24  14  12 305  25]
 [162 106 126 108 130  66  35  27 188  52]
 [155  13 339  74 187  61  24  23 118   6]
 [130  33 224 146 191 102  43  28  94   9]
 [136   8 244  81 365  57  28  10  68   3]
 [103  24 239 137 216 144  38  26  64   9]
 [115  22 214 131 271  71  74   8  85   9]
 [183  22 220 106 143  63  35  52 153  23]
 [220  29 128  59  71  36  19  20 399  19]
 [244  63 102  72  59  26  19  28 289  98]]
9-NN Classifier: 0.2091
Confusion matrix, without normalization
[[460   2 112   7 105  49  12  15   2 236]
 [166  42 107  20 108 188  53  32   2 282]
 [180  10 226  12 299 138  41  22   5  67]
 [164  19 126  21 218 264  67  21   6  94]
 [142  11 148  12 410 168  65   9   2  33]
 [107  16 155  27 229 328  60  29   6  43]
 [125   9 150   8 269 251 113   6   2  67]
 [235  17 148  15 165 184  53  37   7 139]
 [388  14 123   7 103  97  24  12   6 226]
 [240  26  82  13  60  68  21  21  10 459]]
GaussianNB: 0.2102
Confusion matrix, without normalization
[[214  72  97  81  75  64  58  80 142 117]
 [ 91 173  69  93  80  95  92  92  98 117]
 [ 85  69 158 100 134 105 116  89  78  66]
 [ 76  91  87 136 109 119  98 123  92  69]
 [ 85  59 155  98 163  84 125  94  85  52]
 [ 61  89 116 116 139 146 116 105  57  55]
 [ 66  99 113 105 121 114 150 103  74  55]
 [ 91  98  93 118  96 125  91 116  91  81]
 [136  84  89  81  76  71  74  82 197 110]
 [127 143  59  84  57  67  60  99 121 183]]
DecisionTree : 0.1636
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  "of iterations.", ConvergenceWarning)
Confusion matrix, without normalization
[[446  71  40  18  28  37  50  52 101 157]
 [112 242  46  31  67  74 121  58  72 177]
 [112  78 159  49  82 129 165  81  82  63]
 [101 110  70  52  74 203 175  67  47 101]
 [ 80 101  88  40 145 179 191  77  43  56]
 [ 79  87  83  35  87 316 142  76  46  49]
 [ 64  88  74  44  90 128 359  27  43  83]
 [118  74  89  27  76 126 106 151  73 160]
 [259  60  37  17  33  75  56  59 197 207]
 [141 121  28  21  27  36  64  46 100 416]]
Logistic Regression : 0.2483
Confusion matrix, without normalization
[[301  30  80  18  76  43  35  40 197 180]
 [ 91 209  65  29  72 101  89  59  68 217]
 [ 71  55 200  37 210  87 126  57  93  64]
 [ 63  80 102  70 148 139 155  88  58  97]
 [ 64  50 147  24 287  92 187  52  46  51]
 [ 43  84 118  57 179 205 127  85  48  54]
 [ 48  48 126  41 179 113 281  38  54  72]
 [113  75 107  37 120 107  92 129  70 150]
 [210  44  83  30  71  56  54  41 245 166]
 [128 104  50  13  35  49  47  57  83 434]]
AdaBoost : 0.2361
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\svm\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
Confusion matrix, without normalization
[[  0  39 351 144   0   0   0  67   0 399]
 [  0  61 124  87   0   0   0  48   0 680]
 [  0  20 454 179   0   0   0  73   0 274]
 [  0  27 264 204   0   0   0  76   0 429]
 [  0  24 430 228   0   0   0  84   0 234]
 [  0  23 262 185   0   0   0  73   1 456]
 [  0  29 333 222   0   0   0  74   0 342]
 [  0  28 228 134   0   0   0 108   0 502]
 [  0  65 293 158   0   0   0  84   0 400]
 [  0  38 109  87   0   0   0  43   0 723]]
SVM: 0.155
#####################################

Applied PCA-Whitening + LDA to Dataset
Confusion matrix, without normalization
[[389  47  83  51  61  57  30  54 168  60]
 [ 69 419  44  66  27  44  53  44  85 149]
 [ 88  39 279 105 124  95  98  92  45  35]
 [ 48  53 106 257  83 172 129  73  40  39]
 [ 47  25 154  69 312 106 126  88  36  37]
 [ 40  42  95 163  87 324  97  80  42  30]
 [ 29  45 113 102 110  96 402  57  18  28]
 [ 53  42  95  75  99  83  60 391  36  66]
 [142  80  54  49  24  40  27  37 452  95]
 [ 78 139  27  55  43  32  41  75  83 427]]
1-NN Classifier: 0.3652
Confusion matrix, without normalization
[[571  36  63  41  30  28  17  34 137  43]
 [ 89 511  52  59  21  42  28  22  69 107]
 [131  40 353 100 113  67  77  60  43  16]
 [ 77  56 137 341  80 115  97  49  21  27]
 [ 63  27 179  73 380  71  96  64  25  22]
 [ 75  45 131 169  87 331  59  60  30  13]
 [ 33  33 132 116 113  69 444  35  10  15]
 [ 75  48  88  63 112  83  37 422  29  43]
 [170  89  36  37  12  42  18  18 522  56]
 [100 165  30  49  36  27  30  49  76 438]]
5-NN Classifier: 0.4313
Confusion matrix, without normalization
[[549  39  58  44  35  28  13  34 156  44]
 [ 67 533  46  46  21  38  29  33  71 116]
 [ 98  29 392 105 114  61  85  60  41  15]
 [ 64  40 134 364  67 126 109  44  24  28]
 [ 64  26 163  69 408  63  95  70  25  17]
 [ 54  44 128 192  82 344  60  56  28  12]
 [ 17  27 143 101 115  63 479  29   8  18]
 [ 61  30  92  65 104  79  44 453  27  45]
 [135  78  32  41  13  34  17  17 570  63]
 [ 80 159  24  45  26  25  35  50  81 475]]
9-NN Classifier: 0.4567
Confusion matrix, without normalization
[[556  26  32  46  16  31  18  32 190  53]
 [ 56 526  34  50  18  35  33  34  88 126]
 [ 83  23 373 103 110  83 103  48  57  17]
 [ 47  31  82 407  58 170  97  24  38  46]
 [ 61  15 108  77 411  90 115  64  36  23]
 [ 40  27  83 158  73 452  62  42  40  23]
 [ 19  23  73 119  86  61 562  27  16  14]
 [ 52  28  73  66  84  93  32 472  45  55]
 [119  60  15  34  14  34   9  10 641  64]
 [ 59 125  19  34  20  25  34  43  89 552]]
GaussianNB: 0.4952
Confusion matrix, without normalization
[[391  49 104  41  51  53  25  57 155  74]
 [ 65 421  40  55  41  42  44  42  76 174]
 [ 80  36 269  98 119 108 114  83  50  43]
 [ 48  53  95 245 100 170 131  79  38  41]
 [ 50  29 146  94 295  87 127 108  28  36]
 [ 48  36 118 149  88 315  80  91  41  34]
 [ 23  32 117 123 123  99 386  37  17  43]
 [ 64  61  85  77 107  91  53 369  30  63]
 [142  77  56  56  37  44  15  38 425 110]
 [ 65 176  50  37  46  45  33  63 105 380]]
DecisionTree : 0.3496
Confusion matrix, without normalization
[[559  44  39  42  20  33  14  47 143  59]
 [ 49 563  28  36  15  29  37  47  59 137]
 [ 81  31 399  89  97  76 102  61  41  23]
 [ 37  44  93 390  57 159 100  42  26  52]
 [ 52  27 123  58 393  86 126  85  22  28]
 [ 35  35  95 148  55 454  67  57  28  26]
 [ 11  28  78  92  88  63 576  38   7  19]
 [ 35  47  72  52  65  78  39 519  36  57]
 [129  74  26  33   8  29   8  19 596  78]
 [ 54 151  24  29  21  25  33  49  64 550]]
Logistic Regression : 0.4999
Confusion matrix, without normalization
[[390  34  58  44  30  26  26  51 270  71]
 [ 31 525  27  45  17  38  54  42  89 132]
 [ 44  22 341  86 123  59 144  79  79  23]
 [ 34  36  87 357  74 154 138  46  37  37]
 [ 43  19 115  66 400  58 164  84  29  22]
 [ 36  32  93 114  93 394 101  65  49  23]
 [ 11  17  78  80 100  50 602  33  11  18]
 [ 28  32  59  60  89  71  64 492  40  65]
 [ 67  69  21  37  15  31  16  14 670  60]
 [ 33 130  23  34  19  27  41  46 105 542]]
AdaBoost : 0.4713
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\svm\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
Confusion matrix, without normalization
[[537  40  41  50  20  29  17  47 156  63]
 [ 48 548  21  52  17  31  35  43  65 140]
 [ 73  21 400 101 105  59 106  67  49  19]
 [ 35  37  91 426  69 124 111  41  27  39]
 [ 46  19 136  61 436  66 114  82  24  16]
 [ 30  27  95 164  72 428  80  57  26  21]
 [  7  24  82 104  90  53 584  29   7  20]
 [ 37  36  73  65  85  79  35 510  31  49]
 [110  64  24  39  11  25  10  17 621  79]
 [ 48 138  20  38  25  21  26  42  64 578]]
SVM: 0.5068

######################

Applied Feature Selection to Dataset
Confusion matrix, without normalization
[[337  73  98  46  29  37  34  49 210  87]
 [102 155 116  90  77 100  76  91 101  92]
 [157  64 186  77  86 115  82  69 107  57]
 [103  52 129 191  93 132  87  68  80  65]
 [113  66 166 123 140 107  77  69  97  42]
 [104  65 176 105  81 186  91  71  77  44]
 [104  65 169 111  97 123 154  60  79  38]
 [118  82 112  86  81  80  71 156 104 110]
 [268  71 113  49  44  33  31  50 259  82]
 [135 111 104  69  37  61  48  92 139 204]]
1-NN Classifier: 0.1968
Confusion matrix, without normalization
[[531  50  91  16  19  14  17  21 195  46]
 [172 175 160  80  55  90  46  70  76  76]
 [264  78 231  72  58  91  35  43  91  37]
 [187 105 182 155  71 119  57  35  51  38]
 [210  79 263  96 111  59  57  28  71  26]
 [171 105 213 111  71 166  55  24  55  29]
 [181  85 265  87  92  81  99  24  65  21]
 [188 117 142  66  60  69  52 123  95  88]
 [429  69  87  25  19  21  10  25 264  51]
 [237 132  83  66  25  50  23  59 146 179]]
5-NN Classifier: 0.2034
Confusion matrix, without normalization
[[526  36  85  15  13  17  16  28 216  48]
 [162 144 170  94  65  91  47  64  87  76]
 [258  59 249  76  71  95  37  27  95  33]
 [159  86 212 171  69 122  48  46  53  34]
 [191  41 288  90 118  89  51  26  85  21]
 [147  71 238 132  76 162  61  24  57  32]
 [157  60 285 103 102 101  93  28  50  21]
 [168  87 166  84  64  75  30 123 104  99]
 [427  49  92  30  24  22  15  12 286  43]
 [240 118  94  66  22  46  23  52 144 195]]
9-NN Classifier: 0.2067
Confusion matrix, without normalization
[[137   1  45   1  99   1  56   5 521 134]
 [ 71  20  94   8 134   5 294  30 193 151]
 [ 37   2  83   6 253   4 329   9 210  67]
 [ 40  23  96  14 202   8 384  19 168  46]
 [ 20   4  75   6 270   3 445  12 148  17]
 [ 32  15  91   7 223  13 429  14 138  38]
 [ 27   3  70   0 251   1 494   4  98  52]
 [ 61  17  82  10 135   4 274  21 234 162]
 [105   9  58   1  94   1  70   7 530 125]
 [126  21  44   8  79   3  92  26 299 302]]
GaussianNB: 0.1884
Confusion matrix, without normalization
[[239  73 107  48  68  42  44  65 197 117]
 [ 81 145  92 104 102  97  94  98  76 111]
 [ 90  79 144  96 111 109  96 112 105  58]
 [ 59 105 104 144 105 141 106  92  74  70]
 [ 67  80 115 112 150 113 144  92  74  53]
 [ 52  98 102 137 109 178 124  75  58  67]
 [ 55  75 126 122 125 119 169  91  64  54]
 [101 110  87  87  99  90  96 139  85 106]
 [202  73  94  52  56  53  39  77 228 126]
 [120 114  79  76  58  58  46 113 115 221]]
DecisionTree : 0.1757
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  "of iterations.", ConvergenceWarning)
Confusion matrix, without normalization
[[710   7   4   1   6  13   4  22  83 150]
 [366  40  29  12  28  63  24  80 100 258]
 [480  26  13   6  12  63  19  51 104 226]
 [344  33  24  17  27  91  34  64 140 226]
 [392  25  33  11  24  70  15  86 132 212]
 [364  16  17  13  24 152  17  81 111 205]
 [382  24  30  12  30  67  24  74 147 210]
 [298  27  17   8  23  71  19 143 111 283]
 [610  10   5   7   4  10   4  32 107 211]
 [369  21   9   8   8  35   8  70 107 365]]
Logistic Regression : 0.1595
Confusion matrix, without normalization
[[218   4  75   3  66  19  53  13 259 290]
 [ 97  33  80  49 133 112 132  73 100 191]
 [ 93  10 106  34 275  93 154  27 116  92]
 [ 96  22  90  65 208 145 176  27  96  75]
 [ 74   7  79  35 382 112 171  26  72  42]
 [ 86  18  78  55 254 180 186  33  60  50]
 [ 56   7  85  56 319 131 219  19  49  59]
 [ 71  18  69  32 184  79 115  94 120 218]
 [158   6  66  11  68  21  58  29 312 271]
 [116  17  43  28  70  34  58  65 143 426]]
AdaBoost : 0.2035
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\svm\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
Confusion matrix, without normalization
[[ 51  17  25 858   0   1   5   3  25  15]
 [ 10  37   5 926   1   1   2   1   8   9]
 [ 18   7  21 936   1   2   2   4   7   2]
 [  1   3   3 988   0   0   2   1   1   1]
 [  2   1   1 993   2   0   0   1   0   0]
 [  0   0   1 991   0   4   2   1   0   1]
 [  2   6   5 978   1   0   5   2   0   1]
 [  2  12   1 970   0   3   3   2   5   2]
 [ 23   6  18 928   4   0   0   2  18   1]
 [  6  17   3 948   6   0   2   2   5  11]]
SVM: 0.1139

##############################

Raw data
Confusion matrix, without normalization
[[485  16 109  20  73  20  31  11 221  14]
 [107 218  81  55 131  54  74  29 204  47]
 [ 84   0 384  61 244  57  94  16  55   5]
 [ 44   5 168 240 152 148 140  41  49  13]
 [ 58   4 216  51 457  48  85  25  54   2]
 [ 49   3 162 144 148 290 112  30  51  11]
 [ 27   2 193  79 239  56 353  11  36   4]
 [ 64  12 143  62 201  66  86 294  56  16]
 [117  19  50  42  77  22  24  13 619  17]
 [109  57  78  69 112  37  76  56 207 199]]
1-NN Classifier: 0.3539
Confusion matrix, without normalization
[[537   4 117  13  59   7  26   5 225   7]
 [139 205 110  42 155  36  61  10 217  25]
 [107   3 452  52 226  34  66   8  49   3]
 [ 70   8 234 217 193 115  95  17  46   5]
 [ 64   1 262  35 514  21  41   7  53   2]
 [ 71   3 227 155 187 220  66  14  51   6]
 [ 27   2 273  68 314  37 248   2  28   1]
 [ 93  10 181  50 280  52  53 210  67   4]
 [141  14  52  38  54  16  14   8 655   8]
 [153  67  98  68 124  23  46  29 252 140]]
5-NN Classifier: 0.3398
Confusion matrix, without normalization
[[516   3 109  15  55   4  28   5 265   0]
 [113 183 132  40 174  21  74   8 236  19]
 [108   1 433  41 259  31  57   7  61   2]
 [ 57   4 238 197 223  95 122   9  52   3]
 [ 61   0 258  28 535  13  42  12  51   0]
 [ 49   3 217 134 214 220  88  12  57   6]
 [ 19   0 267  62 332  31 263   1  24   1]
 [ 78   6 185  53 293  54  50 200  72   9]
 [107   7  39  39  70  14  12   8 700   4]
 [135  42  99  47 136  19  54  20 297 151]]
9-NN Classifier: 0.3398
Confusion matrix, without normalization
[[494  20  39  10  84  34  50   9 200  60]
 [141 166  24  31  66  72 192  19 121 168]
 [225  24  83  15 292  48 209  21  54  29]
 [163  36  54  76 151 129 262  26  34  69]
 [ 86   8  57  26 417  38 265  22  50  31]
 [156  17  55  51 167 264 159  36  57  38]
 [106   2  60  18 228  46 467  15  19  39]
 [134  24  36  41 228  94 102 131  72 138]
 [168  41  18  17  56  83  39   8 471  99]
 [144  67  17  20  48  32 101  23 141 407]]
GaussianNB: 0.2976
Confusion matrix, without normalization
[[350  68  90  49  64  41  40  62 158  78]
 [ 76 266  69  87  63  61  53  64  97 164]
 [103  46 222  91 138 104 116  93  39  48]
 [ 74  60 107 178 114 139 135 104  38  51]
 [ 63  55 151  81 227 100 129  99  49  46]
 [ 60  63 118 156  77 224  96  97  56  53]
 [ 47  47 142 118 130  98 284  59  24  51]
 [ 81  72  90 100  91  96  62 265  56  87]
 [142 103  51  44  47  56  23  46 378 110]
 [ 96 167  55  67  44  55  48  84 100 284]]
DecisionTree : 0.2678
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.
  "of iterations.", ConvergenceWarning)
Confusion matrix, without normalization
[[484  46  56  41  22  30  22  50 176  73]
 [ 62 486  14  32  22  35  35  52  95 167]
 [121  44 272  85 112  93 136  68  48  21]
 [ 43  56  99 257  52 192 121  54  48  78]
 [ 65  23 133  58 289  95 148 125  33  31]
 [ 51  46  88 151  86 342  85  78  44  29]
 [ 12  33  74 117 100  87 485  40  21  31]
 [ 49  49  65  50  89  79  51 442  44  82]
 [172  77  18  27  10  51   8  16 520 101]
 [ 88 190  17  20  16  28  39  48 107 447]]
Logistic Regression : 0.4024
Confusion matrix, without normalization
[[336  35  50  42  33  35  38  53 302  76]
 [ 61 323  32  35  33  47  65  34 130 240]
 [ 80  72 121  71 185  90 196  81  74  30]
 [ 68  74  66 126  81 139 256  79  51  60]
 [ 49  42  81  44 258  77 299  85  39  26]
 [ 80  49  48 103  89 246 168  90  89  38]
 [ 10  45  48  77 119  55 560  40  22  24]
 [ 79  46  48  56 114  91 114 335  33  84]
 [105  67  23  16  24  35  21  20 521 168]
 [ 69 134  22  26  22  16  60  57 112 482]]
AdaBoost : 0.3308
C:\Users\Hamidreza\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\svm\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
Confusion matrix, without normalization
[[  94    0   17    0    7    0    0    0   15  867]
 [   0   26    0    0    0    0    0    0    0  974]
 [  20    0   86    1   44    1    6    0    3  839]
 [   3    0    2   20    4    1    4    0    1  965]
 [   6    0   39    4  106    1    5    0    3  836]
 [   1    0    2    1    4   14    1    0    1  976]
 [   2    0   14    4   20    2   47    0    2  909]
 [   1    0    1    0    5    0    1   16    0  976]
 [   8    0    1    0    2    0    1    0   66  922]
 [   0    0    0    0    0    0    0    0    0 1000]]
SVM: 0.1475